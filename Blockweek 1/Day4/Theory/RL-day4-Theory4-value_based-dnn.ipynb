{"cells":[{"cell_type":"markdown","metadata":{"id":"B405ogbGDV--"},"source":["# Value-based RL DNN approaches"]},{"cell_type":"markdown","metadata":{"id":"XEkvqbqrDV_H"},"source":["First, consider again the curses we covered in the previous lesson:\n","\n","* curse of dimension\n","* curse of modeling\n","* curse of credit assignment\n","\n","The problem with the first two was the size of the state space and action space. Simply, it is not possible to store the values for each state separately. Therefore we turned toward feature extraction and linear methods. We have also seen, there are pretty good performance guarantees. Here are the tables again."]},{"cell_type":"markdown","metadata":{"id":"HETmCyY7DV_J"},"source":["**Prediction algorithms:**\n","\n","| On/Off-policy | Algorithm | Tabular | Linear | Non-linear|\n","|------|------|------|------|------|\n","| On-policy | MC | YES | YES | YES |\n","| On-policy | TD($\\lambda$) | YES | YES | NO |\n","| On-policy | **Gradient TD** | YES | YES | YES |\n","| Off-policy | MC | YES | YES | YES |\n","| Off-policy | TD($\\lambda$) | YES | NO | NO |\n","| On-policy | **Gradient TD** | YES | YES | YES |"]},{"cell_type":"markdown","metadata":{"id":"0cDhy0AdDV_J"},"source":["**Control algorithms:**\n","\n","| Algorithm | Tabular | Linear | Non-linear|\n","|------|------|------|------|\n","| MC | YES | YES | NO |\n","| Sarsa | YES | YES | NO |\n","| Q-learning | YES | NO | NO |\n","| **Gradient Q** | YES | YES | NO |"]},{"cell_type":"markdown","metadata":{"id":"SmQliDANDV_K"},"source":["In the tables above, two new algorithms are highlighted which have better convergence properties. We do not discuss them in detail but it can be useful to know about them. Details: [Gradient TD](http://incompleteideas.net/Talks/gradient-TD-2011.pdf), [Gradient Q-learning 1](http://agi-conf.org/2010/wp-content/uploads/2009/06/paper_21.pdf) and [Gradient Q-learning 2](https://arxiv.org/pdf/1705.03967.pdf)."]},{"cell_type":"markdown","metadata":{"id":"5_oTeC0DDV_L"},"source":["**Question:** can we eliminate or handle the problem of the curse of modeling and the curse of dimension?\n","\n","So far, we have seen that the model can be eliminated by applying the Q-function with sampling (e.g.: Q-learning, Sarsa-learning and their variants). Those algorithms are called model-free learning algorithms.\n","\n","The **curse of credit assignment** is more challenging. n-step return, $\\lambda$-return can make the learning more stable and by using a long horizon the credit assignment can be easier. But there is no clear-cut solution for this.\n","\n","The **curse of dimension** requires a different approach instead of tabular methods. Manual feature extraction has a long history in machine learning but deep learning makes it possible to learn the features automatically. It is tempting to apply deep learning for representing the state values."]},{"cell_type":"markdown","metadata":{"id":"ikQ14QuADV_L"},"source":["### DQN - Deep Q-Network"]},{"cell_type":"markdown","metadata":{"id":"FAIkzENTDV_M"},"source":["The DQN algorithm was one of the first RL algorithms where the RL framework was combined with DNNs and the result was satisfying. The properties of the algorithm:\n","\n","* relatively easy to implement\n","* simple but powerful\n","* difficult convergence\n","* sensitive to the hyper-parameters\n"]},{"cell_type":"markdown","metadata":{"id":"h1Xlv4PTDV_M"},"source":["DQN is based on Q-learning but the $Q$-function is approximated by a neural network: $Q_\\theta(s, a)$. Then the update rule for the $\\theta$ parameters is given as:\n","\n","$$\\theta_{t+1} = \\theta_t + \\alpha \\cdot \\left( r_t + \\max_{a'}Q_\\theta(s', a') - Q_\\theta(s, a) \\right)\\cdot \\nabla_\\theta Q_\\theta(s, a)|_{\\theta = \\theta_t}$$"]},{"cell_type":"markdown","metadata":{"id":"rNccrJ25DV_N"},"source":["**How can we represent the state?** The problem is when something is moving on the image (e.g.: a ball) then a static frame is not able to represent it at a time point. The convolutional networks are memory less therefore they cannot store information across the consecutive frames. Remember, RL assumes an MDP (Markov decision process), which requires that the state contains all the information about the environment.\n","\n","A good approximation of this is to use a bunch of consecutive frames. Therefore a moving object appears in different places on the consecutive frames."]},{"cell_type":"markdown","metadata":{"id":"gbaQ57zwDV_N"},"source":["**Preprocessing steps:**\n","\n","The frames arriving from the simulator need to be preprocessed before feeding  them into the network.\n","\n","<img src=\"http://drive.google.com/uc?export=view&id=1v6xXmKxSbElHF8RDgwmjP4eou2DxMHDj\" width=75%>"]},{"cell_type":"markdown","metadata":{"id":"HSiUobAgDV_O"},"source":["The preprocessing of the raw input frames consists of the following steps, as the above image illustrates:\n","\n","* grayscale the image\n","* cropping (only the interesting part of the image will remain)\n","* downsampling (or resizing) the image for $84\\times 84$\n","* stacking four frames together to form the state"]},{"cell_type":"markdown","metadata":{"id":"WGiELhqTDV_O"},"source":["**Network architecture:**\n","\n","* Conv2D(kernel\\_num=32, kernel\\_size=(8, 8), padding='valid', input_shape=(84, 84, 4), strides=(4, 4))\n","* Activation('relu')\n","* Conv2D(kernel\\_num=64, kernel\\_size=(4, 4), padding='valid', strides=(2, 2))\n","* Activation('relu')\n","* Conv2D(kernel\\_num=64, kernel\\_size=(3, 3), padding='valid', strides=(1, 1))\n","* Activation('relu')\n","* Flatten()\n","* Dense(units=512, activation='relu')\n","* Dense(num\\_actions)"]},{"cell_type":"markdown","metadata":{"id":"y7VQZH4xDV_O"},"source":["DQN has two major tricks to avoid the instability caused by the neural network approximator:\n","\n","1. experience replay\n","2. iterative update"]},{"cell_type":"markdown","metadata":{"id":"z3VF7ge7DV_O"},"source":["**Experience replay:** Use past experience for the same state/ action (not just the current one) - may have different rewards and different new states they result in\n","<img src=\"http://drive.google.com/uc?export=view&id=1rqQQyPxhDTSFScMwmXDLGAE6eUpAWoSu\" width=75%>"]},{"cell_type":"markdown","metadata":{"id":"ImJ5jJ9IDV_O"},"source":["There are two main reasons why experience replay can help to converge faster:\n","\n","1. If all of the samples are taken consecutively before feeding it into the network then the data samples will be correlated. This correlation makes the learning slower and harms the generalization. The replay buffer gathers the experiences in a buffer and the training batches are sampled according to a uniform distribution.\n","2. There are valuable states (experiences) which should be used more times because it affects the policy strongly. However, may be the state is visited rarely because it is hard to reach it. Because the replay buffer stores a long history of experiences, the rare experiences can be reused several times."]},{"cell_type":"markdown","metadata":{"id":"dc5ylWp1DV_O"},"source":["**Iterative update:**\n","\n","One of the reasons behind the instability of Q-learning combined with a deep neural network is the fast change (high variance) of the one-step return. The one-step return depends on the network itself and the network weight is updated frequently. The network has no time to adapt and follow up changes.\n","\n","Iterative update or (delayed update) uses two networks for representing the $Q$-function. The architecture is the same but the weights are different. The weights are synchronized after a given number of steps.\n","\n","The goal of the first network is to calculate the return and it is not updated until synchronization. The second network is responsible for selecting the next step and it is always updated according to the update rule."]},{"cell_type":"markdown","metadata":{"id":"w9yeZ9MWDV_P"},"source":["The update rule changes to the following one:\n","\n","$$\\theta_{t+1} = \\theta_t + \\alpha \\cdot \\left( r_t + \\max_{a'}\\hat{Q}_{\\theta^-}(s', a') - Q_\\theta(s, a) \\right)\\cdot \\nabla_\\theta Q_\\theta(s, a)|_{\\theta = \\theta_t}$$"]},{"cell_type":"markdown","metadata":{"id":"0hAjGkDnDV_P"},"source":["The next slide shows the pseudo code of the DQN algorithm. The experience replay is the $D$ buffer in the code. The algorithm stores and samples experiences from the buffer. The iterative update is implemented with $\\hat{Q}$ and $Q$."]},{"cell_type":"markdown","metadata":{"id":"kOZIGmXRDV_P"},"source":["<img src=\"http://drive.google.com/uc?export=view&id=1EaDj9o9-ACuMsf9PtmMw4p3CMtknTVAg\" width=55%>"]},{"cell_type":"markdown","metadata":{"id":"4gOl-V7MDV_P"},"source":["[Video playing Atari](https://www.youtube.com/watch?v=V1eYniJ0Rnk)"]},{"cell_type":"markdown","metadata":{"id":"DxO3yqmNDV_P"},"source":["### Double DQN"]},{"cell_type":"markdown","metadata":{"id":"sNnk28MiDV_Q"},"source":["**Problem: Traditional DQN systematically overestimates Q**\n","\n","\n","<img src=\"https://drive.google.com/uc?export=download&id=1jaH7q73Pc_GzuIDt2nkHPzo3rgAq7z16\"  height=400>"]},{"cell_type":"markdown","metadata":{"id":"sQuxwqU_DV_Q"},"source":["Happens in $Q$ learning in general, not just DQN.\n","\n","\n","Because of the way we set the Q target: $y_i=r_i+\\gamma \\color{red} {{\\max}}' _{a \\prime} Q_\\phi\\left(s_i^{\\prime}, a_i^{\\prime}\\right)$\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"pr6_5dEYDV_Q"},"source":["max operator is prone to overestimation\n","- Say, 100 people have equal \"true\" weights $=80  \\mathrm{ kg}$.\n","- We have a weighing scale that is off by $+/-1 \\mathrm{ kg}$.\n","- We measure person $1^{\\prime}$ 's weight and store it in $X^1$.\n","- We measure person 2's weight and store it in $X^2$, and so on.\n","Set $Y=\\max _i X^i$.\n","\n","Then $Y$ is almost always $>80 \\mathrm{kg}$\n","\n","At noise $0, Y=80 \\mathrm{ kg}$. As the measurement noise $\\uparrow, Y \\uparrow$.\n","\n","So under noise, max is biased to be larger than it should be.\n"]},{"cell_type":"markdown","metadata":{"id":"cjJ-xbjBDV_Q"},"source":["Idea: Measure each person's weight twice (independent noise): $X_1^i$ and $X_2^i$ Then set:\n","\n","\n","$$\n","n=\\operatorname{argmax}_i\\left(X_1^i\\right)\n","$$\n","\n","$$\n","Y=X_2^n\n","$$\n","\n","New estimate of the maximum is robust to noise"]},{"cell_type":"markdown","metadata":{"id":"Y4rpZQXwDV_Q"},"source":["**Double Deep Q-Learning**\n","\n","Train two independent $Q$ networks $\\color{red} Q_{\\phi_1}$ and $\\color{blue} Q_{\\phi_2}$ on disjoint subsets of experience.\n","Then, to train $\\color{red}Q_{\\phi_1}$, the target is no longer: $y_i=r_i+\\gamma \\max _{a^{\\prime}} \\color{red}Q_{\\phi_1}\\left(s_i^{\\prime}, a_i^{\\prime}\\right)$\n","\n","Instead, it is: $y_i=r_i+\\gamma \\color{blue}{Q_{\\phi_2}}\\left(s_i^{\\prime}, \\operatorname{argmax}_{a_0^{\\prime}} \\color{red}Q_{\\phi_1}\\left(s^{\\prime}, a^{\\prime}\\right)\\right)$\n","- The Q-network being trained is used to select actions\n","- The other Q-network is used to evaluate actions\n","\n","Note: This is independent of the target network trick to avoid shifting targets. If you use combine target networks and double DQN, you will have four $Q$ networks.\n","\n","\n","[paper](https://arxiv.org/pdf/1509.06461.pdf)"]},{"cell_type":"markdown","metadata":{"id":"o9At6TDLDV_R"},"source":["### DQN with prioritized experience replay"]},{"cell_type":"markdown","metadata":{"id":"LiRgwSPYDV_R"},"source":["[paper](https://arxiv.org/pdf/1511.05952.pdf)\n","\n","Remember, in case of the DQN we sample the experiences uniformly (with equal probability) from the experience replay.\n","Unfortunately, this approach assumes that all of the experiences have equal impact on learning. It is easy to understand that this is not true. There are experiences with more relevance. If we define a metric or indicator, to decide which experience is the more important, then we can create a prioritized experience replay. The experiences with higher priority are chosen more frequently.\n","\n","How can we measure the importance? A common way for that is to calculate the TD-error (we have already seen it), which indicates how surprising or unexpected the transition is:\n","\n","$$\\delta = r + \\gamma \\max_{a'}Q(s', a') - Q(s, a)$$\n","\n","Then, the probability of sampling transition $i$ is:\n","\n","$$P(i) = \\frac{p^\\alpha}{\\sum_k p^\\alpha_k}$$\n","\n","where:\n","\n","$$p_i = |\\delta_i| + \\varepsilon$$\n","\n","$\\alpha$ is a hyperparameter and $\\alpha=0$ is equal with the uniform sampling."]},{"cell_type":"markdown","metadata":{"id":"Oc37ZD_QDV_R"},"source":["<img src=\"https://drive.google.com/uc?export=download&id=1UK6o9wAMGZ_IC8UM2JD9yUt34YwZD7re\"  height=350>"]},{"cell_type":"markdown","metadata":{"id":"XSTnCn-IDV_R"},"source":["<img src=\"https://drive.google.com/uc?export=download&id=1Foki_zqVOJLkE2eJrpXmZJVxQ3w8prDB\" >"]},{"cell_type":"markdown","metadata":{"id":"-TtSWlW3DV_R"},"source":["### Dueling network for DQN"]},{"cell_type":"markdown","metadata":{"id":"0rhi1m-cDV_R"},"source":["**Idea**\n","\n","- Explicitly separate representation of state value (state dependent) and action value advantage\n","- Why does this matter?  learn generally which states are/are not valuable regardless of action\n","- Useful in states where action does not have an effect on the environment in any useful way (here the actions are just like noise)\n","- Another way of seeing this is that having two targets for the same NN and hence regularizing\n"]},{"cell_type":"markdown","metadata":{"id":"rUi8R4SfDV_R"},"source":["[paper](https://arxiv.org/pdf/1511.06581.pdf)\n","\n","\n","\n","<img src=\"http://drive.google.com/uc?export=view&id=1GKSbfJ5rZXIcedspexfhU4rtbIRsjVKR\" width=75%>"]},{"cell_type":"markdown","metadata":{"id":"SXkFnsjSDV_R"},"source":["\n","\n","<img src=\"http://drive.google.com/uc?export=view&id=1Ht03QlfXcq4onRUhAqlJaXKZx7cYsIAM\" width=75%>"]},{"cell_type":"markdown","metadata":{"id":"R9tT9zlgDV_S"},"source":["Definition of the **advantage**:\n","\n","$$A(s, a) = Q(s, a) - V(s)$$\n","\n","Combining the estimated $V$ and $A$ in the network is tricky because of the identification problem, mentioned in the paper. For instance by adding a constant to the $V$ and substracting the same constant from $A$, the $Q$ will be the same.\n","\n","However, the following is true for the relation between $V$ and $Q$ in case of a deterministic policy:\n","\n","$$V(s) = Q(s, a^*)$$\n","\n","If we compare the two equations then:\n","\n","$$A(s, a^*) = 0$$"]},{"cell_type":"markdown","metadata":{"id":"zYxYXPTYDV_S"},"source":["In order to avoid the identification problem, the authors suggest to force the last equation to hold by:\n","\n","$$Q(s, a; \\theta, \\alpha, \\beta) = V(s; \\theta, \\beta) + \\left( A(s, a; \\theta, \\alpha) - \\max_{a'} A(s, a'; \\theta, \\alpha) \\right)$$\n","\n","However, to further improve the stability of optimization, the paper proposed the following module to combine $V$ and $A$ at the output:\n","\n","$$Q(s, a; \\theta, \\alpha, \\beta) = V(s; \\theta, \\beta) + \\left( A(s, a; \\theta, \\alpha) - \\frac{1}{|D|} \\sum_{a' \\in D}A(s, a'; \\theta, \\alpha) \\right)$$\n","\n","This is just saying that\n","\n","q= v + a - a.mean()\n","\n","$D$ now is the action space."]},{"cell_type":"markdown","metadata":{"id":"JJJKytO4DV_S"},"source":["The main contribution of this approach, that it helps generalizing among different environments. In case of the Atari, the games differ in the action space too (e.g.: the number of actions and their meaning)."]},{"cell_type":"markdown","metadata":{"id":"4px1ph12DV_S"},"source":["### TreeQN"]},{"cell_type":"markdown","metadata":{"id":"b-LdiUnDDV_Y"},"source":["[paper](https://arxiv.org/pdf/1710.11417.pdf)\n","\n","- On-line planning with look-ahead trees has proven successful in environments where transition models are known a priori.\n","- However, in complex environments where transition models need to be learned from data, the deficiencies of learned models have limited their utility for planning.\n","- TreeQN dynamically constructs a tree by recursively applying a transition model in a learned abstract state space and then aggregating predicted rewards and state-values using a tree backup to estimate Q-value\n"]},{"cell_type":"markdown","metadata":{"id":"e-W-59AqDV_Y"},"source":["\n","<img src=\"http://drive.google.com/uc?export=view&id=1fX44HRPKzRJI6U7vxaAZPa6bg7tHEGpY\" width=75%>"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"aJ7g2qzFDV_Z"},"outputs":[],"source":[]}],"metadata":{"celltoolbar":"Slideshow","kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.7"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":0}