{"cells":[{"cell_type":"markdown","metadata":{"id":"kWGFyWMwMMh9"},"source":["# Functional approximation"]},{"cell_type":"markdown","metadata":{"id":"2sNW3MQMMMiL"},"source":["## Prelude: curses"]},{"cell_type":"markdown","metadata":{"id":"p1YybojcMMiN"},"source":["### Curse of dimensionality\n","\n","The state space tends to grow to a large size easily. Let's take the Atari game. The input is an $84\\times 84 \\times 4$ image with intensity values between 0 and 256. How many states are possible?"]},{"cell_type":"markdown","metadata":{"id":"jRi5J3twMMiP"},"source":["$$|S| = 255^{84\\cdot 84 \\cdot 4} \\approx 10^{50000}$$\n","\n","Which is huge, and it is not possible to store it in a table. This is the curse of dimensionality."]},{"cell_type":"markdown","metadata":{"id":"7hxRFcrAMMiR"},"source":["### Curse of modeling\n","\n","therefore the transition matrix $T(s, a, s')$ is way bigger than the state space. For instance in case of the Atari game, the size of the transition matrix:\n","\n","$$|T| = |S| \\times |A| \\times |S| \\approx 10^{100000}$$\n","\n","The transition matrix is huge and impossible to store it in the memory. It is also difficult to learn or identify such a huge model by sampling the environment. It is harder than exploring the state itself. This is the curse of modeling. Model-free methods can eliminate this problem."]},{"cell_type":"markdown","metadata":{"id":"5G-TIFPLMMiT"},"source":["### Curse of credit assignment\n","\n","When a longer episode is played or executed, it is not evident which actions were the most relevant to achieve the final result. This is the credit assignment problem."]},{"cell_type":"markdown","metadata":{"id":"ik3E3kHTMMiU"},"source":["## Motivation:\n","\n","1. We've seen RL finds optimal policies for arbitrary environments, if the value functions $V(s)$ and policies $Q(s, a)$ can be exactly represented in tables\n","2. But the real world is too large and complex for tables\n","3. Will RL work for function approximators?\n"]},{"cell_type":"markdown","metadata":{"id":"UKZxS3-rMMiX"},"source":["## Old and new setting in pseudo code\n","$$\n","\\begin{array}{l}\n","\\left.\\mathrm{Q}=\\mathrm{np} . \\text { zeros([n_states, } \\mathrm{n}_{-} \\text {actions }\\right] \\text { ) } \\\\\n","\\mathrm{a}_{-} \\mathrm{p}=\\text { Q[s,:] } \\\\\n","\\# \\text { action-value table is approximated: } \\\\\n","\\mathrm{a}_{-} \\mathrm{p}=\\text { DeepNeuralNetwork(s) }\n","\\end{array}\n","$$"]},{"cell_type":"markdown","metadata":{"id":"H-PjQzu2MMia"},"source":["## Definition: Function approximation\n","There are too many states/actions to fit in memory, which are too slow to process. Therefore we estimate the value function:\n","$$\n","\\hat{v}(S, \\mathbf{w}) \\approx v_{\\pi}(S)\n","$$\n","or for control we do:\n","$$\n","\\hat{q}(S, A, \\mathbf{w}) \\approx q_{\\pi}(S, A)\n","$$\n","\n","Where **$w$** are the learned weights to estimate the parameter function\n","\n","TD or Monte Carlo methods will give us the targets to fit our functional approximators to = target for loss function"]},{"cell_type":"markdown","metadata":{"id":"P3vDRcjEMMic"},"source":["## Types of functional approximations\n","\n","(1) State-Value approximation\n","\n","(2) Action Value approximation: action in approximation\n","\n","(3) Action Value approximation: action out approximation\n","<img src=\"http://drive.google.com/uc?export=view&id=1AVRvfT1Cxhs_my-AM9h51Y70vBGhSRlc\" width=35%>\n","\n"]},{"cell_type":"markdown","metadata":{"id":"wUNFT7ajMMie"},"source":["## Challenges: functional approximation\n","\n","1. Data is non-stationary\n","\n","    * As you explore and discover new rewards, the value function can change drastically, e.g. suddenly being crushed by boulder in Zelda\n","    * As the policy changes there is a constant drift\n","\n","\n","2. Data is not i.i.d.\n","\n","    * When playing a game, all subsequent experiences are highly correlated\n","\n"]},{"cell_type":"markdown","metadata":{"id":"O0U9j00DMMig"},"source":["## Definition: incremental SGD for prediction\n","Incremental ways to do this, using stochastic gradient decent, to achieve incremental value function approximation.\n","$$\n","\\begin{aligned}\n","\\mathbf{w}_{t+1} &=\\mathbf{w}_{t}-\\frac{1}{2} \\alpha \\nabla_{w}\\left(v_{\\pi}\\left(S_{t}\\right)-\\hat{v}\\left(S_{t}, \\mathbf{w}_{t}\\right)\\right)^{2} \\\\\n","&=\\mathbf{w}_{t}+\\alpha\\left(v_{\\pi}\\left(S_{t}\\right)-\\hat{v}\\left(S_{t}, \\mathbf{w}_{t}\\right)\\right) \\nabla_{w} \\hat{v}\\left(S_{t}, \\mathbf{w}_{t}\\right)\n","\\end{aligned}\n","$$\n","How do we compute $v_{\\pi}\\left(S_{t}\\right) ?$ We substitute it with a target.\n","\n","Note that the second line is just an alternative formulation of the first one common in reinforcement learning (take the derivative in the first equation by applying the chain rule)"]},{"cell_type":"markdown","metadata":{"id":"qx5azsRmMMii"},"source":["## Definition: $\\text { substituting } v_{\\pi}\\left(S_{t}\\right)\n","$\n","\n","\n","For **MC** learning, the target is the return $G_{t}$ :\n","$$\n","\\mathbf{w}_{t+1}=\\mathbf{w}_{t}-\\frac{1}{2} \\alpha \\nabla_{\\mathbf{w}} \\left(\\color{red}{G_{t}}-\\color{black}{v}\\left(S_{t}, \\mathbf{w}_{t}\\right)\\right)^{2}\n","$$\n","For $\\mathbf{T D}(0)$, the target is $R_{t+1}+\\gamma \\hat{v}\\left(S_{t+1}, \\mathbf{w}\\right)$ :\n","$$\n","\\mathbf{w}_{t+1}=\\mathbf{w}_{t}-\\frac{1}{2} \\alpha \\nabla_{\\mathbf{w}}\\left(\\color{red}{R_{t+1}+\\gamma \\hat{v}\\left(S_{t+1}, \\mathbf{w}\\right)}\\color{black}-\\hat{v}\\left(S_{t}, \\mathbf{w}_{t}\\right)\\right)^{2}\n","$$\n","\n","\n","For $\\operatorname{TD}(\\lambda)$, the target is the $\\lambda$ return $G_{t}^{\\lambda}$.\n","$$\n","\\mathbf{w}_{t+1}=\\mathbf{w}_{t}-\\frac{1}{2} \\alpha \\nabla_{\\mathbf{w}}\\left(\\color{red}{G_{t}^{\\lambda}}-\\hat{v}\\left(S_{t}, \\mathbf{w}_{t}\\right)\\right)^{2}\n","$$\n"]},{"cell_type":"markdown","metadata":{"id":"sxzS5jn0MMil"},"source":["## Definition: action-value function approximation for control\n","\n","For control, we wish to approximate the action-value function $\\hat{q}(S, A, \\mathrm{w}) \\approx q_{\\pi}(S, A)$\n","$$\n","\\begin{aligned}\n","\\mathbf{w}_{t+1} &=\\mathbf{w}_{t}-\\frac{1}{2} \\alpha \\nabla_{\\mathbf{w}}\\left(q_{\\pi}\\left(S_{t}, A_{t}\\right)-\\hat{q}\\left(S_{t}, A_{t}, \\mathbf{w}_{t}\\right)\\right)^{2} \\\\\n","&=\\mathbf{w}_{t}+\\alpha\\left(q_{\\pi}\\left(S_{t}, A_{t}\\right)-\\hat{q}\\left(S_{t}, A_{t}, \\mathbf{w}_{t}\\right)\\right) \\nabla_{\\mathbf{w}} \\hat{v}\\left(S_{t}, \\mathbf{w}_{t}\\right)\n","\\end{aligned}\n","$$\n","Similarly we substitute $q_{\\pi}\\left(S_{t}, A_{t}\\right)$ with a target.\n","\n"]},{"cell_type":"markdown","metadata":{"id":"YAVwX0wWMMin"},"source":["## Definition: $\n","\\text { substituting } q_{\\pi}\\left(S_{t}, A_{t}\\right)\n","$\n","\n","For $\\mathrm{MC}$ learning, the target is the return $G_{t}$ :\n","$$\n","\\mathbf{w}_{t+1}=\\mathbf{w}_{t}-\\frac{1}{2} \\alpha \\nabla_{\\mathbf{w}}\\left(\\color{red}{G_{t}}-\\hat{q}\\left(S_{t}, A_{t}, \\mathbf{w}_{t}\\right)\\right)^{2}\n","$$\n","For $\\mathrm{TD}(0)$, the target is $R_{t+1}+\\gamma \\hat{q}\\left(S_{t+1}, A_{t+1}, \\mathrm{w}\\right)$ :\n","$$\n","\\mathbf{w}_{t+1}=\\mathbf{w}_{t}-\\frac{1}{2} \\alpha \\nabla_{\\mathbf{w}}\\left(\\color{red}{R_{t+1}+\\gamma \\hat{q}\\left(S_{t+1}, A_{t+1}, \\mathbf{w}\\right)}-\\hat{q}\\left(S_{t}, A_{t}, \\mathbf{w}_{t}\\right)\\right)^{2}\n","$$\n","For $\\operatorname{TD}(\\lambda)$, the target is the $\\lambda$ return:\n","$$\n","\\mathbf{w}_{t+1}=\\mathbf{w}_{t}-\\frac{1}{2} \\alpha \\nabla_{\\mathbf{w}}\\left(\\color{red}{q_{t}^{\\lambda}}-\\hat{q}\\left(S_{t}, A_{t}, \\mathbf{w}_{t}\\right)\\right)^{2}\n","$$"]},{"cell_type":"markdown","metadata":{"id":"n2bImLZlMMio"},"source":["The previous section borrows from [Wilcocks, 2021](https://cwkx.github.io/data/teaching/dl-and-rl/rl-lecture7.pdf)"]},{"cell_type":"markdown","metadata":{"id":"uHOUciFKCWF7"},"source":["### Feature extraction for linear methods\n","\n","The question is how can we create a compressed representation of a large state space. We should choose specific states in the state space then every state that is close to it, can be described in the same way. This is similar to discretization. Can we do a better approach?"]},{"cell_type":"markdown","metadata":{"id":"MspvUuBWCWF7"},"source":["**Polynomials**\n","\n","Suppose each state $s$ corresponds to $k$ numbers, $s_1, s_2, ..., s_k$, with each $s_i \\in R$. For this $k$-dimensional state space, each order-$n$ polynomial-basis feature $x_i$ can be written as\n","\n","$$x_i(s) = \\prod_{j=1}^k{s_j^{c_{i, j}}}$$\n","\n","where each $c_{i, j}$ is an integer in the set $\\{ 0, 1, ..., n \\}$ for an integer $n \\ge 0$."]},{"cell_type":"markdown","metadata":{"id":"WmbLazLCCWF7"},"source":["**Radial Basis Functions**\n","\n","It is useful for continuous-valued features. The RBF feature, $x_i$, depends on the distance between the state $s$ and the corresponding center state, $c_i$, and the feature's width, $\\sigma_i$:\n","\n","$$x_i(s) = e^{\\left( -\\frac{||s-c_i||^2}{2\\sigma_i^2} \\right)}$$"]},{"cell_type":"markdown","metadata":{"id":"Dqf5S_tRCWF7"},"source":["Note that for a two dimensional space the RBF function looks as follows\n","$$x_i(s) = e^{\\left( -\\frac{||s_1-c_1i||^2 +||s_2-c_2i||^2}{2\\sigma_i^2} \\right)}$$"]},{"cell_type":"markdown","metadata":{"id":"KGifsCosCWF7"},"source":["<img src=\"http://drive.google.com/uc?export=view&id=16eO17_rp7JpaGE6jdlOxz7GH-8dpsgY8\" width=75%>"]}],"metadata":{"celltoolbar":"Slideshow","colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"}},"nbformat":4,"nbformat_minor":0}