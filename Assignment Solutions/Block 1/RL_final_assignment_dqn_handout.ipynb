{"cells":[{"cell_type":"markdown","metadata":{"id":"1YL15CyX-NHv"},"source":["# DQN on the CartPole problem"]},{"cell_type":"markdown","metadata":{"id":"OoCN-Ql--NH2"},"source":["In this assignment you will implement the DQN algorithm to solve a classic control problem, the CartPole."]},{"cell_type":"markdown","metadata":{"id":"6Ke2uQQ--NH3"},"source":["### The CartPole problem"]},{"cell_type":"markdown","metadata":{"id":"Sym66S8N-NH4"},"source":["As the image below shows, the goal of the agent is to balance a verticle rod on the top of the car. This position is unstable and that is the main reason for the difficulty.\n","\n","<img src=\"https://drive.google.com/uc?export=download&id=1wiFksyB3-mcirfdZEvrT2DPD7SBEjye2\" >\n","\n","The problem is solved if the average of the agent's scores is greater than 195 gathered in 100 episodes.\n","The agent receives reward 1 in each timestep as long as the position of the rod is correct (not inclined too far away from the vertical position).\n","The length of one episode is 200 time steps. Therefore the possible maximum score is 200.\n","\n","The state is low dimensional and cosists of:\n","* position\n","* velocity\n","* angle\n","* angular velocity\n","\n","Further details can be found on OpenAI gym's webpage: (https://gymnasium.farama.org/environments/classic_control/cart_pole/)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bENAYqWlY_LT"},"outputs":[],"source":["# Installs necessary in Colab\n","!pip install gymnasium\n","!pip install gymnasium[classic-control]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MtzXVOGu-NH6"},"outputs":[],"source":["%matplotlib inline\n","import matplotlib.pyplot as plt\n","import gymnasium as gym\n","import numpy as np\n","import random\n","from enum import Enum\n","from skimage import transform as trf\n","from keras.models import Sequential # Keras: highlevel API above dnn libraries (tendorflow, cntr, theano)\n","from keras.layers import Dense, Convolution2D, Flatten\n","from keras.optimizers import Adam, SGD, RMSprop\n","from numpy.random import seed"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nr_EMSWC-NH8"},"outputs":[],"source":["class Optimizer(Enum): # Enum, Makes easier to try different optimizers\n","    ADAM = 1\n","    RMSPROP = 2\n","    SGD = 3"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6DZSHgJg-NH9"},"outputs":[],"source":["# The implementation of DQN.\n","class Dqn:\n","\n","    def __init__(self, params):\n","        self.env = None                       # The environment where the RL agent will learn.\n","        self.buffer_size = params.buf_size    # The maximum size of the experience replay.\n","        self.batch_size = params.batch        # Batch size during training.\n","        self.epoch = params.epoch             # For one training cycle, the number epoch on a batch.\n","        self.max_episode = params.max_ep      # The number of episodes for training.\n","        self.eps0 = params.eps                # The starting value of epsilon in the epsilon-greedy policy.\n","        self.gamma = params.gamma             # Discounting factor.\n","        self.C = params.C                     # Frequency of synchronizing the frozen network.\n","        self.train_freq = params.train_freq   # Update frequency for the not frozen network.\n","        self.eval_freq = params.eval_freq     # Evaluation frequency.\n","        self.net = params.net                 # The description of the network. List of tuples. A tuple: (number of units, activation)\n","        self.lr = params.lr                   # learning rate\n","        self.opt = params.opt                 # Optimizer.\n","\n","        self.q_cont, self.q_frzn = None, None # two networks for training: continuously updated and frozen\n","\n","        self.buffer = []  # experience replay\n","\n","        self.env = gym.make('CartPole-v0', render_mode=\"rgb_array\" )\n","\n","        self.env.reset(seed=1, options={})\n","        self.q_cont, self.q_frzn = self._init_models()\n","\n","    # ------------------------------------------------------\n","    # functions for initialization\n","\n","    def _init_optimizer(self):\n","\n","        optz = None\n","        if self.opt == Optimizer.ADAM:\n","            optz = Adam(self.lr)\n","        elif self.opt == Optimizer.SGD:\n","            optz = SGD(self.lr)\n","        elif self.opt == Optimizer.RMSPROP:\n","            optz = RMSprop(self.lr)\n","\n","        return optz\n","\n","    # The network builds up from Dense layers (similar to the fully connected)\n","    def _init_models(self):\n","\n","        def build(strc):\n","            # strc - list of tuples\n","            # each tuple contains: number of nodes in the dense layer, activation function name (e.g.: 'relu')\n","            # ----- implement this -----\n","            q = ...  # ----- create a sequential model -----\n","            # ----- add a dense layer with input_shape 4 (4 frames will be stacked) -----\n","            # use the strc for accessing the required parameters\n","\n","\n","            for i in range(1, len(strc)):\n","                # ----- add the remaining dense layers to the model\n","\n","\n","            optz = self._init_optimizer()\n","            # compile the model with an appropriate loss function\n","            return q\n","\n","        q_cont = build(self.net)  # continuously updated network (Q-function)\n","        q_frzn = build(self.net)  # frozen network\n","\n","        q_cont.set_weights(q_frzn.get_weights())  # synchronization\n","        return q_cont, q_frzn\n","\n","    def _init_buffer(self, number):\n","        # gathers 'number' pieces of experiences randomly\n","        # ----- study and understand this piece of code carefully -----\n","        exps = []\n","        obs, rw, terminated, truncated , _ = self.env.step(0)\n","        done = terminated or truncated\n","\n","        for _ in range(number):\n","\n","            if done:\n","                obs, info = self.env.reset(seed=1, options={})\n","\n","            action = self.env.action_space.sample()  # sampling random actions from the environment\n","            obs_next, rw, terminated, truncated , _= self.env.step(action)  # taking the step and observe the results\n","            done = terminated or truncated\n","            exps.append((obs, rw, action, done, obs_next))  # we append a new experience\n","            obs = obs_next\n","\n","        self.append(exps)  # you will implement this function\n","\n","    def close(self):\n","        self.env.close()\n","\n","    def train_function(self):\n","\n","        # initializing experience replay with random experiences\n","        self._init_buffer(self.batch_size)\n","\n","        print(\"Initialization was finished.\")\n","        print(\"Training was started.\")\n","\n","        ep_id = 1\n","        cntr = 0\n","        eval_permitted = True\n","        rtn = 0\n","        exps = []\n","\n","        ep_ids = []\n","        returns = []\n","\n","        eps = self.eps0\n","        self.env.reset()\n","        obs, _, terminated, truncated , _ = self.env.step(0)\n","        done = terminated or truncated\n","\n","\n","        while ep_id < self.max_episode:\n","\n","            cntr += 1\n","\n","            if done:\n","                if ep_id % 10 == 0:\n","                    print('Episode Id: ' + str(ep_id) + ' Return during training: ' + str(rtn))\n","                rtn = 0\n","                ep_id += 1\n","                eval_permitted = True\n","                obs, info = self.env.reset(seed=1, options={}) # when an episode ends (done = True) the environment is reseted\n","\n","            action = ....   # ---- select the next action with epsilon greedy -----\n","\n","\n","            obs_next, rw, terminated, truncated , _ = ....  # ----- take a new step with the environment -----\n","            done = terminated or truncated\n","\n","            rtn += rw\n","\n","            if done:\n","                if rtn < 180:\n","                    rw = -1\n","                    obs_next *= 0.0\n","                    obs *= 0.0\n","                elif rtn >= 180:\n","                    rw = 100\n","\n","            exps.append((obs, rw, action, done, obs_next))\n","            obs = obs_next\n","\n","            if cntr % 128 == 0:\n","                self.append(exps)\n","                exps.clear()\n","\n","            # training\n","            if  cntr % self.train_freq == 0:\n","                # ----- sample experiences from the replay then train q_cont with them\n","\n","\n","            # synchronizing the frozen network\n","            if cntr % self.C == 0:\n","                self.q_frzn.set_weights(self.q_cont.get_weights())\n","\n","            # evaluating at the current stage of learning\n","            if ep_id % self.eval_freq == 0 and eval_permitted:\n","                r = self.evaluation()\n","                ep_ids.append(ep_id)\n","                returns.append(r)\n","                #print('EValuation at episode: ' + str(ep_id) + ' -> ' +  str(r))\n","                eval_permitted = False\n","                if r >= 185:\n","                    break\n","\n","            # Decrasing the epsilon value for epsilon-greedy. Exploration -> exploitation\n","            eps = max(eps - 0.001, 0.01)\n","\n","        print(\"Training was finished.\")\n","        return ep_ids, returns\n","\n","    def evaluation(self, video=False):\n","        orig_env = self.env\n","\n","        obs, info = self.env.reset(seed=1, options={})\n","        done = False\n","        rtn = 0\n","        ep_id = 0\n","        rtns = []\n","\n","        while ep_id < 50:\n","\n","            if done:\n","                rtns.append(rtn)\n","                rtn = 0\n","                ep_id += 1\n","                obs, info  = self.env.reset(seed=1, options={})\n","\n","\n","            action = self.select_action_epsilon(obs, 0.01)\n","            obs, rw, terminated, truncated , _ = self.env.step(action)\n","            done = terminated or truncated\n","\n","            rtn += rw\n","\n","        self.env = orig_env\n","        return np.mean(rtns)\n","\n","    # ------------------------------------------------------\n","    # Functions for handling the experience replay\n","\n","    def clear_buffer(self):\n","        self.buffer.clear()\n","\n","    # The new experiences are added at the end of the buffer.\n","    # The too old experiences are deleted.\n","    def append(self, experiences):\n","        # experiences - list of experiences\n","        # ----- implement this -----\n","\n","        # ----- check if appending the new set of experiences to the buffer has enough space -----\n","        if ...:\n","            # ----- if not, delete as many experiences as required -----\n","\n","        self.buffer += experiences  # finally we append the new experiences to the buffer\n","\n","    def sample(self, number):\n","        exps = random.sample(self.buffer, number)    # experiences list\n","        obs = np.stack([x[0] for x in exps], axis=0) # numpy array is used by keras, for creating a batch observations should be stacked\n","        rws = ... # ----- do similar stacking for the rewards -----\n","        acts = ... # ----- implement this too -----\n","        dones = ... # ----- implement this too -----\n","        next_obs = ... # ----- implement this too -----\n","\n","        q_vals = ... # ----- predict (forward execute) with q_cont on obs -----   # q_vals size should be: (batch_size, 2)\n","        fzn_q_vals = ... # ----- predict with q_frzn on next_obs -----\n","\n","        # The action function is represented by a network.\n","        # The input of this network is the state,\n","        # the output is the set of action-values\n","        # corresponding to the actions.\n","        # So the number of outputs is equal with the nunmber of actions.\n","        # In training we sample one transition at a time, therefore we have loss\n","        # for only one output (action) at a time.\n","        # But for training, we have to provide information for all of the outputs.\n","        # How can we solve this?\n","\n","        ## The input to the neural network is the stack of observations (obs).\n","        ## The target should also be passed during the training. The target is nothing but the immediate reward + discounted value of the next state.\n","        ## We should get the next state’s value by using the frozen network and the max of the value shall be taken as per Q-learning algorithm.\n","        ## this is how the sub_values shall be calculated which is the target value during training.\n","\n","        sub_values = ...  # ----- calculate this according to the one-step return for Q-learning -----\n","        q_vals[list(range(number)), acts] = sub_values  # this will be the target during training\n","\n","        x = obs.astype(dtype=np.float32)\n","        y = q_vals.astype(dtype=np.float32)\n","\n","        return x, y\n","\n","    # ------------------------------------------------------\n","    # Choosing an action\n","\n","    # epsilon-greedy\n","    def select_action_epsilon(self, state, eps):  # state shape: (4) nunmpy array\n","        s = np.expand_dims(state, axis=0)\n","        max_idx = np.argmax(self.q_cont.predict(s, batch_size=1, verbose=0))\n","        if np.random.random() < 1 - eps:\n","            return max_idx\n","        return (max_idx + 1) % 2 # now we have only two actions\n","\n","    # no epsilon-greedy\n","    def select_action(self, state):\n","        s = np.expand_dims(state, axis=0)\n","        return np.argmax(self.q_cont.predict(s, batch_size=1, verbose=0))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"n7eLArh5-NIE"},"outputs":[],"source":["class Parameters:\n","\n","    def __init__(self):\n","                                 # Default values\n","        self.buf_size = 5000     # 5000\n","        self.batch = 256         # 256\n","        self.epoch = 5           # 5\n","        self.max_ep = 100        # 100\n","        self.eps = 0.5           # 0.5\n","        self.gamma = 0.9         # 0.9\n","        self.C = 100             # 100\n","        self.train_freq = 1      # 1\n","        self.eval_freq = 10      # 10\n","        self.net = [(128, 'relu'), (128, 'relu'), (2, 'relu')] # [(128, 'relu'), (128, 'relu'), (2, 'relu')]\n","        self.lr = 0.0001         # 0.0001\n","        self.opt = Optimizer.ADAM # Optimizer.ADAM"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NnSEvZR2-NIG"},"outputs":[],"source":["# Running the training and evaluation\n","pms = Parameters()\n","dqn = Dqn(pms)\n","ep_ids, returns = dqn.train_function()\n","plt.plot(ep_ids, returns)\n","dqn.evaluation(video=False)\n","dqn.close()"]},{"cell_type":"markdown","metadata":{"id":"bT86NWsordwX"},"source":["### Question:\n","\n","* Does the algorithm converge all the time?\n","* What happens if you change the default parameters?\n","* How does your algorithm compare to other algorithms on the leader board?\n","* Search the literature: What type of other algorithms are used to solve this problem (e.g. actor-critic)? (Preset policies do not matter.)\n","* Remove the activation functions from your network. This results in a linear approximator. How do the results change?"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QP_BlUQOrsUJ"},"outputs":[],"source":[]}],"metadata":{"colab":{"machine_shape":"hm","provenance":[]},"gpuClass":"standard","kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"}},"nbformat":4,"nbformat_minor":0}