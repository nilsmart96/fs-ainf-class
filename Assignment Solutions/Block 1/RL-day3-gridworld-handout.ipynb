{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing the necessary libraries\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib import cm\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://drive.google.com/uc?export=view&id=18q7KL4aV6McMtaid_1Let2aGkw6d4QYn\" width=45%>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GridWorld:\n",
    "    \"\"\"\n",
    "    The gridworls is a frequently used demo environment in reinforcement learning\n",
    "    to try and test ideas.\n",
    "    Today, we will use it to understand the concepts so far.\n",
    "    \n",
    "    The environment: (see image)\n",
    "    * cells: the agent can step on a cell. There is exactly one cell to start from.\n",
    "    This is the top left corner. There is one terminal cell where the walking ends, \n",
    "    the agent can not leave it (blue).\n",
    "    * obstacles: there are cells where the agent can not step. (gray)\n",
    "    * agent: it can move from one cell to an other neighboring cell. \n",
    "    Possible directions: up, down, left, right. Each transition happens with probability 1.\n",
    "    * reward: after each transition the agent receives -1 point. In the terminal cell, no reward\n",
    "    received anymore.\n",
    "    \n",
    "    Implement the environment below!\n",
    "    \"\"\"\n",
    "    def __init__(self, size, start_cell, obstacles, terminating_state):\n",
    "        self.size = size\n",
    "        self.start = start_cell\n",
    "        self.obstacles = obstacles\n",
    "        self.termin = terminating_state\n",
    "        self.current_cell = self.start\n",
    "    \n",
    "    def reset(self):\n",
    "        # ----- reset the current cell to the start cell to start again -----\n",
    "    \n",
    "    def transition(self, cell, action):\n",
    "        # ----- IMPLEMENT FUNCTION -----\n",
    "        # cell = (row, column) indices\n",
    "        # action: 0 left, 1 up, 2 right, 3 down\n",
    "        # returns: What will be the next state\n",
    "        # Take care of the borders of the grid!\n",
    "        \n",
    "        # ....\n",
    "        \n",
    "        return (r_next, c_next)\n",
    "\n",
    "    def reward(self, cell, action):\n",
    "        # ----- RETURN REWARD -----\n",
    "        # -1 if not in the terminal state\n",
    "    \n",
    "    def in_terminal(self):\n",
    "        return self.current_cell == self.termin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DPsolver:\n",
    "    \"\"\"\n",
    "    This solver is based on the Bellman-equation and it is \n",
    "    solved iteratively .\n",
    "    The action-value is used to represent the utility of the \n",
    "    actions and states.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, gridworld, gamma, iterations):\n",
    "        # setting parametes according to the input parameters\n",
    "        self.gridworld = gridworld\n",
    "        self.gamma = gamma\n",
    "        self.iterations = iterations\n",
    "        size = gridworld.size\n",
    "        # initialize accumulaters\n",
    "        self.cntr = 0\n",
    "        self.sum_rewards = []\n",
    "        self.path = []\n",
    "        # ----- initialize the table for Q-function -----\n",
    "        self.q_table = \n",
    "\n",
    "    def step(self):\n",
    "        # ----- WRITE THE CODE BELOW -----\n",
    "        # implement one step in the value iteration\n",
    "        rows, cols = self.gridworld.size  # ask for the size of the grid\n",
    "        # ----- cycle over the rows -----\n",
    "        \n",
    "            # ----- cycle over the columns -----\n",
    "\n",
    "                # ----- cycle over the actions -----\n",
    "\n",
    "                    # ----- get the reward -----\n",
    "                    reward = ... \n",
    "                    # ----- calculate the corresponding next step (what would happen) -----\n",
    "                    cell_next = ... \n",
    "                    r2, c2 = cell_next\n",
    "                    self.q_table[act, r, c] = ... # ----- update the q-table -----\n",
    "        # increase the counter\n",
    "        self.cntr += 1\n",
    "        # add the return to the sum_rewards list\n",
    "        self.sum_rewards.append(self.trajectory())\n",
    "\n",
    "    def trajectory(self):\n",
    "        # ----- IMPLEMENT THE FUNCTION -----\n",
    "        # reset the gridworld\n",
    "        \n",
    "        # calculate the return along a trajectory followed by the current policy\n",
    "        # when started from the start_cell\n",
    "        sum_rewards = 0\n",
    "        \n",
    "        # TODO\n",
    "        \n",
    "        return sum_rewards\n",
    "\n",
    "    def is_learning_finished(self):\n",
    "        # ----- IMPLEMENT THIS FUNCTION -----\n",
    "        # check whether learning has finished, return it\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_learning_curve(ql):\n",
    "    values = ql.sum_rewards\n",
    "    x = list(range(len(values)))\n",
    "    y = values\n",
    "    plt.plot(x, y, 'ro')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grid world parameters\n",
    "size = (6, 6)\n",
    "start_cell = (0, 0)\n",
    "obstacles = [(3, 3)]\n",
    "terminating_state = (3, 5)\n",
    "# q learning parameters\n",
    "gamma = 0.9\n",
    "# ----- What is the minimum required number of iterations? -----\n",
    "iterations = 500\n",
    "\n",
    "gw = GridWorld(size, start_cell, obstacles, terminating_state)\n",
    "solver = DPsolver(gw, gamma, iterations)\n",
    "\n",
    "while not solver.is_learning_finished():\n",
    "    solver.step()\n",
    "    sum_rewards = solver.sum_rewards[-1]\n",
    "    print(sum_rewards)\n",
    "\n",
    "sum_rewards = solver.trajectory()\n",
    "print(sum_rewards)\n",
    "plot_learning_curve(solver)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
