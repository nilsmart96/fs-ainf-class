{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-armed bandits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this task you are going to implement ETC and UCB. Then you will play around the parameters in order to draw some more conclusion.\n",
    "\n",
    "Outline:\n",
    "* Simulator writing\n",
    "* ETC and experiments\n",
    "* UCB and experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bulding a simulator for the bandit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import numpy as np  # we can use this library for working with matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Bandit:\n",
    "    def __init__(self, k, means, rounds):\n",
    "        # we assume Gaussian distributions with sigma=1\n",
    "        self.k = k  # number of arms\n",
    "        self.means = means   # List of means, one mean for each arm\n",
    "        self.rounds = rounds  # number of available rounds\n",
    "        \n",
    "        # ----- chose the optimal reward -----\n",
    "        self.optimal_reward = \n",
    "        self.counter = 0\n",
    "        # gather the empirical regret so far (the empirical regret is the best arm, measured empirically, i.e. from the actual drawn samples compared to the actual actions)\n",
    "        self.empirical_regret = 0\n",
    "        self.emp_regrets = []\n",
    "        # gather the expected regret so far (the expected regret is the best arm from the ground truth (the mu values you set initially) compared to the actual draws)\n",
    "        self.expected_regret = 0\n",
    "        self.exp_regrets = []\n",
    "    \n",
    "    def play_arm(self, arm):\n",
    "        # ----- sample the appropriate reward -----\n",
    "        \n",
    "        # ----- calculate the regret so far and save it -----\n",
    "        \n",
    "        \n",
    "        return reward\n",
    "    \n",
    "    def finished(self):\n",
    "        # ----- return if there is no more round remained -----\n",
    "        return \n",
    "    \n",
    "    def plot_regret(self):\n",
    "        plt.plot(list(range(self.counter)), self.emp_regrets, 'b+', list(range(self.counter)), self.exp_regrets, 'ro')\n",
    "        plt.xlabel(\"iteration\")\n",
    "        plt.ylabel(\"regret\")\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ETC algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ETCsolver:\n",
    "    def __init__(self, k, m, bandit):\n",
    "        self.k = k  # number of arms\n",
    "        self.m = m  # number of exploration rounds for each arm\n",
    "        self.bandit = bandit\n",
    "        \n",
    "        # ----- create a cache storing the number of trials and the average rewards -----\n",
    "        # for each action\n",
    "        self.cache = \n",
    "    \n",
    "    def _exploration_phase(self):\n",
    "        counter = 0\n",
    "        while counter < self.k * self.m:\n",
    "            # ----- implement the exploration part -----\n",
    "            # play the bandit and update cache\n",
    "    \n",
    "    def _choose_best_action(self):\n",
    "        # ----- we calculate the average reward of each arm -----\n",
    "        # return the best arm\n",
    "        \n",
    "        return \n",
    "    \n",
    "    def run(self):\n",
    "        self._exploration_phase()\n",
    "        # after exploration we choose the best action\n",
    "        optimal_arm = self._choose_best_action()\n",
    "        # ----- play until finished -----\n",
    "        \n",
    "    \n",
    "    #def get_regret(self):\n",
    "       # return self.bandit.regrets\n",
    "    \n",
    "    def best_action(self):\n",
    "        return self._choose_best_action() + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def experiment_etc(k, mu, m, rounds):\n",
    "    bandit = Bandit(k, mu, rounds)\n",
    "    etc = ETCsolver(k, m, bandit)\n",
    "\n",
    "    etc.run()\n",
    "    etc.bandit.plot_regret()\n",
    "    print(\"Optimal action: {}\".format(etc.best_action()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## UCB algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UCBsolver:\n",
    "    def __init__(self, k, delta, bandit):\n",
    "        self.k = k  # number of actions\n",
    "        self.delta = delta  # error probability\n",
    "        self.bandit = bandit\n",
    "\n",
    "        self.cache = np.zeros((k, 2))  # stores the number and rewards so far\n",
    "        self.actions = []\n",
    "    \n",
    "    def _init_phase(self):\n",
    "        # at the very beginning each arm is equally good\n",
    "        # unexplored arms are always the best\n",
    "        # ----- pick each arm once to initialize the cache -----\n",
    "        # we want to avoid division by zero\n",
    "        for arm in range(self.k):\n",
    "            # TODO\n",
    "    \n",
    "    def _choose_best_action(self):\n",
    "        # this implements the score for ucb\n",
    "        # ----- first is the average reward term -----\n",
    "        \n",
    "        # ----- second is the exploration term -----\n",
    "\n",
    "        return \n",
    "    \n",
    "    def run(self):\n",
    "        self._init_phase()\n",
    "        # ----- while not finished -----\n",
    "\n",
    "            # ----- chooe optimal arm -----\n",
    "\n",
    "            # ----- storing the actions so far -----\n",
    "\n",
    "            # ----- playing the chosen arm -----\n",
    "\n",
    "            # ----- update cache -----\n",
    "    \n",
    "    def plot_actions(self):\n",
    "        plt.plot(list(range(len(self.actions))), self.actions, 'r+')\n",
    "        plt.xlabel(\"iteration\")\n",
    "        plt.ylabel(\"chosen action\")\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def experiment_ucb(k, mu, delta, rounds):\n",
    "    bandit = Bandit(k, mu, rounds)\n",
    "    ucb = UCBsolver(k, delta, bandit)\n",
    "\n",
    "    ucb.run()\n",
    "    ucb.bandit.plot_regret()\n",
    "    ucb.plot_actions()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ETC use-cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try it with two arms and small difference between the mean values\n",
    "# What are the required number of rounds to find the best action?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try it with two arms and big difference between the mean values\n",
    "# What are the required number of rounds to find the best action?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try it with five arms and small differences among the mean values\n",
    "# What are the required number of rounds to find the best action?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try it with fifty arms and small differences among the mean values\n",
    "# What are the required number of rounds to find the best action?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### UCB use-cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try it with two arms and small difference between the mean values\n",
    "# What are the required number of rounds to find the best action?\n",
    "# How the result changes with delta? \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try it with two arms and big difference between the mean values\n",
    "# What are the required number of rounds to find the best action?\n",
    "# How the result changes with delta? \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try it with five arms and small differences among the mean values\n",
    "# What are the required number of rounds to find the best action?\n",
    "# How the result changes with delta? \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try it with fifty arms and small differences among the mean values\n",
    "# What are the required number of rounds to find the best action?\n",
    "# How the result changes with delta?\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
