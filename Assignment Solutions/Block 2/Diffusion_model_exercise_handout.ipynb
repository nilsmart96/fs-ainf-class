{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5xvQoN-xR6j4"
      },
      "source": [
        "# 1 Importing modules"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "k4WIl8w3v7IU"
      },
      "outputs": [],
      "source": [
        "# Importing relevant modules\n",
        "\n",
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from matplotlib import pyplot as plt\n",
        "from tqdm import tqdm\n",
        "from torch import optim\n",
        "import logging\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "import torchvision\n",
        "from PIL import Image\n",
        "from matplotlib import pyplot as plt\n",
        "from torch.utils.data import DataLoader\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mzjG_GYryIVy",
        "outputId": "dee90066-7d8e-4f75-eeb7-bb3186b1e4ee"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/Users/nilsmart96/Code/fs/ainf/fs-ainf-class/Assignment Solutions/Block 2/Landscapes\n",
            "/Users/nilsmart96/Code/fs/ainf/fs-ainf-class/Assignment Solutions/Block 2/Landscapes/Landscapes\n",
            "zsh:1: no matches found: https://www.dropbox.com/sh/7g4e186l8f43s57/AADHqpUCbTC10AviqG9MqFz2a?dl=0\n",
            "unzip:  cannot find or open Landscapes.zip, Landscapes.zip.zip or Landscapes.zip.ZIP.\n",
            "rm: Landscapes.zip: No such file or directory\n",
            "[Errno 2] No such file or directory: '/content/'\n",
            "/Users/nilsmart96/Code/fs/ainf/fs-ainf-class/Assignment Solutions/Block 2/Landscapes/Landscapes\n"
          ]
        }
      ],
      "source": [
        "# Download data\n",
        "# Careful - if you have downloaded the data once, you must not run this again\n",
        "\n",
        "\n",
        "!mkdir Landscapes\n",
        "%cd Landscapes\n",
        "!mkdir Landscapes\n",
        "%cd Landscapes\n",
        "!wget -O Landscapes.zip -P /content/Landscapes/  https://www.dropbox.com/sh/7g4e186l8f43s57/AADHqpUCbTC10AviqG9MqFz2a?dl=0\n",
        "!unzip Landscapes.zip > /dev/null\n",
        "!rm Landscapes.zip\n",
        "%cd /content/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OmMMK2seOiUu",
        "outputId": "b04d6226-1c7a-4a5f-f128-eb88b4719735"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "#from google.colab import drive\n",
        "#drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "v2DxXOaiBDw1"
      },
      "outputs": [],
      "source": [
        "logging.basicConfig(format=\"%(asctime)s - %(levelname)s: %(message)s\", level=logging.INFO, datefmt=\"%I:%M:%S\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aLvC43EQSA0G"
      },
      "source": [
        "# 2 Forward and Backward diffusion"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1S7Qd_x4ZlTg"
      },
      "source": [
        "We first need to build the inputs for our model, which are more and more noisy images. Instead of doing this sequentially, we can use the closed form to calculate the image for any of the timesteps individually. \n",
        "\n",
        "**Key Takeaways**:\n",
        "- The noise-levels/variances can be pre-computed\n",
        "- There are different types of variance schedules\n",
        "- We can sample each timestep image independently (Sums of Gaussians is also Gaussian)\n",
        "- No model is needed in this forward step"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "L41IWakKBH4b"
      },
      "outputs": [],
      "source": [
        "# Key components of diffusion model - noising schedule, function for noising images, function for sampling images\n",
        "\n",
        "class Diffusion:\n",
        "    def __init__(self, noise_steps=1000, beta_start=1e-4, beta_end=0.02, img_size=256, device=\"cuda\"):\n",
        "        self.noise_steps = noise_steps  # 1000, time steps as proposed in first papes (newer versions need less time steps)\n",
        "        self.beta_start = beta_start  # lower end for beta\n",
        "        self.beta_end = beta_end    # Upper end for beta\n",
        "        self.img_size = img_size   # Image size 64*64\n",
        "        self.device = device        # GPU?\n",
        "\n",
        "        # Linear beta schedule for simplicity (not cosine Schedule)\n",
        "        self.beta = self.prepare_noise_schedule().to(device)\n",
        "        # Task: Define Alpha see lecture notes for definition of alpha\n",
        "        self.alpha = (1 - self.beta).cumprod(dim=0)\n",
        "        # Task: Define Alpha hat - see lecture notes for definition of alpha_hat\n",
        "        self.alpha_hat = self.alpha.cumprod(dim=0)\n",
        "\n",
        "    def prepare_noise_schedule(self):\n",
        "        '''Creates linspace from start beta to end beta with the number of noise_steps) \n",
        "        '''\n",
        "        return torch.linspace(self.beta_start, self.beta_end, self.noise_steps)\n",
        "\n",
        "    def noise_images(self, x, t):\n",
        "        '''Creats noisy versions of images for given timestep\n",
        "           First option is to iteratively add noise over and over until we are at required timestep\n",
        "           However closed form to directly get to timestep used here\n",
        "        '''\n",
        "        sqrt_alpha_hat = torch.sqrt(self.alpha_hat[t])[:, None, None, None]      # take the squareroot of alpha_hat and get into right shape (see lecture notes)\n",
        "        # TASK: required noise factor Required noise factor\n",
        "        sqrt_one_minus_alpha_hat = torch.sqrt(1 - self.alpha_hat[t])[:, None, None, None]\n",
        "        #TASK :generate random numbers from a normal distribution with mean 0 and std 1 with same dimensionality as x\n",
        "        Ɛ = torch.randn_like(x)\n",
        "        return sqrt_alpha_hat * x + sqrt_one_minus_alpha_hat * Ɛ, Ɛ                 # first factor is scaling of the mean, second factor is adding the pure noise scaled by the standard deviation\n",
        "\n",
        "    def sample_timesteps(self, n):\n",
        "        return torch.randint(low=1, high=self.noise_steps, size=(n,))      # sample n timesteps between 1 and the (max) number of noise_steps (which is 1000 in our example)\n",
        "\n",
        "    def sample(self, model, n):\n",
        "        logging.info(f\"Sampling {n} new images....\")\n",
        "        #TASK: Set model to evaluation mode\n",
        "        model.eval()\n",
        "        #Task set no grad, as gradients accumulate (are saved) otherwise leading to out of memory\n",
        "        with torch.no_grad():\n",
        "            x = torch.randn((n, 3, self.img_size, self.img_size)).to(self.device)       # Create initial images sampling from normal distribution using torch.randn\n",
        "            #Task: Loop going over 1000 time steps in reverse order, starting with highest and going until 1\n",
        "            for i in reversed(range(1, self.noise_steps + 1)):\n",
        "                t = (torch.ones(n) * i).long().to(self.device)                          # Creating time-steps by creating tensor of length n\n",
        "                predicted_noise = model(x, t)                                           # Time steps fed into model together with current images\n",
        "                alpha = self.alpha[t][:, None, None, None]\n",
        "                alpha_hat = self.alpha_hat[t][:, None, None, None]\n",
        "                beta = self.beta[t][:, None, None, None]\n",
        "                if i > 1:\n",
        "                    noise = torch.randn_like(x)                                         # Need noise for time steps greater than one (scaled with mean an variance below)\n",
        "                else:\n",
        "                    noise = torch.zeros_like(x)\n",
        "                x = 1 / torch.sqrt(alpha) * (x - ((1 - alpha) / (torch.sqrt(1 - alpha_hat))) * predicted_noise) + torch.sqrt(beta) * noise    # remove a little bit of noise according to formula (Algorithm 2 - sampling ) shown in lecture handout\n",
        "        model.train()                                                                 # set model back to training\n",
        "        x = (x.clamp(-1, 1) + 1) / 2                                                  # clamp outputs into valid range -1 to 1 , plus 1 divided by to is to bring range of values back between 0 and 1\n",
        "        x = (x * 255).type(torch.uint8)                                               # multiply by 255 to bring into valid pixel range\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j6PGFg64oP_a"
      },
      "source": [
        "# 3 Defining the UNet neural network\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2EXnwEvvZ0FI"
      },
      "source": [
        "For a great introduction to UNets, have a look at this post: https://amaarora.github.io/2020/09/13/unet.html.\n",
        "\n",
        "Key Takeaways:\n",
        "\n",
        "- We use a simple form of a UNet for to predict the noise in the image.\n",
        "- The input is a noisy image, the ouput the noise in the image\n",
        "- Because the parameters are shared accross time, we need to tell the network in which timestep we are\n",
        "- The Timestep is encoded by the transformer Sinusoidal Embedding\n",
        "We output one single value (mean), because the variance is fixed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "MwKF24YUBYvD"
      },
      "outputs": [],
      "source": [
        "# The UNet achitecture consists of an encoder and a decoder with a bottleneck in between\n",
        "# The encoder and the decoder each have Double Convolutional layers and downsampling blocks ...\n",
        "# ...(in the case of the encoder) as well as upsampling blocks (in the case of the dencoder)\n",
        "# We are going to implement the double convoluational layers, upsampling and downsampling blocks first and then the UNet architecture\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# Convolution block consisting of \n",
        "\n",
        "class DoubleConv(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, mid_channels=None, residual=False):\n",
        "        super().__init__()\n",
        "        self.residual = residual\n",
        "        if not mid_channels:\n",
        "            mid_channels = out_channels\n",
        "        #Task: Set-up 2d covolution followed by group norm and Gelu activation,followed by another 2d convoluation and a group norm \n",
        "        self.double_conv = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, mid_channels, kernel_size=3, padding=1, stride=1),\n",
        "            nn.GroupNorm(32, mid_channels),\n",
        "            nn.GELU(),\n",
        "            nn.Conv2d(mid_channels, out_channels, kernel_size=3, padding=1, stride=1),\n",
        "            nn.GroupNorm(32, out_channels)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        if self.residual:\n",
        "            return F.gelu(x + self.double_conv(x))\n",
        "        else:\n",
        "            return self.double_conv(x)\n",
        "\n",
        "# Downsample block\n",
        "class Down(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, emb_dim=256):\n",
        "        super().__init__()\n",
        "        #Task: Set-up 2d covolution followed by group norm and Gelu activation,followed by another 2d convoluation and a group norm \n",
        "        self.double_conv = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1, stride=1),\n",
        "            nn.GroupNorm(32, out_channels),\n",
        "            nn.GELU(),\n",
        "            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1, stride=1),\n",
        "            nn.GroupNorm(32, out_channels)\n",
        "        )\n",
        "\n",
        "        self.emb_layer = nn.Sequential(            # linear projection to bring time dimension to right dimension\n",
        "            nn.SiLU(),\n",
        "            nn.Linear(\n",
        "                emb_dim,\n",
        "                out_channels\n",
        "            ),\n",
        "        )\n",
        "\n",
        "    def forward(self, x, t):\n",
        "        x = self.maxpool_conv(x)\n",
        "        emb = self.emb_layer(t)[:, :, None, None].repeat(1, 1, x.shape[-2], x.shape[-1]) \n",
        "        return x + emb\n",
        "\n",
        "\n",
        "# As downsample block but with upsample operation instead of maxpool\n",
        "# Also takes skip connection from encoder\n",
        "# After upsampling X concatenate with skip connection and feed through convoluational block\n",
        "class Up(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, emb_dim=256):\n",
        "        super().__init__()\n",
        "\n",
        "        self.up = nn.Upsample(scale_factor=2, mode=\"bilinear\", align_corners=True)\n",
        "        self.conv = nn.Sequential(\n",
        "            DoubleConv(in_channels, in_channels, residual=True),\n",
        "            DoubleConv(in_channels, out_channels, in_channels // 2),\n",
        "        )\n",
        "\n",
        "        self.emb_layer = nn.Sequential(\n",
        "            nn.SiLU(),\n",
        "            nn.Linear(\n",
        "                emb_dim,\n",
        "                out_channels\n",
        "            ),\n",
        "        )\n",
        "\n",
        "    def forward(self, x, skip_x, t):\n",
        "        x = self.up(x)\n",
        "        x = torch.cat([skip_x, x], dim=1)\n",
        "        x = self.conv(x)\n",
        "        emb = self.emb_layer(t)[:, :, None, None].repeat(1, 1, x.shape[-2], x.shape[-1])\n",
        "        return x + emb\n",
        "\n",
        "\n",
        "# Attention block as used in transformers\n",
        "class SelfAttention(nn.Module):\n",
        "    def __init__(self, channels, size):\n",
        "        super(SelfAttention, self).__init__()\n",
        "        self.channels = channels\n",
        "        self.size = size\n",
        "        # Task: implement multi head attentiona layer with (channels, 4, batch_first=True)\n",
        "        self.mha = nn.MultiheadAttention(embed_dim=channels, num_heads=4, batch_first=True)\n",
        "        self.ln = nn.LayerNorm([channels])\n",
        "        self.ff_self = nn.Sequential(\n",
        "            nn.LayerNorm([channels]),\n",
        "            nn.Linear(channels, channels),\n",
        "            nn.GELU(),\n",
        "            nn.Linear(channels, channels),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.view(-1, self.channels, self.size * self.size).swapaxes(1, 2)\n",
        "        x_ln = self.ln(x)\n",
        "        attention_value, _ = self.mha(x_ln, x_ln, x_ln)\n",
        "        attention_value = attention_value + x\n",
        "        attention_value = self.ff_self(attention_value) + attention_value\n",
        "        return attention_value.swapaxes(2, 1).view(-1, self.channels, self.size, self.size)\n",
        "\n",
        "\n",
        "\n",
        "class UNet(nn.Module):\n",
        "    def __init__(self, c_in=3, c_out=3, time_dim=256, device=\"cuda\"):   #input and output channels both 3 since we have RGB\n",
        "        super().__init__()\n",
        "        self.device = device\n",
        "        self.time_dim = time_dim          # dimension of timestep embedding (see below)\n",
        "        # UNet has encoder, bottleneck and decoder\n",
        "      \n",
        "        self.inc = DoubleConv(c_in, 64)   # Double conv wrapper for two convoluational layers\n",
        "\n",
        "        # three downsample blocks each followed by self attention block\n",
        "        # Args downsample block are  (input channels, output channels)\n",
        "        # Args for self attention are (channel dimension, current input resolution) \n",
        "\n",
        "\n",
        "        #Task: implement 3 downsampling block follows by self attention. Each downsample block reduces size by 2 64(start)-32-16-8, at the same time the number of channels is doubled with each layer and increases to 64-128-256 (it is double with each block)\n",
        "        self.down1 = Down(64, 128)\n",
        "        self.sa1 = SelfAttention(128, 32)\n",
        "        self.down2 = Down(128, 256)\n",
        "        self.sa2 = SelfAttention(256, 16)\n",
        "        self.down3 = Down(256, 512)\n",
        "        self.sa3 = SelfAttention(512, 8)\n",
        "        \n",
        "        # Bottleneck with convolutional layers \n",
        "        self.bot1 = DoubleConv(256, 512)\n",
        "        self.bot2 = DoubleConv(512, 512)\n",
        "        self.bot3 = DoubleConv(512, 256)\n",
        "\n",
        "        # decoder which is revers of encoder\n",
        "        # 3 upsampling blocks followed by attention blocks\n",
        "        #Task: implement 3 upsampling blocks each followed by attention blocsk bringing the size back to 8(start)-16-31-64 - the number of channes decreases 512-256-128 it is halvd with each layer\n",
        "        self.up1 = Up(512, 256, 256)\n",
        "        self.sa4 = SelfAttention(256, 16)\n",
        "        self.up2 = Up(256, 128, 128)\n",
        "        self.sa5 = SelfAttention(128, 32)\n",
        "        self.up3 = Up(128, 64, 64)\n",
        "        self.sa6 = SelfAttention(64, 64)\n",
        "\n",
        "        # 2d convolutional layer  projecting back to output dimensions with args  (final size, c_out, kernel_size=1)\n",
        "        self.outc = nn.Conv2d(64, c_out, kernel_size=1)\n",
        "\n",
        "    def pos_encoding(self, t, channels):  \n",
        "        inv_freq = 1.0 / (\n",
        "            10000\n",
        "            ** (torch.arange(0, channels, 2, device=self.device).float() / channels)\n",
        "        )\n",
        "        pos_enc_a = torch.sin(t.repeat(1, channels // 2) * inv_freq)\n",
        "        pos_enc_b = torch.cos(t.repeat(1, channels // 2) * inv_freq)\n",
        "        pos_enc = torch.cat([pos_enc_a, pos_enc_b], dim=-1)\n",
        "        return pos_enc\n",
        "\n",
        "    # Forward pass takes as input noised images and timesteps (tensor with integer timestep values in it)\n",
        "    def forward(self, x, t):\n",
        "        t = t.unsqueeze(-1).type(torch.float)\n",
        "        t = self.pos_encoding(t, self.time_dim)  # using positional encoding (see NLP Lecture on transformers)\n",
        "        \n",
        "\n",
        "        # Same as init with upsampling blocks taking skip connections from encoder as well\n",
        "        x1 = self.inc(x)\n",
        "        x2 = self.down1(x1, t)\n",
        "        x2 = self.sa1(x2)\n",
        "        x3 = self.down2(x2, t)\n",
        "        x3 = self.sa2(x3)\n",
        "        x4 = self.down3(x3, t)\n",
        "        x4 = self.sa3(x4)\n",
        "\n",
        "        x4 = self.bot1(x4)\n",
        "        x4 = self.bot2(x4)\n",
        "        x4 = self.bot3(x4)\n",
        "\n",
        "        x = self.up1(x4, x3, t)\n",
        "        x = self.sa4(x)\n",
        "        x = self.up2(x, x2, t)\n",
        "        x = self.sa5(x)\n",
        "        x = self.up3(x, x1, t)\n",
        "        x = self.sa6(x)\n",
        "        output = self.outc(x)\n",
        "        return output\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nx1RVkHxSYHr"
      },
      "source": [
        "# 4 Helper functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "jbplrR8uqDoc"
      },
      "outputs": [],
      "source": [
        "def plot_images(images):\n",
        "    plt.figure(figsize=(32, 32))\n",
        "    plt.imshow(torch.cat([\n",
        "        torch.cat([i for i in images.cpu()], dim=-1),\n",
        "    ], dim=-2).permute(1, 2, 0).cpu())\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def save_images(images, path, **kwargs):\n",
        "    grid = torchvision.utils.make_grid(images, **kwargs)\n",
        "    ndarr = grid.permute(1, 2, 0).to('cpu').numpy()\n",
        "    im = Image.fromarray(ndarr)\n",
        "    im.save(path)\n",
        "\n",
        "\n",
        "def get_data(args):\n",
        "    transforms = torchvision.transforms.Compose([\n",
        "        torchvision.transforms.Resize(80),  # args.image_size + 1/4 *args.image_size\n",
        "        torchvision.transforms.RandomResizedCrop(args.image_size, scale=(0.8, 1.0)),\n",
        "        torchvision.transforms.ToTensor(),\n",
        "        torchvision.transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "    ])\n",
        "    dataset = torchvision.datasets.ImageFolder(args.dataset_path, transform=transforms)\n",
        "    dataloader = DataLoader(dataset, batch_size=args.batch_size, shuffle=True)\n",
        "    return dataloader\n",
        "\n",
        "def setup_logging(run_name):\n",
        "    os.makedirs(\"models\", exist_ok=True)\n",
        "    os.makedirs(\"results\", exist_ok=True)\n",
        "    os.makedirs(os.path.join(\"models\", run_name), exist_ok=True)\n",
        "    os.makedirs(os.path.join(\"results\", run_name), exist_ok=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vA5SH-tuScCL"
      },
      "source": [
        "#5 Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kp8aFzNpasFw"
      },
      "source": [
        "**Key Takeaways:**\n",
        "- After some maths we end up with a very simple loss function\n",
        "- There are other possible choices like L2 loss ect.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "_vRTlJjTGh_Z"
      },
      "outputs": [],
      "source": [
        "def train(args):\n",
        "    setup_logging(args.run_name)\n",
        "    device = args.device\n",
        "    dataloader = get_data(args)\n",
        "    model = UNet().to(device)\n",
        "    optimizer = optim.AdamW(model.parameters(), lr=args.lr)\n",
        "    mse = nn.MSELoss()\n",
        "    diffusion = Diffusion(img_size=args.image_size, device=device)\n",
        "    logger = SummaryWriter(os.path.join(\"runs\", args.run_name))\n",
        "    l = len(dataloader)\n",
        "\n",
        "    for epoch in range(args.epochs):    #See algorithm 1 in the lecture handount\n",
        "        logging.info(f\"Starting epoch {epoch}:\")\n",
        "        pbar = tqdm(dataloader)\n",
        "        for i, (images, _) in enumerate(pbar):\n",
        "            images = images.to(device)\n",
        "            t = diffusion.sample_timesteps(images.shape[0]).to(device)\n",
        "            x_t, noise = diffusion.noise_images(images, t)\n",
        "            predicted_noise = model(x_t, t)\n",
        "            loss = mse(noise, predicted_noise)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            pbar.set_postfix(MSE=loss.item())\n",
        "            logger.add_scalar(\"MSE\", loss.item(), global_step=epoch * l + i)\n",
        "\n",
        "        sampled_images = diffusion.sample(model, n=images.shape[0])\n",
        "        save_images(sampled_images, os.path.join(\"results\", args.run_name, f\"{epoch}.jpg\"))\n",
        "        torch.save(model.state_dict(), os.path.join(\"models\", args.run_name, f\"ckpt.pt\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "3oR81xI_IITY"
      },
      "outputs": [],
      "source": [
        "class Args:\n",
        "    def __init__(self):\n",
        "        self.run_name = \"DDPM_Uncondtional\"\n",
        "        self.epochs = 10\n",
        "        self.batch_size = 6\n",
        "        self.image_size = 64\n",
        "        self.dataset_path = r\"/content/Landscapes\"\n",
        "        self.device = \"cuda\"\n",
        "        self.lr = 3e-4\n",
        "\n",
        "def launch():\n",
        "    args = Args()\n",
        "    train(args)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "olFmYcM4ISm4",
        "outputId": "481ef92d-cb16-4d72-fb5c-e1923e200509"
      },
      "outputs": [
        {
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '/Landscapes/Landscapes'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[11], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Note- to get good convergence and reasonable images you would have to train for at least 500 epochs \u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# You can try this out if you have a run-time that does not disconnect\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[43mlaunch\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
            "Cell \u001b[0;32mIn[10], line 13\u001b[0m, in \u001b[0;36mlaunch\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mlaunch\u001b[39m():\n\u001b[1;32m     12\u001b[0m     args \u001b[38;5;241m=\u001b[39m Args()\n\u001b[0;32m---> 13\u001b[0m     \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n",
            "Cell \u001b[0;32mIn[7], line 4\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(args)\u001b[0m\n\u001b[1;32m      2\u001b[0m setup_logging(args\u001b[38;5;241m.\u001b[39mrun_name)\n\u001b[1;32m      3\u001b[0m device \u001b[38;5;241m=\u001b[39m args\u001b[38;5;241m.\u001b[39mdevice\n\u001b[0;32m----> 4\u001b[0m dataloader \u001b[38;5;241m=\u001b[39m \u001b[43mget_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m model \u001b[38;5;241m=\u001b[39m UNet()\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m      6\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m optim\u001b[38;5;241m.\u001b[39mAdamW(model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39margs\u001b[38;5;241m.\u001b[39mlr)\n",
            "Cell \u001b[0;32mIn[6], line 23\u001b[0m, in \u001b[0;36mget_data\u001b[0;34m(args)\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_data\u001b[39m(args):\n\u001b[1;32m     17\u001b[0m     transforms \u001b[38;5;241m=\u001b[39m torchvision\u001b[38;5;241m.\u001b[39mtransforms\u001b[38;5;241m.\u001b[39mCompose([\n\u001b[1;32m     18\u001b[0m         torchvision\u001b[38;5;241m.\u001b[39mtransforms\u001b[38;5;241m.\u001b[39mResize(\u001b[38;5;241m80\u001b[39m),  \u001b[38;5;66;03m# args.image_size + 1/4 *args.image_size\u001b[39;00m\n\u001b[1;32m     19\u001b[0m         torchvision\u001b[38;5;241m.\u001b[39mtransforms\u001b[38;5;241m.\u001b[39mRandomResizedCrop(args\u001b[38;5;241m.\u001b[39mimage_size, scale\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m0.8\u001b[39m, \u001b[38;5;241m1.0\u001b[39m)),\n\u001b[1;32m     20\u001b[0m         torchvision\u001b[38;5;241m.\u001b[39mtransforms\u001b[38;5;241m.\u001b[39mToTensor(),\n\u001b[1;32m     21\u001b[0m         torchvision\u001b[38;5;241m.\u001b[39mtransforms\u001b[38;5;241m.\u001b[39mNormalize((\u001b[38;5;241m0.5\u001b[39m, \u001b[38;5;241m0.5\u001b[39m, \u001b[38;5;241m0.5\u001b[39m), (\u001b[38;5;241m0.5\u001b[39m, \u001b[38;5;241m0.5\u001b[39m, \u001b[38;5;241m0.5\u001b[39m))\n\u001b[1;32m     22\u001b[0m     ])\n\u001b[0;32m---> 23\u001b[0m     dataset \u001b[38;5;241m=\u001b[39m \u001b[43mtorchvision\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdatasets\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mImageFolder\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtransform\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtransforms\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     24\u001b[0m     dataloader \u001b[38;5;241m=\u001b[39m DataLoader(dataset, batch_size\u001b[38;5;241m=\u001b[39margs\u001b[38;5;241m.\u001b[39mbatch_size, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     25\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m dataloader\n",
            "File \u001b[0;32m~/anaconda3/envs/fs_base/lib/python3.11/site-packages/torchvision/datasets/folder.py:309\u001b[0m, in \u001b[0;36mImageFolder.__init__\u001b[0;34m(self, root, transform, target_transform, loader, is_valid_file)\u001b[0m\n\u001b[1;32m    301\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\n\u001b[1;32m    302\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    303\u001b[0m     root: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    307\u001b[0m     is_valid_file: Optional[Callable[[\u001b[38;5;28mstr\u001b[39m], \u001b[38;5;28mbool\u001b[39m]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    308\u001b[0m ):\n\u001b[0;32m--> 309\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    310\u001b[0m \u001b[43m        \u001b[49m\u001b[43mroot\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    311\u001b[0m \u001b[43m        \u001b[49m\u001b[43mloader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    312\u001b[0m \u001b[43m        \u001b[49m\u001b[43mIMG_EXTENSIONS\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mis_valid_file\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    313\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtransform\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    314\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtarget_transform\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtarget_transform\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    315\u001b[0m \u001b[43m        \u001b[49m\u001b[43mis_valid_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_valid_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    316\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimgs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msamples\n",
            "File \u001b[0;32m~/anaconda3/envs/fs_base/lib/python3.11/site-packages/torchvision/datasets/folder.py:144\u001b[0m, in \u001b[0;36mDatasetFolder.__init__\u001b[0;34m(self, root, loader, extensions, transform, target_transform, is_valid_file)\u001b[0m\n\u001b[1;32m    134\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\n\u001b[1;32m    135\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    136\u001b[0m     root: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    141\u001b[0m     is_valid_file: Optional[Callable[[\u001b[38;5;28mstr\u001b[39m], \u001b[38;5;28mbool\u001b[39m]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    142\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    143\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(root, transform\u001b[38;5;241m=\u001b[39mtransform, target_transform\u001b[38;5;241m=\u001b[39mtarget_transform)\n\u001b[0;32m--> 144\u001b[0m     classes, class_to_idx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfind_classes\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mroot\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    145\u001b[0m     samples \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmake_dataset(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mroot, class_to_idx, extensions, is_valid_file)\n\u001b[1;32m    147\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloader \u001b[38;5;241m=\u001b[39m loader\n",
            "File \u001b[0;32m~/anaconda3/envs/fs_base/lib/python3.11/site-packages/torchvision/datasets/folder.py:218\u001b[0m, in \u001b[0;36mDatasetFolder.find_classes\u001b[0;34m(self, directory)\u001b[0m\n\u001b[1;32m    191\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfind_classes\u001b[39m(\u001b[38;5;28mself\u001b[39m, directory: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[List[\u001b[38;5;28mstr\u001b[39m], Dict[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mint\u001b[39m]]:\n\u001b[1;32m    192\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Find the class folders in a dataset structured as follows::\u001b[39;00m\n\u001b[1;32m    193\u001b[0m \n\u001b[1;32m    194\u001b[0m \u001b[38;5;124;03m        directory/\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    216\u001b[0m \u001b[38;5;124;03m        (Tuple[List[str], Dict[str, int]]): List of all classes and dictionary mapping each class to an index.\u001b[39;00m\n\u001b[1;32m    217\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 218\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfind_classes\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdirectory\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/anaconda3/envs/fs_base/lib/python3.11/site-packages/torchvision/datasets/folder.py:40\u001b[0m, in \u001b[0;36mfind_classes\u001b[0;34m(directory)\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfind_classes\u001b[39m(directory: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[List[\u001b[38;5;28mstr\u001b[39m], Dict[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mint\u001b[39m]]:\n\u001b[1;32m     36\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Finds the class folders in a dataset.\u001b[39;00m\n\u001b[1;32m     37\u001b[0m \n\u001b[1;32m     38\u001b[0m \u001b[38;5;124;03m    See :class:`DatasetFolder` for details.\u001b[39;00m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 40\u001b[0m     classes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msorted\u001b[39m(entry\u001b[38;5;241m.\u001b[39mname \u001b[38;5;28;01mfor\u001b[39;00m entry \u001b[38;5;129;01min\u001b[39;00m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscandir\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdirectory\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m entry\u001b[38;5;241m.\u001b[39mis_dir())\n\u001b[1;32m     41\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m classes:\n\u001b[1;32m     42\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCouldn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt find any class folder in \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdirectory\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/Landscapes/Landscapes'"
          ]
        }
      ],
      "source": [
        "# Note- to get good convergence and reasonable images you would have to train for at least 500 epochs \n",
        "# You can try this out if you have a run-time that does not disconnect\n",
        "launch()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": []
    },
    "gpuClass": "premium",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
