{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5xvQoN-xR6j4"
      },
      "source": [
        "# 1 Importing modules"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k4WIl8w3v7IU"
      },
      "outputs": [],
      "source": [
        "# Importing relevant modules\n",
        "\n",
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from matplotlib import pyplot as plt\n",
        "from tqdm import tqdm\n",
        "from torch import optim\n",
        "import logging\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "import torchvision\n",
        "from PIL import Image\n",
        "from matplotlib import pyplot as plt\n",
        "from torch.utils.data import DataLoader\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mzjG_GYryIVy",
        "outputId": "dee90066-7d8e-4f75-eeb7-bb3186b1e4ee"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2023-05-08 10:58:25--  https://www.dropbox.com/sh/7g4e186l8f43s57/AADHqpUCbTC10AviqG9MqFz2a?dl=0\n",
            "Resolving www.dropbox.com (www.dropbox.com)... 162.125.2.18, 2620:100:6031:18::a27d:5112\n",
            "Connecting to www.dropbox.com (www.dropbox.com)|162.125.2.18|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: /sh/raw/7g4e186l8f43s57/AADHqpUCbTC10AviqG9MqFz2a [following]\n",
            "--2023-05-08 10:58:26--  https://www.dropbox.com/sh/raw/7g4e186l8f43s57/AADHqpUCbTC10AviqG9MqFz2a\n",
            "Reusing existing connection to www.dropbox.com:443.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://uc5abd0eb64385f66acbc9bcb0ed.dl.dropboxusercontent.com/zip_download_get/BfCpr_n2joRPZHJcp9w9r3sZt1xgjQ8IZWak-JpjKRR67TZ56BmTSM4N6zJxKgM9TEsIFqXzEr4NPHTRxBozKcCkHZmzSSAEvijxjaB4Dnindg# [following]\n",
            "--2023-05-08 10:58:29--  https://uc5abd0eb64385f66acbc9bcb0ed.dl.dropboxusercontent.com/zip_download_get/BfCpr_n2joRPZHJcp9w9r3sZt1xgjQ8IZWak-JpjKRR67TZ56BmTSM4N6zJxKgM9TEsIFqXzEr4NPHTRxBozKcCkHZmzSSAEvijxjaB4Dnindg\n",
            "Resolving uc5abd0eb64385f66acbc9bcb0ed.dl.dropboxusercontent.com (uc5abd0eb64385f66acbc9bcb0ed.dl.dropboxusercontent.com)... 162.125.81.15, 2620:100:6031:15::a27d:510f\n",
            "Connecting to uc5abd0eb64385f66acbc9bcb0ed.dl.dropboxusercontent.com (uc5abd0eb64385f66acbc9bcb0ed.dl.dropboxusercontent.com)|162.125.81.15|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 656260724 (626M) [application/zip]\n",
            "Saving to: ‘/content/drive/My Drive/AADHqpUCbTC10AviqG9MqFz2a?dl=0.1’\n",
            "\n",
            "AADHqpUCbTC10AviqG9 100%[===================>] 625.86M  13.3MB/s    in 52s     \n",
            "\n",
            "2023-05-08 10:59:23 (12.0 MB/s) - ‘/content/drive/My Drive/AADHqpUCbTC10AviqG9MqFz2a?dl=0.1’ saved [656260724/656260724]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Download data\n",
        "# Careful - if you have downloaded the data once, you must not run this again\n",
        "\n",
        "\n",
        "!mkdir Landscapes\n",
        "%cd Landscapes\n",
        "!mkdir Landscapes\n",
        "%cd Landscapes\n",
        "!wget -O Landscapes.zip -P /content/Landscapes/  https://www.dropbox.com/sh/7g4e186l8f43s57/AADHqpUCbTC10AviqG9MqFz2a?dl=0\n",
        "!unzip Landscapes.zip > /dev/null\n",
        "!rm Landscapes.zip\n",
        "%cd /content/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OmMMK2seOiUu",
        "outputId": "b04d6226-1c7a-4a5f-f128-eb88b4719735"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v2DxXOaiBDw1"
      },
      "outputs": [],
      "source": [
        "logging.basicConfig(format=\"%(asctime)s - %(levelname)s: %(message)s\", level=logging.INFO, datefmt=\"%I:%M:%S\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aLvC43EQSA0G"
      },
      "source": [
        "# 2 Forward and Backward diffusion"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1S7Qd_x4ZlTg"
      },
      "source": [
        "We first need to build the inputs for our model, which are more and more noisy images. Instead of doing this sequentially, we can use the closed form to calculate the image for any of the timesteps individually. \n",
        "\n",
        "**Key Takeaways**:\n",
        "- The noise-levels/variances can be pre-computed\n",
        "- There are different types of variance schedules\n",
        "- We can sample each timestep image independently (Sums of Gaussians is also Gaussian)\n",
        "- No model is needed in this forward step"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L41IWakKBH4b"
      },
      "outputs": [],
      "source": [
        "# Key components of diffusion model - noising schedule, function for noising images, function for sampling images\n",
        "\n",
        "class Diffusion:\n",
        "    def __init__(self, noise_steps=1000, beta_start=1e-4, beta_end=0.02, img_size=256, device=\"cuda\"):\n",
        "        self.noise_steps = noise_steps  # 1000, time steps as proposed in first papes (newer versions need less time steps)\n",
        "        self.beta_start = beta_start  # lower end for beta\n",
        "        self.beta_end = beta_end    # Upper end for beta\n",
        "        self.img_size = img_size   # Image size 64*64\n",
        "        self.device = device        # GPU?\n",
        "\n",
        "        # Linear beta schedule for simplicity (not cosine Schedule)\n",
        "        self.beta = self.prepare_noise_schedule().to(device)\n",
        "        # Task: Define Alpha see lecture notes for definition of alpha\n",
        "        self.alpha = (1 - self.beta).cumprod(dim=0)\n",
        "        # Task: Define Alpha hat - see lecture notes for definition of alpha_hat\n",
        "        self.alpha_hat = self.alpha.cumprod(dim=0)\n",
        "\n",
        "    def prepare_noise_schedule(self):\n",
        "        '''Creates linspace from start beta to end beta with the number of noise_steps) \n",
        "        '''\n",
        "        return torch.linspace(self.beta_start, self.beta_end, self.noise_steps)\n",
        "\n",
        "    def noise_images(self, x, t):\n",
        "        '''Creats noisy versions of images for given timestep\n",
        "           First option is to iteratively add noise over and over until we are at required timestep\n",
        "           However closed form to directly get to timestep used here\n",
        "        '''\n",
        "        sqrt_alpha_hat = torch.sqrt(self.alpha_hat[t])[:, None, None, None]      # take the squareroot of alpha_hat and get into right shape (see lecture notes)\n",
        "        # TASK: required noise factor Required noise factor\n",
        "        sqrt_one_minus_alpha_hat = torch.sqrt(1 - self.alpha_hat[t])[:, None, None, None]\n",
        "        #TASK :generate random numbers from a normal distribution with mean 0 and std 1 with same dimensionality as x\n",
        "        Ɛ = torch.randn_like(x)\n",
        "        return sqrt_alpha_hat * x + sqrt_one_minus_alpha_hat * Ɛ, Ɛ                 # first factor is scaling of the mean, second factor is adding the pure noise scaled by the standard deviation\n",
        "\n",
        "    def sample_timesteps(self, n):\n",
        "        return torch.randint(low=1, high=self.noise_steps, size=(n,))      # sample n timesteps between 1 and the (max) number of noise_steps (which is 1000 in our example)\n",
        "\n",
        "    def sample(self, model, n):\n",
        "        logging.info(f\"Sampling {n} new images....\")\n",
        "        #TASK: Set model to evaluation mode\n",
        "        model.eval()\n",
        "        #Task set no grad, as gradients accumulate (are saved) otherwise leading to out of memory\n",
        "        with torch.no_grad():\n",
        "            x = torch.randn((n, 3, self.img_size, self.img_size)).to(self.device)       # Create initial images sampling from normal distribution using torch.randn\n",
        "            #Task: Loop going over 1000 time steps in reverse order, starting with highest and going until 1\n",
        "            for i in reversed(range(1, self.noise_steps + 1)):\n",
        "                t = (torch.ones(n) * i).long().to(self.device)                          # Creating time-steps by creating tensor of length n\n",
        "                predicted_noise = model(x, t)                                           # Time steps fed into model together with current images\n",
        "                alpha = self.alpha[t][:, None, None, None]\n",
        "                alpha_hat = self.alpha_hat[t][:, None, None, None]\n",
        "                beta = self.beta[t][:, None, None, None]\n",
        "                if i > 1:\n",
        "                    noise = torch.randn_like(x)                                         # Need noise for time steps greater than one (scaled with mean an variance below)\n",
        "                else:\n",
        "                    noise = torch.zeros_like(x)\n",
        "                x = 1 / torch.sqrt(alpha) * (x - ((1 - alpha) / (torch.sqrt(1 - alpha_hat))) * predicted_noise) + torch.sqrt(beta) * noise    # remove a little bit of noise according to formula (Algorithm 2 - sampling ) shown in lecture handout\n",
        "        model.train()                                                                 # set model back to training\n",
        "        x = (x.clamp(-1, 1) + 1) / 2                                                  # clamp outputs into valid range -1 to 1 , plus 1 divided by to is to bring range of values back between 0 and 1\n",
        "        x = (x * 255).type(torch.uint8)                                               # multiply by 255 to bring into valid pixel range\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j6PGFg64oP_a"
      },
      "source": [
        "# 3 Defining the UNet neural network\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2EXnwEvvZ0FI"
      },
      "source": [
        "For a great introduction to UNets, have a look at this post: https://amaarora.github.io/2020/09/13/unet.html.\n",
        "\n",
        "Key Takeaways:\n",
        "\n",
        "- We use a simple form of a UNet for to predict the noise in the image.\n",
        "- The input is a noisy image, the ouput the noise in the image\n",
        "- Because the parameters are shared accross time, we need to tell the network in which timestep we are\n",
        "- The Timestep is encoded by the transformer Sinusoidal Embedding\n",
        "We output one single value (mean), because the variance is fixed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MwKF24YUBYvD"
      },
      "outputs": [],
      "source": [
        "# The UNet achitecture consists of an encoder and a decoder with a bottleneck in between\n",
        "# The encoder and the decoder each have Double Convolutional layers and downsampling blocks ...\n",
        "# ...(in the case of the encoder) as well as upsampling blocks (in the case of the dencoder)\n",
        "# We are going to implement the double convoluational layers, upsampling and downsampling blocks first and then the UNet architecture\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# Convolution block consisting of \n",
        "\n",
        "class DoubleConv(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, mid_channels=None, residual=False):\n",
        "        super().__init__()\n",
        "        self.residual = residual\n",
        "        if not mid_channels:\n",
        "            mid_channels = out_channels\n",
        "        #Task: Set-up 2d covolution followed by group norm and Gelu activation,followed by another 2d convoluation and a group norm \n",
        "        self.double_conv = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, mid_channels, kernel_size=3, padding=1, stride=1),\n",
        "            nn.GroupNorm(32, mid_channels),\n",
        "            nn.GELU(),\n",
        "            nn.Conv2d(mid_channels, out_channels, kernel_size=3, padding=1, stride=1),\n",
        "            nn.GroupNorm(32, out_channels)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        if self.residual:\n",
        "            return F.gelu(x + self.double_conv(x))\n",
        "        else:\n",
        "            return self.double_conv(x)\n",
        "\n",
        "# Downsample block\n",
        "class Down(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, emb_dim=256):\n",
        "        super().__init__()\n",
        "        #Task: Set-up 2d covolution followed by group norm and Gelu activation,followed by another 2d convoluation and a group norm \n",
        "        self.double_conv = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1, stride=1),\n",
        "            nn.GroupNorm(32, out_channels),\n",
        "            nn.GELU(),\n",
        "            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1, stride=1),\n",
        "            nn.GroupNorm(32, out_channels)\n",
        "        )\n",
        "\n",
        "        self.emb_layer = nn.Sequential(            # linear projection to bring time dimension to right dimension\n",
        "            nn.SiLU(),\n",
        "            nn.Linear(\n",
        "                emb_dim,\n",
        "                out_channels\n",
        "            ),\n",
        "        )\n",
        "\n",
        "    def forward(self, x, t):\n",
        "        x = self.maxpool_conv(x)\n",
        "        emb = self.emb_layer(t)[:, :, None, None].repeat(1, 1, x.shape[-2], x.shape[-1]) \n",
        "        return x + emb\n",
        "\n",
        "\n",
        "# As downsample block but with upsample operation instead of maxpool\n",
        "# Also takes skip connection from encoder\n",
        "# After upsampling X concatenate with skip connection and feed through convoluational block\n",
        "class Up(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, emb_dim=256):\n",
        "        super().__init__()\n",
        "\n",
        "        self.up = nn.Upsample(scale_factor=2, mode=\"bilinear\", align_corners=True)\n",
        "        self.conv = nn.Sequential(\n",
        "            DoubleConv(in_channels, in_channels, residual=True),\n",
        "            DoubleConv(in_channels, out_channels, in_channels // 2),\n",
        "        )\n",
        "\n",
        "        self.emb_layer = nn.Sequential(\n",
        "            nn.SiLU(),\n",
        "            nn.Linear(\n",
        "                emb_dim,\n",
        "                out_channels\n",
        "            ),\n",
        "        )\n",
        "\n",
        "    def forward(self, x, skip_x, t):\n",
        "        x = self.up(x)\n",
        "        x = torch.cat([skip_x, x], dim=1)\n",
        "        x = self.conv(x)\n",
        "        emb = self.emb_layer(t)[:, :, None, None].repeat(1, 1, x.shape[-2], x.shape[-1])\n",
        "        return x + emb\n",
        "\n",
        "\n",
        "# Attention block as used in transformers\n",
        "class SelfAttention(nn.Module):\n",
        "    def __init__(self, channels, size):\n",
        "        super(SelfAttention, self).__init__()\n",
        "        self.channels = channels\n",
        "        self.size = size\n",
        "        # Task: implement multi head attentiona layer with (channels, 4, batch_first=True)\n",
        "        self.mha = nn.MultiheadAttention(embed_dim=channels, num_heads=4, batch_first=True)\n",
        "        self.ln = nn.LayerNorm([channels])\n",
        "        self.ff_self = nn.Sequential(\n",
        "            nn.LayerNorm([channels]),\n",
        "            nn.Linear(channels, channels),\n",
        "            nn.GELU(),\n",
        "            nn.Linear(channels, channels),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.view(-1, self.channels, self.size * self.size).swapaxes(1, 2)\n",
        "        x_ln = self.ln(x)\n",
        "        attention_value, _ = self.mha(x_ln, x_ln, x_ln)\n",
        "        attention_value = attention_value + x\n",
        "        attention_value = self.ff_self(attention_value) + attention_value\n",
        "        return attention_value.swapaxes(2, 1).view(-1, self.channels, self.size, self.size)\n",
        "\n",
        "\n",
        "\n",
        "class UNet(nn.Module):\n",
        "    def __init__(self, c_in=3, c_out=3, time_dim=256, device=\"cuda\"):   #input and output channels both 3 since we have RGB\n",
        "        super().__init__()\n",
        "        self.device = device\n",
        "        self.time_dim = time_dim          # dimension of timestep embedding (see below)\n",
        "        # UNet has encoder, bottleneck and decoder\n",
        "      \n",
        "        self.inc = DoubleConv(c_in, 64)   # Double conv wrapper for two convoluational layers\n",
        "\n",
        "        # three downsample blocks each followed by self attention block\n",
        "        # Args downsample block are  (input channels, output channels)\n",
        "        # Args for self attention are (channel dimension, current input resolution) \n",
        "\n",
        "\n",
        "        #Task: implement 3 downsampling block follows by self attention. Each downsample block reduces size by 2 64(start)-32-16-8, at the same time the number of channels is doubled with each layer and increases to 64-128-256 (it is double with each block)\n",
        "        self.down1 = Down(64, 128)\n",
        "        self.sa1 = SelfAttention(128, 32)\n",
        "        self.down2 = Down(128, 256)\n",
        "        self.sa2 = SelfAttention(256, 16)\n",
        "        self.down3 = Down(256, 512)\n",
        "        self.sa3 = SelfAttention(512, 8)\n",
        "        \n",
        "        # Bottleneck with convolutional layers \n",
        "        self.bot1 = DoubleConv(256, 512)\n",
        "        self.bot2 = DoubleConv(512, 512)\n",
        "        self.bot3 = DoubleConv(512, 256)\n",
        "\n",
        "        # decoder which is revers of encoder\n",
        "        # 3 upsampling blocks followed by attention blocks\n",
        "        #Task: implement 3 upsampling blocks each followed by attention blocsk bringing the size back to 8(start)-16-31-64 - the number of channes decreases 512-256-128 it is halvd with each layer\n",
        "        self.up1 = Up(512, 256, 256)\n",
        "        self.sa4 = SelfAttention(256, 16)\n",
        "        self.up2 = Up(256, 128, 128)\n",
        "        self.sa5 = SelfAttention(128, 32)\n",
        "        self.up3 = Up(128, 64, 64)\n",
        "        self.sa6 = SelfAttention(64, 64)\n",
        "\n",
        "        # 2d convolutional layer  projecting back to output dimensions with args  (final size, c_out, kernel_size=1)\n",
        "        self.outc = nn.Conv2d(64, c_out, kernel_size=1)\n",
        "\n",
        "    def pos_encoding(self, t, channels):  \n",
        "        inv_freq = 1.0 / (\n",
        "            10000\n",
        "            ** (torch.arange(0, channels, 2, device=self.device).float() / channels)\n",
        "        )\n",
        "        pos_enc_a = torch.sin(t.repeat(1, channels // 2) * inv_freq)\n",
        "        pos_enc_b = torch.cos(t.repeat(1, channels // 2) * inv_freq)\n",
        "        pos_enc = torch.cat([pos_enc_a, pos_enc_b], dim=-1)\n",
        "        return pos_enc\n",
        "\n",
        "    # Forward pass takes as input noised images and timesteps (tensor with integer timestep values in it)\n",
        "    def forward(self, x, t):\n",
        "        t = t.unsqueeze(-1).type(torch.float)\n",
        "        t = self.pos_encoding(t, self.time_dim)  # using positional encoding (see NLP Lecture on transformers)\n",
        "        \n",
        "\n",
        "        # Same as init with upsampling blocks taking skip connections from encoder as well\n",
        "        x1 = self.inc(x)\n",
        "        x2 = self.down1(x1, t)\n",
        "        x2 = self.sa1(x2)\n",
        "        x3 = self.down2(x2, t)\n",
        "        x3 = self.sa2(x3)\n",
        "        x4 = self.down3(x3, t)\n",
        "        x4 = self.sa3(x4)\n",
        "\n",
        "        x4 = self.bot1(x4)\n",
        "        x4 = self.bot2(x4)\n",
        "        x4 = self.bot3(x4)\n",
        "\n",
        "        x = self.up1(x4, x3, t)\n",
        "        x = self.sa4(x)\n",
        "        x = self.up2(x, x2, t)\n",
        "        x = self.sa5(x)\n",
        "        x = self.up3(x, x1, t)\n",
        "        x = self.sa6(x)\n",
        "        output = self.outc(x)\n",
        "        return output\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nx1RVkHxSYHr"
      },
      "source": [
        "# 4 Helper functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jbplrR8uqDoc"
      },
      "outputs": [],
      "source": [
        "def plot_images(images):\n",
        "    plt.figure(figsize=(32, 32))\n",
        "    plt.imshow(torch.cat([\n",
        "        torch.cat([i for i in images.cpu()], dim=-1),\n",
        "    ], dim=-2).permute(1, 2, 0).cpu())\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def save_images(images, path, **kwargs):\n",
        "    grid = torchvision.utils.make_grid(images, **kwargs)\n",
        "    ndarr = grid.permute(1, 2, 0).to('cpu').numpy()\n",
        "    im = Image.fromarray(ndarr)\n",
        "    im.save(path)\n",
        "\n",
        "\n",
        "def get_data(args):\n",
        "    transforms = torchvision.transforms.Compose([\n",
        "        torchvision.transforms.Resize(80),  # args.image_size + 1/4 *args.image_size\n",
        "        torchvision.transforms.RandomResizedCrop(args.image_size, scale=(0.8, 1.0)),\n",
        "        torchvision.transforms.ToTensor(),\n",
        "        torchvision.transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "    ])\n",
        "    dataset = torchvision.datasets.ImageFolder(args.dataset_path, transform=transforms)\n",
        "    dataloader = DataLoader(dataset, batch_size=args.batch_size, shuffle=True)\n",
        "    return dataloader\n",
        "\n",
        "def setup_logging(run_name):\n",
        "    os.makedirs(\"models\", exist_ok=True)\n",
        "    os.makedirs(\"results\", exist_ok=True)\n",
        "    os.makedirs(os.path.join(\"models\", run_name), exist_ok=True)\n",
        "    os.makedirs(os.path.join(\"results\", run_name), exist_ok=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vA5SH-tuScCL"
      },
      "source": [
        "#5 Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kp8aFzNpasFw"
      },
      "source": [
        "**Key Takeaways:**\n",
        "- After some maths we end up with a very simple loss function\n",
        "- There are other possible choices like L2 loss ect.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_vRTlJjTGh_Z"
      },
      "outputs": [],
      "source": [
        "def train(args):\n",
        "    setup_logging(args.run_name)\n",
        "    device = args.device\n",
        "    dataloader = get_data(args)\n",
        "    model = UNet().to(device)\n",
        "    optimizer = optim.AdamW(model.parameters(), lr=args.lr)\n",
        "    mse = nn.MSELoss()\n",
        "    diffusion = Diffusion(img_size=args.image_size, device=device)\n",
        "    logger = SummaryWriter(os.path.join(\"runs\", args.run_name))\n",
        "    l = len(dataloader)\n",
        "\n",
        "    for epoch in range(args.epochs):    #See algorithm 1 in the lecture handount\n",
        "        logging.info(f\"Starting epoch {epoch}:\")\n",
        "        pbar = tqdm(dataloader)\n",
        "        for i, (images, _) in enumerate(pbar):\n",
        "            images = images.to(device)\n",
        "            t = diffusion.sample_timesteps(images.shape[0]).to(device)\n",
        "            x_t, noise = diffusion.noise_images(images, t)\n",
        "            predicted_noise = model(x_t, t)\n",
        "            loss = mse(noise, predicted_noise)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            pbar.set_postfix(MSE=loss.item())\n",
        "            logger.add_scalar(\"MSE\", loss.item(), global_step=epoch * l + i)\n",
        "\n",
        "        sampled_images = diffusion.sample(model, n=images.shape[0])\n",
        "        save_images(sampled_images, os.path.join(\"results\", args.run_name, f\"{epoch}.jpg\"))\n",
        "        torch.save(model.state_dict(), os.path.join(\"models\", args.run_name, f\"ckpt.pt\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3oR81xI_IITY"
      },
      "outputs": [],
      "source": [
        "class Args:\n",
        "    def __init__(self):\n",
        "        self.run_name = \"DDPM_Uncondtional\"\n",
        "        self.epochs = 10\n",
        "        self.batch_size = 6\n",
        "        self.image_size = 64\n",
        "        self.dataset_path = r\"/content/gdrive/MyDrive/Landscapes\"\n",
        "        self.device = \"cuda\"\n",
        "        self.lr = 3e-4\n",
        "\n",
        "def launch():\n",
        "    args = Args()\n",
        "    train(args)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "olFmYcM4ISm4",
        "outputId": "481ef92d-cb16-4d72-fb5c-e1923e200509"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 720/720 [04:37<00:00,  2.59it/s, MSE=0.021]\n",
            "999it [00:20, 48.89it/s]\n",
            "100%|██████████| 720/720 [02:19<00:00,  5.17it/s, MSE=0.0242]\n",
            "999it [00:20, 48.89it/s]\n",
            "100%|██████████| 720/720 [02:18<00:00,  5.19it/s, MSE=0.0297]\n",
            "999it [00:20, 49.10it/s]\n",
            "100%|██████████| 720/720 [02:19<00:00,  5.15it/s, MSE=0.0257]\n",
            "999it [00:20, 48.92it/s]\n",
            "100%|██████████| 720/720 [02:17<00:00,  5.22it/s, MSE=0.00994]\n",
            "999it [00:20, 48.77it/s]\n",
            "100%|██████████| 720/720 [02:19<00:00,  5.17it/s, MSE=0.00419]\n",
            "999it [00:20, 48.94it/s]\n",
            "100%|██████████| 720/720 [02:18<00:00,  5.19it/s, MSE=0.0269]\n",
            "999it [00:20, 48.74it/s]\n",
            "100%|██████████| 720/720 [02:19<00:00,  5.17it/s, MSE=0.0134]\n",
            "999it [00:20, 49.04it/s]\n",
            "100%|██████████| 720/720 [02:18<00:00,  5.18it/s, MSE=0.00634]\n",
            "999it [00:20, 48.92it/s]\n",
            "100%|██████████| 720/720 [02:18<00:00,  5.20it/s, MSE=0.031]\n",
            "999it [00:20, 48.90it/s]\n"
          ]
        }
      ],
      "source": [
        "# Note- to get good convergence and reasonable images you would have to train for at least 500 epochs \n",
        "# You can try this out if you have a run-time that does not disconnect\n",
        "launch()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": []
    },
    "gpuClass": "premium",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
