{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RJEIW9AK8Yqj"
   },
   "source": [
    "# Overall layout of the assignment\n",
    "Overview: In this assignment you are first going to implement a convolutional graph neural network and then an attention graph neural network by \"hand\"\n",
    "\n",
    "Thereafter you are going to implement both using PyTorch Geometric as well as training them on a simple task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7n4XoftJ9HFa"
   },
   "source": [
    "# Graph Convoluational Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OqbpnGlrBPz8",
    "outputId": "7be459bc-3e5c-41b9-f93a-bef18ee81058"
   },
   "outputs": [],
   "source": [
    "# Installing relevant modules\n",
    "#!pip install pytorch_lightning\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vzETpNCowGvh",
    "outputId": "6056f94a-1f8b-4b76-b560-a0f90957747c"
   },
   "outputs": [],
   "source": [
    "#!pip install networx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "kT-iQ8Ot8UeO"
   },
   "outputs": [],
   "source": [
    "# Import relevan libraries\n",
    "import os\n",
    "import math\n",
    "import numpy as np\n",
    "\n",
    "# Flor plotting\n",
    "import matplotlib.pyplot as plt\n",
    "#import seaborn as sns\n",
    "import tqdm\n",
    "\n",
    "# pytorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as data\n",
    "import torch.optim as optim\n",
    "\n",
    "#torchvision\n",
    "import torchvision\n",
    "from torchvision.datasets import CIFAR10\n",
    "from torchvision import transforms\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import LearningRateMonitor, ModelCheckpoint\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PpaW7zdxCL2n"
   },
   "source": [
    "# Graph Convolutions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "pK4L-EbAAzPw"
   },
   "outputs": [],
   "source": [
    "# Pytorch module for Graph Convolutions\n",
    "#TASK_Create a class CGNLayer inhereting class nn.Module (imported from Torch)\n",
    "#__________#\n",
    "class GCNLayer(nn.Module):\n",
    "    def __init__(self, c_in, c_out):                  # c_in : dimension of input feature, c_out : dimension of output feature\n",
    "        super().__init__()\n",
    "        #TASK: Add a linear projection for input features\n",
    "        self.projection = nn.Linear(c_in, c_out) \n",
    "\n",
    "    def forward (self, node_feats, adj_matrix):\n",
    "        '''\n",
    "        Inputs:\n",
    "        node_feats = Tensor with node featuers of the shape[batch_size, num_nodes, c_in]\n",
    "        adj_matrix  = Batch adjecency matrix of the graph - if we are considering self node we will provide 1s on leading Diagonal\n",
    "        shape:[batch_size, num_nodes, num_nodes]\n",
    "        '''\n",
    "    \n",
    "        # num_neighbours = number of incoming edges\n",
    "        num_neighbours = adj_matrix.sum(dim =-1, keepdims =True)    # Find nubmer of neighbours needed for normalization\n",
    "        # TASK: linear projection of the original node features using \"self.projection\"\n",
    "        node_feats_transformed = self.projection(node_feats)\n",
    "        # TASK: Batchwise matrix multiplication with dimensionality (adj_matrix, node_feats)  - since only nodes with featues have a 1 this results in summing over the linearly projected node featues \n",
    "        node_feats_aggregated = torch.bmm(adj_matrix, node_feats_transformed)\n",
    "        # TASK: Normalization (taking into account the number of adjacent nodes)\n",
    "        node_feats = node_feats_aggregated / num_neighbours.clamp(min=1)\n",
    "    \n",
    "        return node_feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 337
    },
    "id": "N3ZjNKG-kWGy",
    "outputId": "1e9307da-c17d-4f04-e491-40e312db7a5c"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x300 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Let's apply our Convolutional Graph Layer to an example graph\n",
    "# Creating and plotting a sample graph\n",
    "\n",
    "import networkx as nx\n",
    "\n",
    "# creating a graph\n",
    "nx_g = nx.from_edgelist([(2,1), (2,3), (4,2), (3,4)])   # creates as touples indicating the connections between two nodes \n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "#nx.draw(g, with_labels=true)\n",
    "#Plt.show()\n",
    "\n",
    "fig = plt.figure(figsize = (8,3))\n",
    "nx.draw(nx_g, with_labels=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aOpC8StBv3EY",
    "outputId": "a50462ef-f345-4904-e694-6e00ead9c1be"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "node featues:\n",
      " tensor([[[0., 1.],\n",
      "         [2., 3.],\n",
      "         [4., 5.],\n",
      "         [6., 7.]]])\n",
      "\n",
      "Adjacency matrix:\n",
      " tensor([[[1., 1., 0., 0.],\n",
      "         [1., 1., 1., 1.],\n",
      "         [0., 1., 1., 1.],\n",
      "         [0., 1., 1., 1.]]])\n"
     ]
    }
   ],
   "source": [
    "# Defining nodes featues with torch.arrange\n",
    "\n",
    "node_feats = torch.arange(8, dtype = torch.float32).view(1,4,2)   # creating some featurs\n",
    "\n",
    "adj_matrix = torch.Tensor([[[1, 1, 0, 0],     # hand based translation of the graph we saw above (could be done using networkx as well)\n",
    "                            [1, 1, 1, 1],\n",
    "                            [0, 1, 1, 1],\n",
    "                            [0, 1, 1, 1]]])  # leading diagonal always 1, implying that the self node is also a feature \n",
    "\n",
    "print(\"node featues:\\n\", node_feats)\n",
    "print(\"\\nAdjacency matrix:\\n\", adj_matrix)\n",
    "                                                      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b6KDUqbjy71L",
    "outputId": "145b223c-b564-494c-a3a3-6fbf1fa0cd68"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adjacency matrix tensor([[[1., 1., 0., 0.],\n",
      "         [1., 1., 1., 1.],\n",
      "         [0., 1., 1., 1.],\n",
      "         [0., 1., 1., 1.]]])\n",
      "Input feature tensor([[[0., 1.],\n",
      "         [2., 3.],\n",
      "         [4., 5.],\n",
      "         [6., 7.]]])\n",
      "Output feaures tensor([[[1., 2.],\n",
      "         [3., 4.],\n",
      "         [4., 5.],\n",
      "         [4., 5.]]])\n"
     ]
    }
   ],
   "source": [
    "# Now let's apply a GCN layer to the graph: for simplicity we inialize the linear weight matrix as identity matrix so that input featues are equal to the message\n",
    "\n",
    "layer = GCNLayer(c_in=2, c_out=2)   # model\n",
    "\n",
    "layer.projection.weight.data = torch.Tensor([[1, 0], [0, 1]])\n",
    "layer.projection.bias.data = torch.Tensor([0, 0])\n",
    "\n",
    "with torch.no_grad():                       # .no_grad(), since we are doing a simple forward pass\n",
    "  out_feats = layer(node_feats, adj_matrix) # passingthe features\n",
    "\n",
    "print(\"Adjacency matrix\", adj_matrix)\n",
    "print(\"Input feature\", node_feats)\n",
    "print(\"Output feaures\", out_feats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TCDVd9D72djD"
   },
   "source": [
    "# Graph Attention Neural Network - GAT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "Ugr0QfHV5Rx6"
   },
   "outputs": [],
   "source": [
    "class GATLayer(nn.Module):  # inheriting nn.module\n",
    "  def __init__(self, c_in, c_out, num_heads=1, concat_heads=True, alpha=0.2):\n",
    "    super().__init__()\n",
    "\n",
    "    '''Inputs:\n",
    "          c_in = dimensionality of input features\n",
    "          c_out = dimensionality of output features\n",
    "\n",
    "          num_heads = number of heads in attention mechanism  - if bigger > 1 multihead attention\n",
    "          concat = if True output of all heads will be concatenated instead of averaging\n",
    "          alpha = negative slope for leaky ReLu\n",
    "    '''\n",
    "\n",
    "    \n",
    "    self.num_heads = num_heads\n",
    "    self.concat_heads = concat_heads\n",
    "\n",
    "    if self.concat_heads:               # if we want to concatenate heads instead of taking the average \n",
    "      assert c_out % num_heads == 0;    \"number of output featues must be the multiple of the count of heads\"\n",
    "      c_out = c_out // num_heads\n",
    "\n",
    "\n",
    "    # sub module and parameters needed in the layer\n",
    "    # TASK: create first linear projection ouput whiche depends on number of heads we want attention for\n",
    "    self.projection = nn.Linear(c_in, c_out * num_heads)\n",
    "    # TASK:  Initalize weight one weith matrix per head with dimensionality 2*c_out \n",
    "    self.a = nn.Parameter(torch.zeros(size=(2 * c_out, num_heads)))\n",
    "    # TASK: Intialize leaky relu activation with alpha passed as parameter\n",
    "    self.leakyrelu = nn.LeakyReLU(alpha)\n",
    "\n",
    "    #Initialization\n",
    "    nn.init.xavier_uniform_(self.projection.weight.data, gain = 1.414)   # xavier uniform initialization\n",
    "    nn.init.xavier_uniform_(self.a.data, gain = 1.414)\n",
    "\n",
    "  def forward(self, node_feats, adj_matrix, print_attn_probs = False):\n",
    "\n",
    "    batch_size, num_nodes = node_feats.size(0), node_feats.size(1)     # calculating batch size and number of node featues \n",
    "\n",
    "    #apply linear layer\n",
    "    # TASK: Projecting the node features as for GNNs\n",
    "    node_feats = self.projection(node_feats)\n",
    "    node_feats = node_feats.view(batch_size, num_nodes, self.num_heads, -1)   # Convert into batch size, number of nodes and number of heads\n",
    "\n",
    "    # we need to calculate the attention logits for every edge in the adjacency matrix\n",
    "    # doing this on all possible nodes will be very expensive\n",
    "    #TASK: return indices where the adjacency matrix is not zero\n",
    "    edges = adj_matrix.nonzero(as_tuple=False)\n",
    "\n",
    "    node_feats_flat = node_feats.view(batch_size*num_nodes, self.num_heads, -1)    # Flattening out node features\n",
    "\n",
    "    edge_indices_row = edges[:, 0] #TASK: Pick the edge indices of the row\n",
    "    edge_indices_col = edges[:, 1] #TASK:  Pick the edge indices of the column\n",
    "    \n",
    "    # Concatenating the features of two nodes\n",
    "    a_input =torch.cat([\n",
    "        torch.index_select(input = node_feats_flat, index = edge_indices_row, dim =0),\n",
    "        torch.index_select(input = node_feats_flat, index = edge_indices_col, dim =0)\n",
    "    ], dim=-1)   #Index select returns a tensor with node_feats_flat being indexed as the desired position along dim =0 \n",
    "\n",
    "    # Calculate attention MLP output (independen for each head)\n",
    "  \n",
    "    attn_logits = torch.einsum('bhc, hc ->bh', a_input, self.a)  # passing our input a_input and weights self.a\n",
    "    \n",
    "    attn_logits = self.leakyrelu(attn_logits) #TASK: apply leaky relu to attention logits\n",
    "\n",
    "    # Map list of attention values back into matrix\n",
    "    attn_matrix = attn_logits.new_zeros(adj_matrix.shape + (self.num_heads,)).fill_(-9e15)   # creating matrix with zeros\n",
    "    attn_matrix[adj_matrix[...,None].repeat(1,1,1,self.num_heads)==1] = attn_logits.reshape(-1) # Filling with the attention values for relevant indices\n",
    "\n",
    "    # Weighted average of attention\n",
    "    attn_probs = F.softmax(attn_matrix, dim=1) # TASK: pass attn_matrix through softmax\n",
    "  \n",
    "    if print_attn_probs:\n",
    "      print(\"Attention probs\\n\", attn_probs.permute(0, 3, 1, 2))\n",
    "    node_feats = torch.einsum('bijh, bjhc -> bihc', attn_probs, node_feats)\n",
    "\n",
    "    # If heads should be concatenated, we cna do this by reshaping. Otherwise, take the mean\n",
    "    if self.concat_heads:\n",
    "      node_feats = node_feats.reshape(batch_size,num_nodes, -1)\n",
    "    else:\n",
    "      node_feats = node_feats.mean(dim=2)\n",
    "\n",
    "    return node_feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ODrDOxMeQRWC",
    "outputId": "dd88d081-bad9-4981-8466-622f9ca98959"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adjacency matrix tensor([[[1., 1., 0., 0.],\n",
      "         [1., 1., 1., 1.],\n",
      "         [0., 1., 1., 1.],\n",
      "         [0., 1., 1., 1.]]])\n",
      "Input feature tensor([[[0., 1.],\n",
      "         [2., 3.],\n",
      "         [4., 5.],\n",
      "         [6., 7.]]])\n",
      "Output feaures tensor([[[0.1640, 1.3056],\n",
      "         [1.9270, 5.4154],\n",
      "         [3.5112, 4.7323],\n",
      "         [6.3978, 4.5467]]])\n"
     ]
    }
   ],
   "source": [
    "# instantiating the GATNN and doing a forward pass\n",
    "layer = GATLayer(2,2, num_heads =2, concat_heads=True) \n",
    "\n",
    "\n",
    "layer.projection.weight.data = torch.Tensor([[1, 0], [0, 1]])\n",
    "layer.projection.bias.data = torch.Tensor([0, 0])\n",
    "\n",
    "\n",
    "layer.a.data = torch.Tensor([[-0.2, 0.3],[0.1,-0.1]])\n",
    "\n",
    "with torch.no_grad():                       # .no_grad(), since we are doing a simple forward pass\n",
    "  out_feats = layer(node_feats, adj_matrix, print_attn_probs = False) # passing the features\n",
    "\n",
    "print(\"Adjacency matrix\", adj_matrix)\n",
    "print(\"Input feature\", node_feats)\n",
    "print(\"Output feaures\", out_feats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Uu_vDiSrK1G-"
   },
   "source": [
    "# Using Pytorch Geometric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jZhIpLTlLaxL",
    "outputId": "668e8021-5d59-400c-8e05-068eaa219694"
   },
   "outputs": [],
   "source": [
    "#!pip install torch_geometric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "PZ4IChcCbkGe"
   },
   "outputs": [],
   "source": [
    "import torch_geometric\n",
    "import torch_geometric.nn as g_nn\n",
    "import torch_geometric.data as g_data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TG7BoMjnLg7x"
   },
   "source": [
    "## Example of node level task: Semi supervised node classification\n",
    "\n",
    "A popular example that we will use here is the Cora dataset, a citation network among papers. The Cora dataset consists of 2708 scientific publications with links between each other representing the citation of one paper by another. The task is to classify each publication into one of seven classes. Each publication is represented by a bag-of-words vector. This means that we have a vector of 1433 elements for each publication, where 1 for a feature indicates that the i-th word of a pre-defined dicationary is present in this article."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "if-2A8tuLXha",
    "outputId": "d04580eb-1955-4c7c-f0f8-e148eeefad9b"
   },
   "outputs": [],
   "source": [
    "# Load the dataset and normalized features\n",
    "from torch_geometric.transforms import NormalizeFeatures \n",
    "from torch_geometric.datasets import Planetoid # needed to get the dataset\n",
    "#TASK: Load troch_geometic dataset \"Cora\" ind save in \"Pytorch_Practice_Models\", use  NormalizeFeatures() as transform\n",
    "dataset = Planetoid(root='./Pytorch_Practice_Models', name='Cora', transform=NormalizeFeatures())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xgPWDFM4ND3I",
    "outputId": "42f0e494-d444-43db-a2b2-95e8b9ec3c9c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Data(x=[2708, 1433], edge_index=[2, 10556], y=[2708], train_mask=[2708], val_mask=[2708], test_mask=[2708])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = dataset[0]\n",
    "data\n",
    "\n",
    "# 2708 nodes (publications)\n",
    "# Every node has 1433 features\n",
    "# edge indexes show what nodes are connected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dvK1juCcNNYs",
    "outputId": "a5620972-ca58-4e9c-81c3-a4232da7d3ab"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GCN(\n",
      "  (conv1): GCNConv(1433, 16)\n",
      "  (conv2): GCNConv(16, 7)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# GCN using pytorch-geometric library\n",
    "from torch_geometric.nn import GCNConv    # Import GCNConv\n",
    "\n",
    "# TASK: instantiate class GCN as subclass of nn.Module\n",
    "class GCN(nn.Module):\n",
    "  def __init__(self, hidden_channels):\n",
    "    super().__init__()\n",
    "    torch.manual_seed(1234)\n",
    "    # TASK: set up first convolutional GCN layer  with (dataset.num_features, hidden_channels) as dimensionality\n",
    "    self.conv1 = GCNConv(dataset.num_features, hidden_channels)\n",
    "    # TASK: second convoluational GCN layer (maps to the output ), (hidden_channels, dataset.num_classes) as dimensionality\n",
    "    self.conv2 = GCNConv(hidden_channels, dataset.num_classes)\n",
    "\n",
    "  def forward(self, x, edge_index):\n",
    "     #TASK: conv 1 layer\n",
    "    x = self.conv1(x, edge_index)\n",
    "    #TASK: apply relu activation\n",
    "    x = F.relu(x)\n",
    "    #TASK: dropout with p=0.5 \n",
    "    x = F.dropout(x, p=0.5, training=self.training)\n",
    "    #TASL:  second conv layer\n",
    "    x = self.conv2(x, edge_index)\n",
    "    return x\n",
    "\n",
    "model = GCN(hidden_channels  =16)     # Take 16 hidden channels/ hidden features\n",
    "print(model)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "toPoYjDpOged",
    "outputId": "6cd4bdff-d071-4e79-9b69-58635750e630"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 001, Loss: 1.9461\n",
      "Epoch: 002, Loss: 1.9414\n",
      "Epoch: 003, Loss: 1.9343\n",
      "Epoch: 004, Loss: 1.9274\n",
      "Epoch: 005, Loss: 1.9199\n",
      "Epoch: 006, Loss: 1.9132\n",
      "Epoch: 007, Loss: 1.8977\n",
      "Epoch: 008, Loss: 1.8931\n",
      "Epoch: 009, Loss: 1.8791\n",
      "Epoch: 010, Loss: 1.8689\n",
      "Epoch: 011, Loss: 1.8581\n",
      "Epoch: 012, Loss: 1.8449\n",
      "Epoch: 013, Loss: 1.8346\n",
      "Epoch: 014, Loss: 1.8211\n",
      "Epoch: 015, Loss: 1.8139\n",
      "Epoch: 016, Loss: 1.7876\n",
      "Epoch: 017, Loss: 1.7789\n",
      "Epoch: 018, Loss: 1.7678\n",
      "Epoch: 019, Loss: 1.7491\n",
      "Epoch: 020, Loss: 1.7218\n",
      "Epoch: 021, Loss: 1.7238\n",
      "Epoch: 022, Loss: 1.6986\n",
      "Epoch: 023, Loss: 1.6777\n",
      "Epoch: 024, Loss: 1.6534\n",
      "Epoch: 025, Loss: 1.6534\n",
      "Epoch: 026, Loss: 1.6279\n",
      "Epoch: 027, Loss: 1.6144\n",
      "Epoch: 028, Loss: 1.5818\n",
      "Epoch: 029, Loss: 1.5823\n",
      "Epoch: 030, Loss: 1.5491\n",
      "Epoch: 031, Loss: 1.5195\n",
      "Epoch: 032, Loss: 1.5177\n",
      "Epoch: 033, Loss: 1.4970\n",
      "Epoch: 034, Loss: 1.4455\n",
      "Epoch: 035, Loss: 1.4448\n",
      "Epoch: 036, Loss: 1.4392\n",
      "Epoch: 037, Loss: 1.3998\n",
      "Epoch: 038, Loss: 1.3734\n",
      "Epoch: 039, Loss: 1.3814\n",
      "Epoch: 040, Loss: 1.3734\n",
      "Epoch: 041, Loss: 1.3261\n",
      "Epoch: 042, Loss: 1.3221\n",
      "Epoch: 043, Loss: 1.2769\n",
      "Epoch: 044, Loss: 1.2493\n",
      "Epoch: 045, Loss: 1.2482\n",
      "Epoch: 046, Loss: 1.2279\n",
      "Epoch: 047, Loss: 1.2146\n",
      "Epoch: 048, Loss: 1.1750\n",
      "Epoch: 049, Loss: 1.1339\n",
      "Epoch: 050, Loss: 1.1775\n",
      "Epoch: 051, Loss: 1.1202\n",
      "Epoch: 052, Loss: 1.1047\n",
      "Epoch: 053, Loss: 1.1437\n",
      "Epoch: 054, Loss: 1.0542\n",
      "Epoch: 055, Loss: 1.0546\n",
      "Epoch: 056, Loss: 1.0692\n",
      "Epoch: 057, Loss: 1.0661\n",
      "Epoch: 058, Loss: 1.0397\n",
      "Epoch: 059, Loss: 1.0274\n",
      "Epoch: 060, Loss: 0.9797\n",
      "Epoch: 061, Loss: 0.9071\n",
      "Epoch: 062, Loss: 0.9533\n",
      "Epoch: 063, Loss: 0.9268\n",
      "Epoch: 064, Loss: 0.9105\n",
      "Epoch: 065, Loss: 0.8874\n",
      "Epoch: 066, Loss: 0.8473\n",
      "Epoch: 067, Loss: 0.9054\n",
      "Epoch: 068, Loss: 0.8263\n",
      "Epoch: 069, Loss: 0.8237\n",
      "Epoch: 070, Loss: 0.8480\n",
      "Epoch: 071, Loss: 0.8296\n",
      "Epoch: 072, Loss: 0.8028\n",
      "Epoch: 073, Loss: 0.7921\n",
      "Epoch: 074, Loss: 0.8237\n",
      "Epoch: 075, Loss: 0.7894\n",
      "Epoch: 076, Loss: 0.7824\n",
      "Epoch: 077, Loss: 0.7705\n",
      "Epoch: 078, Loss: 0.7426\n",
      "Epoch: 079, Loss: 0.7532\n",
      "Epoch: 080, Loss: 0.7036\n",
      "Epoch: 081, Loss: 0.7399\n",
      "Epoch: 082, Loss: 0.7096\n",
      "Epoch: 083, Loss: 0.6660\n",
      "Epoch: 084, Loss: 0.6864\n",
      "Epoch: 085, Loss: 0.6552\n",
      "Epoch: 086, Loss: 0.6739\n",
      "Epoch: 087, Loss: 0.6386\n",
      "Epoch: 088, Loss: 0.6788\n",
      "Epoch: 089, Loss: 0.6302\n",
      "Epoch: 090, Loss: 0.6447\n",
      "Epoch: 091, Loss: 0.6285\n",
      "Epoch: 092, Loss: 0.6549\n",
      "Epoch: 093, Loss: 0.6687\n",
      "Epoch: 094, Loss: 0.6534\n",
      "Epoch: 095, Loss: 0.6117\n",
      "Epoch: 096, Loss: 0.5606\n",
      "Epoch: 097, Loss: 0.5899\n",
      "Epoch: 098, Loss: 0.6019\n",
      "Epoch: 099, Loss: 0.5692\n",
      "Epoch: 100, Loss: 0.5674\n",
      "Epoch: 101, Loss: 0.5760\n",
      "Epoch: 102, Loss: 0.5387\n",
      "Epoch: 103, Loss: 0.5790\n",
      "Epoch: 104, Loss: 0.5219\n",
      "Epoch: 105, Loss: 0.5613\n",
      "Epoch: 106, Loss: 0.5544\n",
      "Epoch: 107, Loss: 0.5520\n",
      "Epoch: 108, Loss: 0.5132\n",
      "Epoch: 109, Loss: 0.5199\n",
      "Epoch: 110, Loss: 0.5348\n",
      "Epoch: 111, Loss: 0.5427\n",
      "Epoch: 112, Loss: 0.5076\n",
      "Epoch: 113, Loss: 0.5660\n",
      "Epoch: 114, Loss: 0.5524\n",
      "Epoch: 115, Loss: 0.5364\n",
      "Epoch: 116, Loss: 0.5083\n",
      "Epoch: 117, Loss: 0.5024\n",
      "Epoch: 118, Loss: 0.4788\n",
      "Epoch: 119, Loss: 0.5105\n",
      "Epoch: 120, Loss: 0.4749\n",
      "Epoch: 121, Loss: 0.4901\n",
      "Epoch: 122, Loss: 0.4747\n",
      "Epoch: 123, Loss: 0.4472\n",
      "Epoch: 124, Loss: 0.4948\n",
      "Epoch: 125, Loss: 0.4204\n",
      "Epoch: 126, Loss: 0.5191\n",
      "Epoch: 127, Loss: 0.4387\n",
      "Epoch: 128, Loss: 0.4990\n",
      "Epoch: 129, Loss: 0.4640\n",
      "Epoch: 130, Loss: 0.4722\n",
      "Epoch: 131, Loss: 0.4436\n",
      "Epoch: 132, Loss: 0.5177\n",
      "Epoch: 133, Loss: 0.4450\n",
      "Epoch: 134, Loss: 0.4298\n",
      "Epoch: 135, Loss: 0.4434\n",
      "Epoch: 136, Loss: 0.4249\n",
      "Epoch: 137, Loss: 0.4531\n",
      "Epoch: 138, Loss: 0.4388\n",
      "Epoch: 139, Loss: 0.4002\n",
      "Epoch: 140, Loss: 0.4122\n",
      "Epoch: 141, Loss: 0.4693\n",
      "Epoch: 142, Loss: 0.4537\n",
      "Epoch: 143, Loss: 0.4537\n",
      "Epoch: 144, Loss: 0.4257\n",
      "Epoch: 145, Loss: 0.4291\n",
      "Epoch: 146, Loss: 0.3870\n",
      "Epoch: 147, Loss: 0.4318\n",
      "Epoch: 148, Loss: 0.4481\n",
      "Epoch: 149, Loss: 0.4341\n",
      "Epoch: 150, Loss: 0.4305\n",
      "Epoch: 151, Loss: 0.3794\n",
      "Epoch: 152, Loss: 0.3971\n",
      "Epoch: 153, Loss: 0.3983\n",
      "Epoch: 154, Loss: 0.3827\n",
      "Epoch: 155, Loss: 0.4371\n",
      "Epoch: 156, Loss: 0.4224\n",
      "Epoch: 157, Loss: 0.3695\n",
      "Epoch: 158, Loss: 0.3924\n",
      "Epoch: 159, Loss: 0.3929\n",
      "Epoch: 160, Loss: 0.3726\n",
      "Epoch: 161, Loss: 0.4028\n",
      "Epoch: 162, Loss: 0.4171\n",
      "Epoch: 163, Loss: 0.3812\n",
      "Epoch: 164, Loss: 0.4003\n",
      "Epoch: 165, Loss: 0.3805\n",
      "Epoch: 166, Loss: 0.3601\n",
      "Epoch: 167, Loss: 0.3425\n",
      "Epoch: 168, Loss: 0.3876\n",
      "Epoch: 169, Loss: 0.3605\n",
      "Epoch: 170, Loss: 0.3781\n",
      "Epoch: 171, Loss: 0.3661\n",
      "Epoch: 172, Loss: 0.3857\n",
      "Epoch: 173, Loss: 0.4135\n",
      "Epoch: 174, Loss: 0.3488\n",
      "Epoch: 175, Loss: 0.3741\n",
      "Epoch: 176, Loss: 0.3883\n",
      "Epoch: 177, Loss: 0.3789\n",
      "Epoch: 178, Loss: 0.3434\n",
      "Epoch: 179, Loss: 0.3428\n",
      "Epoch: 180, Loss: 0.3289\n",
      "Epoch: 181, Loss: 0.3576\n",
      "Epoch: 182, Loss: 0.3590\n",
      "Epoch: 183, Loss: 0.3724\n",
      "Epoch: 184, Loss: 0.3542\n",
      "Epoch: 185, Loss: 0.3279\n",
      "Epoch: 186, Loss: 0.3681\n",
      "Epoch: 187, Loss: 0.3564\n",
      "Epoch: 188, Loss: 0.3428\n",
      "Epoch: 189, Loss: 0.3503\n",
      "Epoch: 190, Loss: 0.3307\n",
      "Epoch: 191, Loss: 0.3263\n",
      "Epoch: 192, Loss: 0.3452\n",
      "Epoch: 193, Loss: 0.3393\n",
      "Epoch: 194, Loss: 0.3078\n",
      "Epoch: 195, Loss: 0.3416\n",
      "Epoch: 196, Loss: 0.3617\n",
      "Epoch: 197, Loss: 0.3226\n",
      "Epoch: 198, Loss: 0.3442\n",
      "Epoch: 199, Loss: 0.3660\n"
     ]
    }
   ],
   "source": [
    "model = GCN(hidden_channels = 16)   #instantiating model\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay =5e-4) # Adam optimizer with appropriate learning rate and cross entropy loss\n",
    "criterion = torch.nn.CrossEntropyLoss()  #\n",
    "\n",
    "def train():\n",
    "  #TASK: put model in training mode\n",
    "  model.train()\n",
    "   #TASK: clear gradients \n",
    "  optimizer.zero_grad()\n",
    "  out = model(data.x, data.edge_index)  #perform a single forward pass \n",
    "  loss = criterion(out[data.train_mask], data.y[data.train_mask]) #compute the loss solely based on the training nodes \n",
    "  #TASK: derive gradients (backward pass)\n",
    "  loss.backward()\n",
    "  #TASK: Update parameters based on gradient\n",
    "  optimizer.step()\n",
    "  return loss\n",
    "\n",
    "\n",
    "def test():\n",
    "  model.eval()\n",
    "  out = model(data.x, data.edge_index) \n",
    "  pred = out.argmax(dim=1) # use the class with the highest probability\n",
    "  test_correct = pred[data.test_mask] == data.y[data.test_mask] # check against ground-truth labels\n",
    "  test_acc = int(test_correct.sum()) /int(data.test_mask.sum())  # Derive ratio of correct predictions\n",
    "  return test_acc\n",
    "\n",
    "for epoch in range(1,200):\n",
    "  loss = train()\n",
    "  print(f'Epoch: {epoch:03d}, Loss: {loss:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ws8dwHbwYdvt",
    "outputId": "67edfc26-8166-4eba-8db8-8af14c639422"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.8190\n"
     ]
    }
   ],
   "source": [
    "# Observing performance on test set\n",
    "test_acc = test()\n",
    "print(f'Test Accuracy: {test_acc:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "z0dJ66JBwznc",
    "outputId": "d13cabcf-eb19-4533-faf0-6a7602a82330"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GAT(\n",
      "  (conv1): GATConv(1433, 8, heads=1)\n",
      "  (conv2): GATConv(8, 7, heads=1)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Geaph Attention Neural Network\n",
    "\n",
    "from torch_geometric.nn import GATConv\n",
    "\n",
    "class GAT(torch.nn.Module):\n",
    "  def __init__(self, hidden_channels, heads):\n",
    "    super().__init__()\n",
    "    torch.manual_seed(1234)\n",
    "    # TASK: first convolutional GAT layer      (takes the input)\n",
    "    self.conv1 = GATConv(dataset.num_features, hidden_channels, heads=heads)\n",
    "    # TASK: second convoluational GAT layer (maps to the output )\n",
    "    self.conv2 = GATConv(hidden_channels * heads, dataset.num_classes, heads=heads, concat=False)\n",
    "\n",
    "  def forward(self, x, edge_index):\n",
    "    #TASK: dropout p=0.6 use dropout\n",
    "    x = F.dropout(x, p=0.6, training=self.training)\n",
    "    #TASK: GAT 1 layer\n",
    "    x = self.conv1(x, edge_index)\n",
    "    #TAKS: apply elu activation\n",
    "    x = F.elu(x)\n",
    "    #TASK: use dropout, p=0.6\n",
    "    x = F.dropout(x, p=0.6, training=self.training)\n",
    "    #TASK apply second GAT layer\n",
    "    x = self.conv2(x, edge_index)\n",
    "    return x\n",
    "\n",
    "model = GAT(hidden_channels  =8, heads=1)     # Take 8 hidden channels/ hidden features and 8 heads\n",
    "print(model)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fQp3rpSuyBna",
    "outputId": "c29aee9c-5547-4c63-f3bf-a6910fa1396f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 001, Loss: 1.9464\n",
      "Epoch: 002, Loss: 1.9449\n",
      "Epoch: 003, Loss: 1.9420\n",
      "Epoch: 004, Loss: 1.9399\n",
      "Epoch: 005, Loss: 1.9383\n",
      "Epoch: 006, Loss: 1.9364\n",
      "Epoch: 007, Loss: 1.9334\n",
      "Epoch: 008, Loss: 1.9321\n",
      "Epoch: 009, Loss: 1.9300\n",
      "Epoch: 010, Loss: 1.9260\n",
      "Epoch: 011, Loss: 1.9224\n",
      "Epoch: 012, Loss: 1.9198\n",
      "Epoch: 013, Loss: 1.9149\n",
      "Epoch: 014, Loss: 1.9105\n",
      "Epoch: 015, Loss: 1.9054\n",
      "Epoch: 016, Loss: 1.9000\n",
      "Epoch: 017, Loss: 1.8957\n",
      "Epoch: 018, Loss: 1.8895\n",
      "Epoch: 019, Loss: 1.8840\n",
      "Epoch: 020, Loss: 1.8679\n",
      "Epoch: 021, Loss: 1.8635\n",
      "Epoch: 022, Loss: 1.8621\n",
      "Epoch: 023, Loss: 1.8441\n",
      "Epoch: 024, Loss: 1.8417\n",
      "Epoch: 025, Loss: 1.8267\n",
      "Epoch: 026, Loss: 1.8137\n",
      "Epoch: 027, Loss: 1.8036\n",
      "Epoch: 028, Loss: 1.7961\n",
      "Epoch: 029, Loss: 1.7789\n",
      "Epoch: 030, Loss: 1.7607\n",
      "Epoch: 031, Loss: 1.7546\n",
      "Epoch: 032, Loss: 1.7424\n",
      "Epoch: 033, Loss: 1.7065\n",
      "Epoch: 034, Loss: 1.6983\n",
      "Epoch: 035, Loss: 1.6620\n",
      "Epoch: 036, Loss: 1.6643\n",
      "Epoch: 037, Loss: 1.6518\n",
      "Epoch: 038, Loss: 1.6213\n",
      "Epoch: 039, Loss: 1.6040\n",
      "Epoch: 040, Loss: 1.5940\n",
      "Epoch: 041, Loss: 1.5664\n",
      "Epoch: 042, Loss: 1.5505\n",
      "Epoch: 043, Loss: 1.5387\n",
      "Epoch: 044, Loss: 1.4798\n",
      "Epoch: 045, Loss: 1.4692\n",
      "Epoch: 046, Loss: 1.4713\n",
      "Epoch: 047, Loss: 1.4323\n",
      "Epoch: 048, Loss: 1.4026\n",
      "Epoch: 049, Loss: 1.3823\n",
      "Epoch: 050, Loss: 1.3675\n",
      "Epoch: 051, Loss: 1.3144\n",
      "Epoch: 052, Loss: 1.2971\n",
      "Epoch: 053, Loss: 1.2750\n",
      "Epoch: 054, Loss: 1.2382\n",
      "Epoch: 055, Loss: 1.2125\n",
      "Epoch: 056, Loss: 1.2093\n",
      "Epoch: 057, Loss: 1.1873\n",
      "Epoch: 058, Loss: 1.1700\n",
      "Epoch: 059, Loss: 1.1312\n",
      "Epoch: 060, Loss: 1.1277\n",
      "Epoch: 061, Loss: 1.0846\n",
      "Epoch: 062, Loss: 1.0692\n",
      "Epoch: 063, Loss: 1.0170\n",
      "Epoch: 064, Loss: 0.9915\n",
      "Epoch: 065, Loss: 1.0266\n",
      "Epoch: 066, Loss: 0.9711\n",
      "Epoch: 067, Loss: 0.9325\n",
      "Epoch: 068, Loss: 0.9195\n",
      "Epoch: 069, Loss: 0.9239\n",
      "Epoch: 070, Loss: 0.8745\n",
      "Epoch: 071, Loss: 0.8342\n",
      "Epoch: 072, Loss: 0.8673\n",
      "Epoch: 073, Loss: 0.8374\n",
      "Epoch: 074, Loss: 0.8208\n",
      "Epoch: 075, Loss: 0.7774\n",
      "Epoch: 076, Loss: 0.7682\n",
      "Epoch: 077, Loss: 0.7494\n",
      "Epoch: 078, Loss: 0.7188\n",
      "Epoch: 079, Loss: 0.7518\n",
      "Epoch: 080, Loss: 0.7302\n",
      "Epoch: 081, Loss: 0.7597\n",
      "Epoch: 082, Loss: 0.6405\n",
      "Epoch: 083, Loss: 0.7032\n",
      "Epoch: 084, Loss: 0.6571\n",
      "Epoch: 085, Loss: 0.6400\n",
      "Epoch: 086, Loss: 0.6384\n",
      "Epoch: 087, Loss: 0.6626\n",
      "Epoch: 088, Loss: 0.6448\n",
      "Epoch: 089, Loss: 0.6058\n",
      "Epoch: 090, Loss: 0.5963\n",
      "Epoch: 091, Loss: 0.6111\n",
      "Epoch: 092, Loss: 0.6229\n",
      "Epoch: 093, Loss: 0.5869\n",
      "Epoch: 094, Loss: 0.5698\n",
      "Epoch: 095, Loss: 0.6183\n",
      "Epoch: 096, Loss: 0.5284\n",
      "Epoch: 097, Loss: 0.6068\n",
      "Epoch: 098, Loss: 0.5477\n",
      "Epoch: 099, Loss: 0.5685\n",
      "Epoch: 100, Loss: 0.5545\n",
      "Epoch: 101, Loss: 0.5578\n",
      "Epoch: 102, Loss: 0.5139\n",
      "Epoch: 103, Loss: 0.5711\n",
      "Epoch: 104, Loss: 0.5473\n",
      "Epoch: 105, Loss: 0.5492\n",
      "Epoch: 106, Loss: 0.5233\n",
      "Epoch: 107, Loss: 0.4718\n",
      "Epoch: 108, Loss: 0.4825\n",
      "Epoch: 109, Loss: 0.5114\n",
      "Epoch: 110, Loss: 0.5251\n",
      "Epoch: 111, Loss: 0.4804\n",
      "Epoch: 112, Loss: 0.4957\n",
      "Epoch: 113, Loss: 0.5207\n",
      "Epoch: 114, Loss: 0.4819\n",
      "Epoch: 115, Loss: 0.5054\n",
      "Epoch: 116, Loss: 0.4989\n",
      "Epoch: 117, Loss: 0.4377\n",
      "Epoch: 118, Loss: 0.4801\n",
      "Epoch: 119, Loss: 0.4908\n",
      "Epoch: 120, Loss: 0.4657\n",
      "Epoch: 121, Loss: 0.4749\n",
      "Epoch: 122, Loss: 0.5265\n",
      "Epoch: 123, Loss: 0.4725\n",
      "Epoch: 124, Loss: 0.4903\n",
      "Epoch: 125, Loss: 0.4761\n",
      "Epoch: 126, Loss: 0.4618\n",
      "Epoch: 127, Loss: 0.4800\n",
      "Epoch: 128, Loss: 0.4588\n",
      "Epoch: 129, Loss: 0.4423\n",
      "Epoch: 130, Loss: 0.4539\n",
      "Epoch: 131, Loss: 0.4585\n",
      "Epoch: 132, Loss: 0.4172\n",
      "Epoch: 133, Loss: 0.4621\n",
      "Epoch: 134, Loss: 0.4845\n",
      "Epoch: 135, Loss: 0.4716\n",
      "Epoch: 136, Loss: 0.4259\n",
      "Epoch: 137, Loss: 0.4381\n",
      "Epoch: 138, Loss: 0.4349\n",
      "Epoch: 139, Loss: 0.4501\n",
      "Epoch: 140, Loss: 0.4353\n",
      "Epoch: 141, Loss: 0.4310\n",
      "Epoch: 142, Loss: 0.3972\n",
      "Epoch: 143, Loss: 0.4138\n",
      "Epoch: 144, Loss: 0.4065\n",
      "Epoch: 145, Loss: 0.3897\n",
      "Epoch: 146, Loss: 0.4625\n",
      "Epoch: 147, Loss: 0.4783\n",
      "Epoch: 148, Loss: 0.4187\n",
      "Epoch: 149, Loss: 0.4589\n",
      "Epoch: 150, Loss: 0.4114\n",
      "Epoch: 151, Loss: 0.4481\n",
      "Epoch: 152, Loss: 0.4085\n",
      "Epoch: 153, Loss: 0.4016\n",
      "Epoch: 154, Loss: 0.4128\n",
      "Epoch: 155, Loss: 0.4617\n",
      "Epoch: 156, Loss: 0.4870\n",
      "Epoch: 157, Loss: 0.4346\n",
      "Epoch: 158, Loss: 0.4128\n",
      "Epoch: 159, Loss: 0.4570\n",
      "Epoch: 160, Loss: 0.4015\n",
      "Epoch: 161, Loss: 0.4108\n",
      "Epoch: 162, Loss: 0.4316\n",
      "Epoch: 163, Loss: 0.4360\n",
      "Epoch: 164, Loss: 0.4292\n",
      "Epoch: 165, Loss: 0.4705\n",
      "Epoch: 166, Loss: 0.3845\n",
      "Epoch: 167, Loss: 0.4682\n",
      "Epoch: 168, Loss: 0.3801\n",
      "Epoch: 169, Loss: 0.4237\n",
      "Epoch: 170, Loss: 0.3515\n",
      "Epoch: 171, Loss: 0.4420\n",
      "Epoch: 172, Loss: 0.3972\n",
      "Epoch: 173, Loss: 0.4515\n",
      "Epoch: 174, Loss: 0.3844\n",
      "Epoch: 175, Loss: 0.4498\n",
      "Epoch: 176, Loss: 0.4171\n",
      "Epoch: 177, Loss: 0.3938\n",
      "Epoch: 178, Loss: 0.4104\n",
      "Epoch: 179, Loss: 0.4316\n",
      "Epoch: 180, Loss: 0.4002\n",
      "Epoch: 181, Loss: 0.3569\n",
      "Epoch: 182, Loss: 0.3880\n",
      "Epoch: 183, Loss: 0.4168\n",
      "Epoch: 184, Loss: 0.4006\n",
      "Epoch: 185, Loss: 0.4003\n",
      "Epoch: 186, Loss: 0.4631\n",
      "Epoch: 187, Loss: 0.3827\n",
      "Epoch: 188, Loss: 0.4117\n",
      "Epoch: 189, Loss: 0.3766\n",
      "Epoch: 190, Loss: 0.3987\n",
      "Epoch: 191, Loss: 0.3748\n",
      "Epoch: 192, Loss: 0.3919\n",
      "Epoch: 193, Loss: 0.3907\n",
      "Epoch: 194, Loss: 0.3798\n",
      "Epoch: 195, Loss: 0.4237\n",
      "Epoch: 196, Loss: 0.3521\n",
      "Epoch: 197, Loss: 0.3622\n",
      "Epoch: 198, Loss: 0.3622\n",
      "Epoch: 199, Loss: 0.4040\n"
     ]
    }
   ],
   "source": [
    "model = GAT(hidden_channels  =8, heads=8)  #instantiating model\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.005, weight_decay =5e-4) # Adam optimizer with appropriate learning rate and cross entropy loss\n",
    "criterion = torch.nn.CrossEntropyLoss()  #\n",
    "\n",
    "def train():\n",
    "  # TASK: set model in training mode \n",
    "  model.train()\n",
    "  # TASK:  clear gradients \n",
    "  optimizer.zero_grad()\n",
    "  out = model(data.x, data.edge_index)  #perform a single forward pass \n",
    "  loss = criterion(out[data.train_mask], data.y[data.train_mask]) #compute the loss solely based on the training nodes \n",
    "  #TASK: derive gradients (backward pass)\n",
    "  loss.backward()\n",
    "  #TASK Update parameters based on gradient\n",
    "  optimizer.step()\n",
    "  return loss\n",
    "\n",
    "\n",
    "def test():\n",
    "  model.eval()\n",
    "  out = model(data.x, data.edge_index) \n",
    "  pred = out.argmax(dim=1) # use the class with the highest probability\n",
    "  test_correct = pred[data.test_mask] == data.y[data.test_mask] # check against ground-truth labels\n",
    "  test_acc = int(test_correct.sum()) /int(data.test_mask.sum())  # Derive ratio of correct predictions\n",
    "  return test_acc\n",
    "\n",
    " \n",
    "for epoch in range(1,200):\n",
    "  loss = train()\n",
    "  print(f'Epoch: {epoch:03d}, Loss: {loss:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4wDuugxfy7t8",
    "outputId": "822280e4-efaf-4d01-a36c-36182d8e8df0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.8170\n"
     ]
    }
   ],
   "source": [
    "# Observing performance on test set\n",
    "test_acc = test()\n",
    "print(f'Test Accuracy: {test_acc:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aZIYr14hDwKX"
   },
   "source": [
    "Summary: in this context and the particular set-up graph convolutions are more effective than graph attention. Propose (argue for), implement and evaluate two potential improvements for the graph attention neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Jtji0EdOzDdf"
   },
   "source": [
    "In the scenario where graph convolutions are found more effective than graph attention mechanisms, it could be due to several reasons such as the architectures inability to leverage the flexibility provided by attention mechanisms effectively, or perhaps due to the problem domain and data characteristics that favor convolutions for feature aggregation over attention mechanisms. Here are two proposed improvements to enhance the performance of a Graph Attention Network (GAT):\n",
    "\n",
    "### 1. **Multi-Head Attention with Diverse Initialization**\n",
    "**Proposition**: The effectiveness of the attention mechanism can be greatly enhanced by using multi-head attention, where each head learns a different aspect of the data. Diverse initialization of each attention head might encourage exploration of different representational spaces, potentially capturing a wider range of node feature relationships.\n",
    "\n",
    "**Implementation Suggestion**:\n",
    "- Increase the number of heads in the GAT layer while ensuring that each head starts with a unique initial state. This could be achieved by varying the initialization parameters across heads.\n",
    "- Consider using orthogonal or sparse initialization techniques which might encourage each head to develop different and potentially complementary representations of the input features.\n",
    "\n",
    "**Evaluation Approach**:\n",
    "- Test the network's performance with varying numbers of attention heads and types of initializations.\n",
    "- Compare the results against a baseline model to see if there's an improvement in performance metrics like accuracy on a validation set or a specific task-related metric.\n",
    "\n",
    "### 2. **Incorporate Edge Features in Attention Mechanisms**\n",
    "**Proposition**: Standard GATs do not explicitly incorporate edge features, which can provide crucial contextual information about the relationship between nodes. Modifying the GAT to account for edge features could enhance its ability to model inter-node relationships and improve performance.\n",
    "\n",
    "**Implementation Suggestion**:\n",
    "- Modify the attention mechanism to include edge features in the computation of attention coefficients. This can be done by extending the attention mechanism's input to include edge attributes, which influence the computation of attention scores between nodes.\n",
    "- The attention function could be redefined as \\( e_{ij} = a(\\mathbf{W}[h_i \\| h_j \\| e_{ij}]) \\), where \\( e_{ij} \\) are the edge features between nodes \\( i \\) and \\( j \\), and \\( a \\) is a learnable function (e.g., a neural network).\n",
    "\n",
    "**Evaluation Approach**:\n",
    "- Evaluate the model on datasets where edge features are known to be informative (e.g., molecular graphs where edge types represent different bond types).\n",
    "- Compare performance metrics (like classification accuracy or regression error) against a version of the model that does not use edge features.\n",
    "\n",
    "### General Evaluation Setup\n",
    "For both improvements, a rigorous evaluation would involve:\n",
    "- Training the modified GAT models on a standard dataset where baseline performance is established.\n",
    "- Using k-fold cross-validation or a held-out test set to assess performance.\n",
    "- Analyzing the results using metrics such as accuracy, F1 score, or ROC-AUC for classification tasks. For regression or other types of tasks, appropriate metrics like RMSE or MAE should be used.\n",
    "- Optionally, visualize attention scores or learned embeddings to qualitatively assess how the model is interpreting the graph structure.\n",
    "\n",
    "These improvements aim to leverage the flexibility and capacity of GATs more effectively, potentially overcoming scenarios where traditional GCN architectures perform better."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
