{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RJEIW9AK8Yqj"
   },
   "source": [
    "# Overall layout of the assignment\n",
    "Overview: In this assignment you are first going to implement a convolutional graph neural network and then an attention graph neural network by \"hand\"\n",
    "\n",
    "Thereafter you are going to implement both using PyTorch Geometric as well as training them on a simple task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7n4XoftJ9HFa"
   },
   "source": [
    "# Graph Convoluational Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OqbpnGlrBPz8",
    "outputId": "7be459bc-3e5c-41b9-f93a-bef18ee81058"
   },
   "outputs": [],
   "source": [
    "# Installing relevant modules\n",
    "#!pip install pytorch_lightning\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vzETpNCowGvh",
    "outputId": "6056f94a-1f8b-4b76-b560-a0f90957747c"
   },
   "outputs": [],
   "source": [
    "#!pip install networx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "kT-iQ8Ot8UeO"
   },
   "outputs": [],
   "source": [
    "# Import relevan libraries\n",
    "import os\n",
    "import math\n",
    "import numpy as np\n",
    "\n",
    "# Flor plotting\n",
    "import matplotlib.pyplot as plt\n",
    "#import seaborn as sns\n",
    "import tqdm\n",
    "\n",
    "# pytorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as data\n",
    "import torch.optim as optim\n",
    "\n",
    "#torchvision\n",
    "import torchvision\n",
    "from torchvision.datasets import CIFAR10\n",
    "from torchvision import transforms\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import LearningRateMonitor, ModelCheckpoint\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PpaW7zdxCL2n"
   },
   "source": [
    "# Graph Convolutions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "pK4L-EbAAzPw"
   },
   "outputs": [],
   "source": [
    "# Pytorch module for Graph Convolutions\n",
    "#TASK_Create a class CGNLayer inhereting class nn.Module (imported from Torch)\n",
    "#__________#\n",
    "class GCNLayer(nn.Module):\n",
    "    def __init__(self, c_in, c_out):                  # c_in : dimension of input feature, c_out : dimension of output feature\n",
    "        super().__init__()\n",
    "        #TASK: Add a linear projection for input features\n",
    "        self.projection = nn.Linear(c_in, c_out) \n",
    "\n",
    "    def forward (self, node_feats, adj_matrix):\n",
    "        '''\n",
    "        Inputs:\n",
    "        node_feats = Tensor with node featuers of the shape[batch_size, num_nodes, c_in]\n",
    "        adj_matrix  = Batch adjecency matrix of the graph - if we are considering self node we will provide 1s on leading Diagonal\n",
    "        shape:[batch_size, num_nodes, num_nodes]\n",
    "        '''\n",
    "    \n",
    "        # num_neighbours = number of incoming edges\n",
    "        num_neighbours = adj_matrix.sum(dim =-1, keepdims =True)    # Find nubmer of neighbours needed for normalization\n",
    "        # TASK: linear projection of the original node features using \"self.projection\"\n",
    "        node_feats_transformed = self.projection(node_feats)\n",
    "        # TASK: Batchwise matrix multiplication with dimensionality (adj_matrix, node_feats)  - since only nodes with featues have a 1 this results in summing over the linearly projected node featues \n",
    "        node_feats_aggregated = torch.bmm(adj_matrix, node_feats_transformed)\n",
    "        # TASK: Normalization (taking into account the number of adjacent nodes)\n",
    "        node_feats = node_feats_aggregated / num_neighbours.clamp(min=1)\n",
    "    \n",
    "        return node_feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 337
    },
    "id": "N3ZjNKG-kWGy",
    "outputId": "1e9307da-c17d-4f04-e491-40e312db7a5c"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x300 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Let's apply our Convolutional Graph Layer to an example graph\n",
    "# Creating and plotting a sample graph\n",
    "\n",
    "import networkx as nx\n",
    "\n",
    "# creating a graph\n",
    "nx_g = nx.from_edgelist([(2,1), (2,3), (4,2), (3,4)])   # creates as touples indicating the connections between two nodes \n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "#nx.draw(g, with_labels=true)\n",
    "#Plt.show()\n",
    "\n",
    "fig = plt.figure(figsize = (8,3))\n",
    "nx.draw(nx_g, with_labels=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aOpC8StBv3EY",
    "outputId": "a50462ef-f345-4904-e694-6e00ead9c1be"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "node featues:\n",
      " tensor([[[0., 1.],\n",
      "         [2., 3.],\n",
      "         [4., 5.],\n",
      "         [6., 7.]]])\n",
      "\n",
      "Adjacency matrix:\n",
      " tensor([[[1., 1., 0., 0.],\n",
      "         [1., 1., 1., 1.],\n",
      "         [0., 1., 1., 1.],\n",
      "         [0., 1., 1., 1.]]])\n"
     ]
    }
   ],
   "source": [
    "# Defining nodes featues with torch.arrange\n",
    "\n",
    "node_feats = torch.arange(8, dtype = torch.float32).view(1,4,2)   # creating some featurs\n",
    "\n",
    "adj_matrix = torch.Tensor([[[1, 1, 0, 0],     # hand based translation of the graph we saw above (could be done using networkx as well)\n",
    "                            [1, 1, 1, 1],\n",
    "                            [0, 1, 1, 1],\n",
    "                            [0, 1, 1, 1]]])  # leading diagonal always 1, implying that the self node is also a feature \n",
    "\n",
    "print(\"node featues:\\n\", node_feats)\n",
    "print(\"\\nAdjacency matrix:\\n\", adj_matrix)\n",
    "                                                      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b6KDUqbjy71L",
    "outputId": "145b223c-b564-494c-a3a3-6fbf1fa0cd68"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adjacency matrix tensor([[[1., 1., 0., 0.],\n",
      "         [1., 1., 1., 1.],\n",
      "         [0., 1., 1., 1.],\n",
      "         [0., 1., 1., 1.]]])\n",
      "Input feature tensor([[[0., 1.],\n",
      "         [2., 3.],\n",
      "         [4., 5.],\n",
      "         [6., 7.]]])\n",
      "Output feaures tensor([[[1., 2.],\n",
      "         [3., 4.],\n",
      "         [4., 5.],\n",
      "         [4., 5.]]])\n"
     ]
    }
   ],
   "source": [
    "# Now let's apply a GCN layer to the graph: for simplicity we inialize the linear weight matrix as identity matrix so that input featues are equal to the message\n",
    "\n",
    "layer = GCNLayer(c_in=2, c_out=2)   # model\n",
    "\n",
    "layer.projection.weight.data = torch.Tensor([[1, 0], [0, 1]])\n",
    "layer.projection.bias.data = torch.Tensor([0, 0])\n",
    "\n",
    "with torch.no_grad():                       # .no_grad(), since we are doing a simple forward pass\n",
    "  out_feats = layer(node_feats, adj_matrix) # passingthe features\n",
    "\n",
    "print(\"Adjacency matrix\", adj_matrix)\n",
    "print(\"Input feature\", node_feats)\n",
    "print(\"Output feaures\", out_feats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TCDVd9D72djD"
   },
   "source": [
    "# Graph Attention Neural Network - GAT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "Ugr0QfHV5Rx6"
   },
   "outputs": [],
   "source": [
    "class GATLayer(nn.Module):  # inheriting nn.module\n",
    "  def __init__(self, c_in, c_out, num_heads=1, concat_heads=True, alpha=0.2):\n",
    "    super().__init__()\n",
    "\n",
    "    '''Inputs:\n",
    "          c_in = dimensionality of input features\n",
    "          c_out = dimensionality of output features\n",
    "\n",
    "          num_heads = number of heads in attention mechanism  - if bigger > 1 multihead attention\n",
    "          concat = if True output of all heads will be concatenated instead of averaging\n",
    "          alpha = negative slope for leaky ReLu\n",
    "    '''\n",
    "\n",
    "    \n",
    "    self.num_heads = num_heads\n",
    "    self.concat_heads = concat_heads\n",
    "\n",
    "    if self.concat_heads:               # if we want to concatenate heads instead of taking the average \n",
    "      assert c_out % num_heads == 0;    \"number of output featues must be the multiple of the count of heads\"\n",
    "      c_out = c_out // num_heads\n",
    "\n",
    "\n",
    "    # sub module and parameters needed in the layer\n",
    "    # TASK: create first linear projection ouput whiche depends on number of heads we want attention for\n",
    "    self.projection = nn.Linear(c_in, c_out * num_heads)\n",
    "    # TASK:  Initalize weight one weith matrix per head with dimensionality 2*c_out \n",
    "    self.a = nn.Parameter(torch.zeros(size=(2 * c_out, num_heads)))\n",
    "    # TASK: Intialize leaky relu activation with alpha passed as parameter\n",
    "    self.leakyrelu = nn.LeakyReLU(alpha)\n",
    "\n",
    "    #Initialization\n",
    "    nn.init.xavier_uniform_(self.projection.weight.data, gain = 1.414)   # xavier uniform initialization\n",
    "    nn.init.xavier_uniform_(self.a.data, gain = 1.414)\n",
    "\n",
    "  def forward(self, node_feats, adj_matrix, print_attn_probs = False):\n",
    "\n",
    "    batch_size, num_nodes = node_feats.size(0), node_feats.size(1)     # calculating batch size and number of node featues \n",
    "\n",
    "    #apply linear layer\n",
    "    # TASK: Projecting the node features as for GNNs\n",
    "    node_feats = self.projection(node_feats)\n",
    "    node_feats = node_feats.view(batch_size, num_nodes, self.num_heads, -1)   # Convert into batch size, number of nodes and number of heads\n",
    "\n",
    "    # we need to calculate the attention logits for every edge in the adjacency matrix\n",
    "    # doing this on all possible nodes will be very expensive\n",
    "    #TASK: return indices where the adjacency matrix is not zero\n",
    "    edges = adj_matrix.nonzero(as_tuple=False)\n",
    "\n",
    "    node_feats_flat = node_feats.view(batch_size*num_nodes, self.num_heads, -1)    # Flattening out node features\n",
    "\n",
    "    edge_indices_row = edges[:, 0] #TASK: Pick the edge indices of the row\n",
    "    edge_indices_col = edges[:, 1] #TASK:  Pick the edge indices of the column\n",
    "    \n",
    "    # Concatenating the features of two nodes\n",
    "    a_input =torch.cat([\n",
    "        torch.index_select(input = node_feats_flat, index = edge_indices_row, dim =0),\n",
    "        torch.index_select(input = node_feats_flat, index = edge_indices_col, dim =0)\n",
    "    ], dim=-1)   #Index select returns a tensor with node_feats_flat being indexed as the desired position along dim =0 \n",
    "\n",
    "    # Calculate attention MLP output (independen for each head)\n",
    "  \n",
    "    attn_logits = torch.einsum('bhc, hc ->bh', a_input, self.a)  # passing our input a_input and weights self.a\n",
    "    \n",
    "    attn_logits = self.leakyrelu(attn_logits) #TASK: apply leaky relu to attention logits\n",
    "\n",
    "    # Map list of attention values back into matrix\n",
    "    attn_matrix = attn_logits.new_zeros(adj_matrix.shape + (self.num_heads,)).fill_(-9e15)   # creating matrix with zeros\n",
    "    attn_matrix[adj_matrix[...,None].repeat(1,1,1,self.num_heads)==1] = attn_logits.reshape(-1) # Filling with the attention values for relevant indices\n",
    "\n",
    "    # Weighted average of attention\n",
    "    attn_probs = F.softmax(attn_matrix, dim=1) # TASK: pass attn_matrix through softmax\n",
    "  \n",
    "    if print_attn_probs:\n",
    "      print(\"Attention probs\\n\", attn_probs.permute(0, 3, 1, 2))\n",
    "    node_feats = torch.einsum('bijh, bjhc -> bihc', attn_probs, node_feats)\n",
    "\n",
    "    # If heads should be concatenated, we cna do this by reshaping. Otherwise, take the mean\n",
    "    if self.concat_heads:\n",
    "      node_feats = node_feats.reshape(batch_size,num_nodes, -1)\n",
    "    else:\n",
    "      node_feats = node_feats.mean(dim=2)\n",
    "\n",
    "    return node_feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ODrDOxMeQRWC",
    "outputId": "dd88d081-bad9-4981-8466-622f9ca98959"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adjacency matrix tensor([[[1., 1., 0., 0.],\n",
      "         [1., 1., 1., 1.],\n",
      "         [0., 1., 1., 1.],\n",
      "         [0., 1., 1., 1.]]])\n",
      "Input feature tensor([[[0., 1.],\n",
      "         [2., 3.],\n",
      "         [4., 5.],\n",
      "         [6., 7.]]])\n",
      "Output feaures tensor([[[0.1640, 1.3056],\n",
      "         [1.9270, 5.4154],\n",
      "         [3.5112, 4.7323],\n",
      "         [6.3978, 4.5467]]])\n"
     ]
    }
   ],
   "source": [
    "# instantiating the GATNN and doing a forward pass\n",
    "layer = GATLayer(2,2, num_heads =2, concat_heads=True) \n",
    "\n",
    "\n",
    "layer.projection.weight.data = torch.Tensor([[1, 0], [0, 1]])\n",
    "layer.projection.bias.data = torch.Tensor([0, 0])\n",
    "\n",
    "\n",
    "layer.a.data = torch.Tensor([[-0.2, 0.3],[0.1,-0.1]])\n",
    "\n",
    "with torch.no_grad():                       # .no_grad(), since we are doing a simple forward pass\n",
    "  out_feats = layer(node_feats, adj_matrix, print_attn_probs = False) # passing the features\n",
    "\n",
    "print(\"Adjacency matrix\", adj_matrix)\n",
    "print(\"Input feature\", node_feats)\n",
    "print(\"Output feaures\", out_feats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Uu_vDiSrK1G-"
   },
   "source": [
    "# Using Pytorch Geometric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jZhIpLTlLaxL",
    "outputId": "668e8021-5d59-400c-8e05-068eaa219694"
   },
   "outputs": [],
   "source": [
    "#!pip install torch_geometric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "PZ4IChcCbkGe"
   },
   "outputs": [],
   "source": [
    "import torch_geometric\n",
    "import torch_geometric.nn as g_nn\n",
    "import torch_geometric.data as g_data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TG7BoMjnLg7x"
   },
   "source": [
    "## Example of node level task: Semi supervised node classification\n",
    "\n",
    "A popular example that we will use here is the Cora dataset, a citation network among papers. The Cora dataset consists of 2708 scientific publications with links between each other representing the citation of one paper by another. The task is to classify each publication into one of seven classes. Each publication is represented by a bag-of-words vector. This means that we have a vector of 1433 elements for each publication, where 1 for a feature indicates that the i-th word of a pre-defined dicationary is present in this article."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "if-2A8tuLXha",
    "outputId": "d04580eb-1955-4c7c-f0f8-e148eeefad9b"
   },
   "outputs": [],
   "source": [
    "# Load the dataset and normalized features\n",
    "from torch_geometric.transforms import NormalizeFeatures \n",
    "from torch_geometric.datasets import Planetoid # needed to get the dataset\n",
    "#TASK: Load troch_geometic dataset \"Cora\" ind save in \"Pytorch_Practice_Models\", use  NormalizeFeatures() as transform\n",
    "dataset = Planetoid(root='./Pytorch_Practice_Models', name='Cora', transform=NormalizeFeatures())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xgPWDFM4ND3I",
    "outputId": "42f0e494-d444-43db-a2b2-95e8b9ec3c9c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Data(x=[2708, 1433], edge_index=[2, 10556], y=[2708], train_mask=[2708], val_mask=[2708], test_mask=[2708])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = dataset[0]\n",
    "data\n",
    "\n",
    "# 2708 nodes (publications)\n",
    "# Every node has 1433 features\n",
    "# edge indexes show what nodes are connected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dvK1juCcNNYs",
    "outputId": "a5620972-ca58-4e9c-81c3-a4232da7d3ab"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GCN(\n",
      "  (conv1): GCNConv(1433, 16)\n",
      "  (conv2): GCNConv(16, 7)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# GCN using pytorch-geometric library\n",
    "from torch_geometric.nn import GCNConv    # Import GCNConv\n",
    "\n",
    "# TASK: instantiate class GCN as subclass of nn.Module\n",
    "class GCN(nn.Module):\n",
    "  def __init__(self, hidden_channels):\n",
    "    super().__init__()\n",
    "    torch.manual_seed(1234)\n",
    "    # TASK: set up first convolutional GCN layer  with (dataset.num_features, hidden_channels) as dimensionality\n",
    "    self.conv1 = GCNConv(dataset.num_features, hidden_channels)\n",
    "    # TASK: second convoluational GCN layer (maps to the output ), (hidden_channels, dataset.num_classes) as dimensionality\n",
    "    self.conv2 = GCNConv(hidden_channels, dataset.num_classes)\n",
    "\n",
    "  def forward(self, x, edge_index):\n",
    "     #TASK: conv 1 layer\n",
    "    x = self.conv1(x, edge_index)\n",
    "    #TASK: apply relu activation\n",
    "    x = F.relu(x)\n",
    "    #TASK: dropout with p=0.5 \n",
    "    x = F.dropout(x, p=0.5, training=self.training)\n",
    "    #TASL:  second conv layer\n",
    "    x = self.conv2(x, edge_index)\n",
    "    return x\n",
    "\n",
    "model = GCN(hidden_channels  =16)     # Take 16 hidden channels/ hidden features\n",
    "print(model)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "toPoYjDpOged",
    "outputId": "6cd4bdff-d071-4e79-9b69-58635750e630"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 001, Loss: 1.9461\n",
      "Epoch: 002, Loss: 1.9414\n",
      "Epoch: 003, Loss: 1.9343\n",
      "Epoch: 004, Loss: 1.9274\n",
      "Epoch: 005, Loss: 1.9199\n",
      "Epoch: 006, Loss: 1.9132\n",
      "Epoch: 007, Loss: 1.8977\n",
      "Epoch: 008, Loss: 1.8931\n",
      "Epoch: 009, Loss: 1.8791\n",
      "Epoch: 010, Loss: 1.8689\n",
      "Epoch: 011, Loss: 1.8581\n",
      "Epoch: 012, Loss: 1.8449\n",
      "Epoch: 013, Loss: 1.8346\n",
      "Epoch: 014, Loss: 1.8211\n",
      "Epoch: 015, Loss: 1.8139\n",
      "Epoch: 016, Loss: 1.7876\n",
      "Epoch: 017, Loss: 1.7789\n",
      "Epoch: 018, Loss: 1.7678\n",
      "Epoch: 019, Loss: 1.7491\n",
      "Epoch: 020, Loss: 1.7218\n",
      "Epoch: 021, Loss: 1.7238\n",
      "Epoch: 022, Loss: 1.6986\n",
      "Epoch: 023, Loss: 1.6777\n",
      "Epoch: 024, Loss: 1.6534\n",
      "Epoch: 025, Loss: 1.6534\n",
      "Epoch: 026, Loss: 1.6279\n",
      "Epoch: 027, Loss: 1.6144\n",
      "Epoch: 028, Loss: 1.5818\n",
      "Epoch: 029, Loss: 1.5823\n",
      "Epoch: 030, Loss: 1.5491\n",
      "Epoch: 031, Loss: 1.5195\n",
      "Epoch: 032, Loss: 1.5177\n",
      "Epoch: 033, Loss: 1.4970\n",
      "Epoch: 034, Loss: 1.4455\n",
      "Epoch: 035, Loss: 1.4448\n",
      "Epoch: 036, Loss: 1.4392\n",
      "Epoch: 037, Loss: 1.3998\n",
      "Epoch: 038, Loss: 1.3734\n",
      "Epoch: 039, Loss: 1.3814\n",
      "Epoch: 040, Loss: 1.3734\n",
      "Epoch: 041, Loss: 1.3261\n",
      "Epoch: 042, Loss: 1.3221\n",
      "Epoch: 043, Loss: 1.2769\n",
      "Epoch: 044, Loss: 1.2493\n",
      "Epoch: 045, Loss: 1.2482\n",
      "Epoch: 046, Loss: 1.2279\n",
      "Epoch: 047, Loss: 1.2146\n",
      "Epoch: 048, Loss: 1.1750\n",
      "Epoch: 049, Loss: 1.1339\n",
      "Epoch: 050, Loss: 1.1775\n",
      "Epoch: 051, Loss: 1.1202\n",
      "Epoch: 052, Loss: 1.1047\n",
      "Epoch: 053, Loss: 1.1437\n",
      "Epoch: 054, Loss: 1.0542\n",
      "Epoch: 055, Loss: 1.0546\n",
      "Epoch: 056, Loss: 1.0692\n",
      "Epoch: 057, Loss: 1.0661\n",
      "Epoch: 058, Loss: 1.0397\n",
      "Epoch: 059, Loss: 1.0274\n",
      "Epoch: 060, Loss: 0.9797\n",
      "Epoch: 061, Loss: 0.9071\n",
      "Epoch: 062, Loss: 0.9533\n",
      "Epoch: 063, Loss: 0.9268\n",
      "Epoch: 064, Loss: 0.9105\n",
      "Epoch: 065, Loss: 0.8874\n",
      "Epoch: 066, Loss: 0.8473\n",
      "Epoch: 067, Loss: 0.9054\n",
      "Epoch: 068, Loss: 0.8263\n",
      "Epoch: 069, Loss: 0.8237\n",
      "Epoch: 070, Loss: 0.8480\n",
      "Epoch: 071, Loss: 0.8296\n",
      "Epoch: 072, Loss: 0.8028\n",
      "Epoch: 073, Loss: 0.7921\n",
      "Epoch: 074, Loss: 0.8237\n",
      "Epoch: 075, Loss: 0.7894\n",
      "Epoch: 076, Loss: 0.7824\n",
      "Epoch: 077, Loss: 0.7705\n",
      "Epoch: 078, Loss: 0.7426\n",
      "Epoch: 079, Loss: 0.7532\n",
      "Epoch: 080, Loss: 0.7036\n",
      "Epoch: 081, Loss: 0.7399\n",
      "Epoch: 082, Loss: 0.7096\n",
      "Epoch: 083, Loss: 0.6660\n",
      "Epoch: 084, Loss: 0.6864\n",
      "Epoch: 085, Loss: 0.6552\n",
      "Epoch: 086, Loss: 0.6739\n",
      "Epoch: 087, Loss: 0.6386\n",
      "Epoch: 088, Loss: 0.6788\n",
      "Epoch: 089, Loss: 0.6302\n",
      "Epoch: 090, Loss: 0.6447\n",
      "Epoch: 091, Loss: 0.6285\n",
      "Epoch: 092, Loss: 0.6549\n",
      "Epoch: 093, Loss: 0.6687\n",
      "Epoch: 094, Loss: 0.6534\n",
      "Epoch: 095, Loss: 0.6117\n",
      "Epoch: 096, Loss: 0.5606\n",
      "Epoch: 097, Loss: 0.5899\n",
      "Epoch: 098, Loss: 0.6019\n",
      "Epoch: 099, Loss: 0.5692\n",
      "Epoch: 100, Loss: 0.5674\n",
      "Epoch: 101, Loss: 0.5760\n",
      "Epoch: 102, Loss: 0.5387\n",
      "Epoch: 103, Loss: 0.5790\n",
      "Epoch: 104, Loss: 0.5219\n",
      "Epoch: 105, Loss: 0.5613\n",
      "Epoch: 106, Loss: 0.5544\n",
      "Epoch: 107, Loss: 0.5520\n",
      "Epoch: 108, Loss: 0.5132\n",
      "Epoch: 109, Loss: 0.5199\n",
      "Epoch: 110, Loss: 0.5348\n",
      "Epoch: 111, Loss: 0.5427\n",
      "Epoch: 112, Loss: 0.5076\n",
      "Epoch: 113, Loss: 0.5660\n",
      "Epoch: 114, Loss: 0.5524\n",
      "Epoch: 115, Loss: 0.5364\n",
      "Epoch: 116, Loss: 0.5083\n",
      "Epoch: 117, Loss: 0.5024\n",
      "Epoch: 118, Loss: 0.4788\n",
      "Epoch: 119, Loss: 0.5105\n",
      "Epoch: 120, Loss: 0.4749\n",
      "Epoch: 121, Loss: 0.4901\n",
      "Epoch: 122, Loss: 0.4747\n",
      "Epoch: 123, Loss: 0.4472\n",
      "Epoch: 124, Loss: 0.4948\n",
      "Epoch: 125, Loss: 0.4204\n",
      "Epoch: 126, Loss: 0.5191\n",
      "Epoch: 127, Loss: 0.4387\n",
      "Epoch: 128, Loss: 0.4990\n",
      "Epoch: 129, Loss: 0.4640\n",
      "Epoch: 130, Loss: 0.4722\n",
      "Epoch: 131, Loss: 0.4436\n",
      "Epoch: 132, Loss: 0.5177\n",
      "Epoch: 133, Loss: 0.4450\n",
      "Epoch: 134, Loss: 0.4298\n",
      "Epoch: 135, Loss: 0.4434\n",
      "Epoch: 136, Loss: 0.4249\n",
      "Epoch: 137, Loss: 0.4531\n",
      "Epoch: 138, Loss: 0.4388\n",
      "Epoch: 139, Loss: 0.4002\n",
      "Epoch: 140, Loss: 0.4122\n",
      "Epoch: 141, Loss: 0.4693\n",
      "Epoch: 142, Loss: 0.4537\n",
      "Epoch: 143, Loss: 0.4537\n",
      "Epoch: 144, Loss: 0.4257\n",
      "Epoch: 145, Loss: 0.4291\n",
      "Epoch: 146, Loss: 0.3870\n",
      "Epoch: 147, Loss: 0.4318\n",
      "Epoch: 148, Loss: 0.4481\n",
      "Epoch: 149, Loss: 0.4341\n",
      "Epoch: 150, Loss: 0.4305\n",
      "Epoch: 151, Loss: 0.3794\n",
      "Epoch: 152, Loss: 0.3971\n",
      "Epoch: 153, Loss: 0.3983\n",
      "Epoch: 154, Loss: 0.3827\n",
      "Epoch: 155, Loss: 0.4371\n",
      "Epoch: 156, Loss: 0.4224\n",
      "Epoch: 157, Loss: 0.3695\n",
      "Epoch: 158, Loss: 0.3924\n",
      "Epoch: 159, Loss: 0.3929\n",
      "Epoch: 160, Loss: 0.3726\n",
      "Epoch: 161, Loss: 0.4028\n",
      "Epoch: 162, Loss: 0.4171\n",
      "Epoch: 163, Loss: 0.3812\n",
      "Epoch: 164, Loss: 0.4003\n",
      "Epoch: 165, Loss: 0.3805\n",
      "Epoch: 166, Loss: 0.3601\n",
      "Epoch: 167, Loss: 0.3425\n",
      "Epoch: 168, Loss: 0.3876\n",
      "Epoch: 169, Loss: 0.3605\n",
      "Epoch: 170, Loss: 0.3781\n",
      "Epoch: 171, Loss: 0.3661\n",
      "Epoch: 172, Loss: 0.3857\n",
      "Epoch: 173, Loss: 0.4135\n",
      "Epoch: 174, Loss: 0.3488\n",
      "Epoch: 175, Loss: 0.3741\n",
      "Epoch: 176, Loss: 0.3883\n",
      "Epoch: 177, Loss: 0.3789\n",
      "Epoch: 178, Loss: 0.3434\n",
      "Epoch: 179, Loss: 0.3428\n",
      "Epoch: 180, Loss: 0.3289\n",
      "Epoch: 181, Loss: 0.3576\n",
      "Epoch: 182, Loss: 0.3590\n",
      "Epoch: 183, Loss: 0.3724\n",
      "Epoch: 184, Loss: 0.3542\n",
      "Epoch: 185, Loss: 0.3279\n",
      "Epoch: 186, Loss: 0.3681\n",
      "Epoch: 187, Loss: 0.3564\n",
      "Epoch: 188, Loss: 0.3428\n",
      "Epoch: 189, Loss: 0.3503\n",
      "Epoch: 190, Loss: 0.3307\n",
      "Epoch: 191, Loss: 0.3263\n",
      "Epoch: 192, Loss: 0.3452\n",
      "Epoch: 193, Loss: 0.3393\n",
      "Epoch: 194, Loss: 0.3078\n",
      "Epoch: 195, Loss: 0.3416\n",
      "Epoch: 196, Loss: 0.3617\n",
      "Epoch: 197, Loss: 0.3226\n",
      "Epoch: 198, Loss: 0.3442\n",
      "Epoch: 199, Loss: 0.3660\n"
     ]
    }
   ],
   "source": [
    "model = GCN(hidden_channels = 16)   #instantiating model\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay =5e-4) # Adam optimizer with appropriate learning rate and cross entropy loss\n",
    "criterion = torch.nn.CrossEntropyLoss()  #\n",
    "\n",
    "def train():\n",
    "  #TASK: put model in training mode\n",
    "  model.train()\n",
    "   #TASK: clear gradients \n",
    "  optimizer.zero_grad()\n",
    "  out = model(data.x, data.edge_index)  #perform a single forward pass \n",
    "  loss = criterion(out[data.train_mask], data.y[data.train_mask]) #compute the loss solely based on the training nodes \n",
    "  #TASK: derive gradients (backward pass)\n",
    "  loss.backward()\n",
    "  #TASK: Update parameters based on gradient\n",
    "  optimizer.step()\n",
    "  return loss\n",
    "\n",
    "\n",
    "def test():\n",
    "  model.eval()\n",
    "  out = model(data.x, data.edge_index) \n",
    "  pred = out.argmax(dim=1) # use the class with the highest probability\n",
    "  test_correct = pred[data.test_mask] == data.y[data.test_mask] # check against ground-truth labels\n",
    "  test_acc = int(test_correct.sum()) /int(data.test_mask.sum())  # Derive ratio of correct predictions\n",
    "  return test_acc\n",
    "\n",
    "for epoch in range(1,200):\n",
    "  loss = train()\n",
    "  print(f'Epoch: {epoch:03d}, Loss: {loss:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ws8dwHbwYdvt",
    "outputId": "67edfc26-8166-4eba-8db8-8af14c639422"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.8190\n"
     ]
    }
   ],
   "source": [
    "# Observing performance on test set\n",
    "test_acc = test()\n",
    "print(f'Test Accuracy: {test_acc:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "z0dJ66JBwznc",
    "outputId": "d13cabcf-eb19-4533-faf0-6a7602a82330"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GAT(\n",
      "  (conv1): GATConv(1433, 8, heads=8)\n",
      "  (conv2): GATConv(64, 7, heads=8)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Geaph Attention Neural Network\n",
    "\n",
    "from torch_geometric.nn import GATConv\n",
    "\n",
    "class GAT(torch.nn.Module):\n",
    "  def __init__(self, hidden_channels, heads):\n",
    "    super().__init__()\n",
    "    torch.manual_seed(1234)\n",
    "    # TASK: first convolutional GAT layer      (takes the input)\n",
    "    self.conv1 = GATConv(dataset.num_features, hidden_channels, heads=heads)\n",
    "    # TASK: second convoluational GAT layer (maps to the output )\n",
    "    self.conv2 = GATConv(hidden_channels * heads, dataset.num_classes, heads=heads, concat=False)\n",
    "\n",
    "  def forward(self, x, edge_index):\n",
    "    #TASK: dropout p=0.6 use dropout\n",
    "    x = F.dropout(x, p=0.6, training=self.training)\n",
    "    #TASK: GAT 1 layer\n",
    "    x = self.conv1(x, edge_index)\n",
    "    #TAKS: apply elu activation\n",
    "    x = F.elu(x)\n",
    "    #TASK: use dropout, p=0.6\n",
    "    x = F.dropout(x, p=0.6, training=self.training)\n",
    "    #TASK apply second GAT layer\n",
    "    x = self.conv2(x, edge_index)\n",
    "    return x\n",
    "\n",
    "model = GAT(hidden_channels  =8, heads=8)     # Take 8 hidden channels/ hidden features and 8 heads\n",
    "print(model)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fQp3rpSuyBna",
    "outputId": "c29aee9c-5547-4c63-f3bf-a6910fa1396f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 001, Loss: 1.9464\n",
      "Epoch: 002, Loss: 1.9449\n",
      "Epoch: 003, Loss: 1.9420\n",
      "Epoch: 004, Loss: 1.9399\n",
      "Epoch: 005, Loss: 1.9383\n",
      "Epoch: 006, Loss: 1.9364\n",
      "Epoch: 007, Loss: 1.9334\n",
      "Epoch: 008, Loss: 1.9321\n",
      "Epoch: 009, Loss: 1.9300\n",
      "Epoch: 010, Loss: 1.9260\n",
      "Epoch: 011, Loss: 1.9224\n",
      "Epoch: 012, Loss: 1.9198\n",
      "Epoch: 013, Loss: 1.9149\n",
      "Epoch: 014, Loss: 1.9105\n",
      "Epoch: 015, Loss: 1.9054\n",
      "Epoch: 016, Loss: 1.9000\n",
      "Epoch: 017, Loss: 1.8957\n",
      "Epoch: 018, Loss: 1.8895\n",
      "Epoch: 019, Loss: 1.8840\n",
      "Epoch: 020, Loss: 1.8679\n",
      "Epoch: 021, Loss: 1.8635\n",
      "Epoch: 022, Loss: 1.8621\n",
      "Epoch: 023, Loss: 1.8441\n",
      "Epoch: 024, Loss: 1.8417\n",
      "Epoch: 025, Loss: 1.8267\n",
      "Epoch: 026, Loss: 1.8137\n",
      "Epoch: 027, Loss: 1.8036\n",
      "Epoch: 028, Loss: 1.7961\n",
      "Epoch: 029, Loss: 1.7789\n",
      "Epoch: 030, Loss: 1.7607\n",
      "Epoch: 031, Loss: 1.7546\n",
      "Epoch: 032, Loss: 1.7424\n",
      "Epoch: 033, Loss: 1.7065\n",
      "Epoch: 034, Loss: 1.6983\n",
      "Epoch: 035, Loss: 1.6620\n",
      "Epoch: 036, Loss: 1.6643\n",
      "Epoch: 037, Loss: 1.6518\n",
      "Epoch: 038, Loss: 1.6213\n",
      "Epoch: 039, Loss: 1.6040\n",
      "Epoch: 040, Loss: 1.5940\n",
      "Epoch: 041, Loss: 1.5664\n",
      "Epoch: 042, Loss: 1.5505\n",
      "Epoch: 043, Loss: 1.5387\n",
      "Epoch: 044, Loss: 1.4798\n",
      "Epoch: 045, Loss: 1.4692\n",
      "Epoch: 046, Loss: 1.4713\n",
      "Epoch: 047, Loss: 1.4323\n",
      "Epoch: 048, Loss: 1.4026\n",
      "Epoch: 049, Loss: 1.3823\n",
      "Epoch: 050, Loss: 1.3675\n",
      "Epoch: 051, Loss: 1.3144\n",
      "Epoch: 052, Loss: 1.2971\n",
      "Epoch: 053, Loss: 1.2750\n",
      "Epoch: 054, Loss: 1.2382\n",
      "Epoch: 055, Loss: 1.2125\n",
      "Epoch: 056, Loss: 1.2093\n",
      "Epoch: 057, Loss: 1.1873\n",
      "Epoch: 058, Loss: 1.1700\n",
      "Epoch: 059, Loss: 1.1312\n",
      "Epoch: 060, Loss: 1.1277\n",
      "Epoch: 061, Loss: 1.0846\n",
      "Epoch: 062, Loss: 1.0692\n",
      "Epoch: 063, Loss: 1.0170\n",
      "Epoch: 064, Loss: 0.9915\n",
      "Epoch: 065, Loss: 1.0266\n",
      "Epoch: 066, Loss: 0.9711\n",
      "Epoch: 067, Loss: 0.9325\n",
      "Epoch: 068, Loss: 0.9195\n",
      "Epoch: 069, Loss: 0.9239\n",
      "Epoch: 070, Loss: 0.8745\n",
      "Epoch: 071, Loss: 0.8342\n",
      "Epoch: 072, Loss: 0.8673\n",
      "Epoch: 073, Loss: 0.8374\n",
      "Epoch: 074, Loss: 0.8208\n",
      "Epoch: 075, Loss: 0.7774\n",
      "Epoch: 076, Loss: 0.7682\n",
      "Epoch: 077, Loss: 0.7494\n",
      "Epoch: 078, Loss: 0.7188\n",
      "Epoch: 079, Loss: 0.7518\n",
      "Epoch: 080, Loss: 0.7302\n",
      "Epoch: 081, Loss: 0.7597\n",
      "Epoch: 082, Loss: 0.6405\n",
      "Epoch: 083, Loss: 0.7032\n",
      "Epoch: 084, Loss: 0.6571\n",
      "Epoch: 085, Loss: 0.6400\n",
      "Epoch: 086, Loss: 0.6384\n",
      "Epoch: 087, Loss: 0.6626\n",
      "Epoch: 088, Loss: 0.6448\n",
      "Epoch: 089, Loss: 0.6058\n",
      "Epoch: 090, Loss: 0.5963\n",
      "Epoch: 091, Loss: 0.6111\n",
      "Epoch: 092, Loss: 0.6229\n",
      "Epoch: 093, Loss: 0.5869\n",
      "Epoch: 094, Loss: 0.5698\n",
      "Epoch: 095, Loss: 0.6183\n",
      "Epoch: 096, Loss: 0.5284\n",
      "Epoch: 097, Loss: 0.6068\n",
      "Epoch: 098, Loss: 0.5477\n",
      "Epoch: 099, Loss: 0.5685\n",
      "Epoch: 100, Loss: 0.5545\n",
      "Epoch: 101, Loss: 0.5578\n",
      "Epoch: 102, Loss: 0.5139\n",
      "Epoch: 103, Loss: 0.5711\n",
      "Epoch: 104, Loss: 0.5473\n",
      "Epoch: 105, Loss: 0.5492\n",
      "Epoch: 106, Loss: 0.5233\n",
      "Epoch: 107, Loss: 0.4718\n",
      "Epoch: 108, Loss: 0.4825\n",
      "Epoch: 109, Loss: 0.5114\n",
      "Epoch: 110, Loss: 0.5251\n",
      "Epoch: 111, Loss: 0.4804\n",
      "Epoch: 112, Loss: 0.4957\n",
      "Epoch: 113, Loss: 0.5207\n",
      "Epoch: 114, Loss: 0.4819\n",
      "Epoch: 115, Loss: 0.5054\n",
      "Epoch: 116, Loss: 0.4989\n",
      "Epoch: 117, Loss: 0.4377\n",
      "Epoch: 118, Loss: 0.4801\n",
      "Epoch: 119, Loss: 0.4908\n",
      "Epoch: 120, Loss: 0.4657\n",
      "Epoch: 121, Loss: 0.4749\n",
      "Epoch: 122, Loss: 0.5265\n",
      "Epoch: 123, Loss: 0.4725\n",
      "Epoch: 124, Loss: 0.4903\n",
      "Epoch: 125, Loss: 0.4761\n",
      "Epoch: 126, Loss: 0.4618\n",
      "Epoch: 127, Loss: 0.4800\n",
      "Epoch: 128, Loss: 0.4588\n",
      "Epoch: 129, Loss: 0.4423\n",
      "Epoch: 130, Loss: 0.4539\n",
      "Epoch: 131, Loss: 0.4585\n",
      "Epoch: 132, Loss: 0.4172\n",
      "Epoch: 133, Loss: 0.4621\n",
      "Epoch: 134, Loss: 0.4845\n",
      "Epoch: 135, Loss: 0.4716\n",
      "Epoch: 136, Loss: 0.4259\n",
      "Epoch: 137, Loss: 0.4381\n",
      "Epoch: 138, Loss: 0.4349\n",
      "Epoch: 139, Loss: 0.4501\n",
      "Epoch: 140, Loss: 0.4353\n",
      "Epoch: 141, Loss: 0.4310\n",
      "Epoch: 142, Loss: 0.3972\n",
      "Epoch: 143, Loss: 0.4138\n",
      "Epoch: 144, Loss: 0.4065\n",
      "Epoch: 145, Loss: 0.3897\n",
      "Epoch: 146, Loss: 0.4625\n",
      "Epoch: 147, Loss: 0.4783\n",
      "Epoch: 148, Loss: 0.4187\n",
      "Epoch: 149, Loss: 0.4589\n",
      "Epoch: 150, Loss: 0.4114\n",
      "Epoch: 151, Loss: 0.4481\n",
      "Epoch: 152, Loss: 0.4085\n",
      "Epoch: 153, Loss: 0.4016\n",
      "Epoch: 154, Loss: 0.4128\n",
      "Epoch: 155, Loss: 0.4617\n",
      "Epoch: 156, Loss: 0.4870\n",
      "Epoch: 157, Loss: 0.4346\n",
      "Epoch: 158, Loss: 0.4128\n",
      "Epoch: 159, Loss: 0.4570\n",
      "Epoch: 160, Loss: 0.4015\n",
      "Epoch: 161, Loss: 0.4108\n",
      "Epoch: 162, Loss: 0.4316\n",
      "Epoch: 163, Loss: 0.4360\n",
      "Epoch: 164, Loss: 0.4292\n",
      "Epoch: 165, Loss: 0.4705\n",
      "Epoch: 166, Loss: 0.3845\n",
      "Epoch: 167, Loss: 0.4682\n",
      "Epoch: 168, Loss: 0.3801\n",
      "Epoch: 169, Loss: 0.4237\n",
      "Epoch: 170, Loss: 0.3515\n",
      "Epoch: 171, Loss: 0.4420\n",
      "Epoch: 172, Loss: 0.3972\n",
      "Epoch: 173, Loss: 0.4515\n",
      "Epoch: 174, Loss: 0.3844\n",
      "Epoch: 175, Loss: 0.4498\n",
      "Epoch: 176, Loss: 0.4171\n",
      "Epoch: 177, Loss: 0.3938\n",
      "Epoch: 178, Loss: 0.4104\n",
      "Epoch: 179, Loss: 0.4316\n",
      "Epoch: 180, Loss: 0.4002\n",
      "Epoch: 181, Loss: 0.3569\n",
      "Epoch: 182, Loss: 0.3880\n",
      "Epoch: 183, Loss: 0.4168\n",
      "Epoch: 184, Loss: 0.4006\n",
      "Epoch: 185, Loss: 0.4003\n",
      "Epoch: 186, Loss: 0.4631\n",
      "Epoch: 187, Loss: 0.3827\n",
      "Epoch: 188, Loss: 0.4117\n",
      "Epoch: 189, Loss: 0.3766\n",
      "Epoch: 190, Loss: 0.3987\n",
      "Epoch: 191, Loss: 0.3748\n",
      "Epoch: 192, Loss: 0.3919\n",
      "Epoch: 193, Loss: 0.3907\n",
      "Epoch: 194, Loss: 0.3798\n",
      "Epoch: 195, Loss: 0.4237\n",
      "Epoch: 196, Loss: 0.3521\n",
      "Epoch: 197, Loss: 0.3622\n",
      "Epoch: 198, Loss: 0.3622\n",
      "Epoch: 199, Loss: 0.4040\n"
     ]
    }
   ],
   "source": [
    "model = GAT(hidden_channels  =8, heads=8)  #instantiating model\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.005, weight_decay =5e-4) # Adam optimizer with appropriate learning rate and cross entropy loss\n",
    "criterion = torch.nn.CrossEntropyLoss()  #\n",
    "\n",
    "def train():\n",
    "  # TASK: set model in training mode \n",
    "  model.train()\n",
    "  # TASK:  clear gradients \n",
    "  optimizer.zero_grad()\n",
    "  out = model(data.x, data.edge_index)  #perform a single forward pass \n",
    "  loss = criterion(out[data.train_mask], data.y[data.train_mask]) #compute the loss solely based on the training nodes \n",
    "  #TASK: derive gradients (backward pass)\n",
    "  loss.backward()\n",
    "  #TASK Update parameters based on gradient\n",
    "  optimizer.step()\n",
    "  return loss\n",
    "\n",
    "\n",
    "def test():\n",
    "  model.eval()\n",
    "  out = model(data.x, data.edge_index) \n",
    "  pred = out.argmax(dim=1) # use the class with the highest probability\n",
    "  test_correct = pred[data.test_mask] == data.y[data.test_mask] # check against ground-truth labels\n",
    "  test_acc = int(test_correct.sum()) /int(data.test_mask.sum())  # Derive ratio of correct predictions\n",
    "  return test_acc\n",
    "\n",
    " \n",
    "for epoch in range(1,200):\n",
    "  loss = train()\n",
    "  print(f'Epoch: {epoch:03d}, Loss: {loss:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4wDuugxfy7t8",
    "outputId": "822280e4-efaf-4d01-a36c-36182d8e8df0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.8170\n"
     ]
    }
   ],
   "source": [
    "# Observing performance on test set\n",
    "test_acc = test()\n",
    "print(f'Test Accuracy: {test_acc:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aZIYr14hDwKX"
   },
   "source": [
    "Summary: in this context and the particular set-up graph convolutions are more effective than graph attention. Propose (argue for), implement and evaluate two potential improvements for the graph attention neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Jtji0EdOzDdf"
   },
   "source": [
    "### 1. **Deepening the Network Architecture**\n",
    "**Proposition**: Adding more layers to the GAT can potentially enhance its capability to capture deeper and more complex relational patterns within the graph, a trait often necessary for outperforming convolutional approaches in graphs with highly intricate structures.\n",
    "\n",
    "**Implementation Details**:\n",
    "- Implement skip connections (residual learning) between layers to help mitigate the vanishing gradient problem, which is common in deeper networks.\n",
    "- Use layer normalization to stabilize learning and adaptive learning rate techniques to fine-tune training across deeper architectures.\n",
    "\n",
    "### 2. **Optimized Use of Multi-Head Attention**\n",
    "**Proposition**: Enhancing the design and initialization of multi-head attention in GATs can lead to better feature representation and learning. By diversifying the initial states and roles of each attention head, the network can explore and capture a wider array of relationships in the data, potentially overcoming areas where graph convolutions typically excel.\n",
    "\n",
    "**Implementation Details**:\n",
    "- Increase the number of attention heads to provide multiple representations of the nodes' features, each focusing on different aspects of the data.\n",
    "- Diverse initialization of these heads, using orthogonal initialization, to ensure that each head learns distinct, complementary aspects of the data.\n",
    "- Introduce regularization strategies (Dropout applied selectively on the heads) to prevent overfitting and promote robustness in feature learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GAT(\n",
      "  (conv1): GATConv(1433, 8, heads=8)\n",
      "  (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "  (conv2): GATConv(64, 8, heads=8)\n",
      "  (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "  (conv3): GATConv(64, 7, heads=1)\n",
      "  (norm3): LayerNorm((7,), eps=1e-05, elementwise_affine=True)\n",
      "  (skip1): Linear(in_features=1433, out_features=64, bias=True)\n",
      "  (skip2): Identity()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "from torch.nn import LayerNorm\n",
    "class GAT(torch.nn.Module):\n",
    "    def __init__(self, num_features, num_classes, hidden_channels, heads):\n",
    "        super().__init__()\n",
    "        torch.manual_seed(12345)\n",
    "        self.conv1 = GATConv(num_features, hidden_channels, heads=heads)\n",
    "        self.norm1 = LayerNorm(hidden_channels * heads)  # Layer normalization\n",
    "        self.conv2 = GATConv(hidden_channels * heads, hidden_channels, heads=heads, dropout=0.6)\n",
    "        self.norm2 = LayerNorm(hidden_channels * heads)  # Another layer normalization\n",
    "        self.conv3 = GATConv(hidden_channels * heads, num_classes, heads=1, concat=False)  # Output layer\n",
    "        self.norm3 = LayerNorm(num_classes)  # Normalization for the output layer\n",
    "\n",
    "        # Skip connection if dimensions are the same\n",
    "        if hidden_channels * heads == num_features:\n",
    "            self.skip1 = torch.nn.Identity()\n",
    "        else:\n",
    "            self.skip1 = torch.nn.Linear(num_features, hidden_channels * heads)\n",
    "        \n",
    "        # Skip connection for second layer\n",
    "        if hidden_channels * heads == hidden_channels * heads:\n",
    "            self.skip2 = torch.nn.Identity()\n",
    "        else:\n",
    "            self.skip2 = torch.nn.Linear(hidden_channels * heads, hidden_channels * heads)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        # Initial input\n",
    "        skip1 = self.skip1(x)\n",
    "\n",
    "        # First GAT layer\n",
    "        x = F.dropout(x, p=0.6, training=self.training)\n",
    "        x = F.elu(self.conv1(x, edge_index))\n",
    "        x = self.norm1(x + skip1)  # Add skip connection after normalization\n",
    "\n",
    "        # Second GAT layer\n",
    "        skip2 = self.skip2(x)\n",
    "        x = F.dropout(x, p=0.6, training=self.training)\n",
    "        x = F.elu(self.conv2(x, edge_index))\n",
    "        x = self.norm2(x + skip2)  # Add skip connection after normalization\n",
    "\n",
    "        # Output layer\n",
    "        x = F.dropout(x, p=0.6, training=self.training)\n",
    "        x = self.conv3(x, edge_index)\n",
    "        x = self.norm3(x)  # Normalize output layer features\n",
    "\n",
    "        return x\n",
    "\n",
    "# Example initialization\n",
    "model = GAT(num_features=dataset.num_features, num_classes=dataset.num_classes, hidden_channels=8, heads=8)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 001, Loss: 2.4158\n",
      "Epoch: 002, Loss: 2.1897\n",
      "Epoch: 003, Loss: 2.0141\n",
      "Epoch: 004, Loss: 1.8515\n",
      "Epoch: 005, Loss: 1.7366\n",
      "Epoch: 006, Loss: 1.7940\n",
      "Epoch: 007, Loss: 1.5468\n",
      "Epoch: 008, Loss: 1.3807\n",
      "Epoch: 009, Loss: 1.2618\n",
      "Epoch: 010, Loss: 1.0749\n",
      "Epoch: 011, Loss: 1.0902\n",
      "Epoch: 012, Loss: 0.9680\n",
      "Epoch: 013, Loss: 0.9572\n",
      "Epoch: 014, Loss: 0.8887\n",
      "Epoch: 015, Loss: 0.8693\n",
      "Epoch: 016, Loss: 0.7868\n",
      "Epoch: 017, Loss: 0.8027\n",
      "Epoch: 018, Loss: 0.7656\n",
      "Epoch: 019, Loss: 0.7070\n",
      "Epoch: 020, Loss: 0.7437\n",
      "Epoch: 021, Loss: 0.7162\n",
      "Epoch: 022, Loss: 0.6185\n",
      "Epoch: 023, Loss: 0.6221\n",
      "Epoch: 024, Loss: 0.5761\n",
      "Epoch: 025, Loss: 0.5953\n",
      "Epoch: 026, Loss: 0.5881\n",
      "Epoch: 027, Loss: 0.6025\n",
      "Epoch: 028, Loss: 0.5695\n",
      "Epoch: 029, Loss: 0.5320\n",
      "Epoch: 030, Loss: 0.5027\n",
      "Epoch: 031, Loss: 0.4760\n",
      "Epoch: 032, Loss: 0.4952\n",
      "Epoch: 033, Loss: 0.4965\n",
      "Epoch: 034, Loss: 0.5145\n",
      "Epoch: 035, Loss: 0.4593\n",
      "Epoch: 036, Loss: 0.4675\n",
      "Epoch: 037, Loss: 0.4533\n",
      "Epoch: 038, Loss: 0.4225\n",
      "Epoch: 039, Loss: 0.4496\n",
      "Epoch: 040, Loss: 0.4575\n",
      "Epoch: 041, Loss: 0.4275\n",
      "Epoch: 042, Loss: 0.4321\n",
      "Epoch: 043, Loss: 0.3984\n",
      "Epoch: 044, Loss: 0.4043\n",
      "Epoch: 045, Loss: 0.3692\n",
      "Epoch: 046, Loss: 0.3884\n",
      "Epoch: 047, Loss: 0.3573\n",
      "Epoch: 048, Loss: 0.3560\n",
      "Epoch: 049, Loss: 0.3588\n",
      "Epoch: 050, Loss: 0.3497\n",
      "Epoch: 051, Loss: 0.3537\n",
      "Epoch: 052, Loss: 0.3392\n",
      "Epoch: 053, Loss: 0.3140\n",
      "Epoch: 054, Loss: 0.3197\n",
      "Epoch: 055, Loss: 0.3070\n",
      "Epoch: 056, Loss: 0.3131\n",
      "Epoch: 057, Loss: 0.2936\n",
      "Epoch: 058, Loss: 0.3133\n",
      "Epoch: 059, Loss: 0.2987\n",
      "Epoch: 060, Loss: 0.2984\n",
      "Epoch: 061, Loss: 0.2866\n",
      "Epoch: 062, Loss: 0.2796\n",
      "Epoch: 063, Loss: 0.2724\n",
      "Epoch: 064, Loss: 0.2824\n",
      "Epoch: 065, Loss: 0.2894\n",
      "Epoch: 066, Loss: 0.2667\n",
      "Epoch: 067, Loss: 0.2621\n",
      "Epoch: 068, Loss: 0.2671\n",
      "Epoch: 069, Loss: 0.2567\n",
      "Epoch: 070, Loss: 0.2473\n",
      "Epoch: 071, Loss: 0.2618\n",
      "Epoch: 072, Loss: 0.2431\n",
      "Epoch: 073, Loss: 0.2457\n",
      "Epoch: 074, Loss: 0.2401\n",
      "Epoch: 075, Loss: 0.2593\n",
      "Epoch: 076, Loss: 0.2369\n",
      "Epoch: 077, Loss: 0.2244\n",
      "Epoch: 078, Loss: 0.2239\n",
      "Epoch: 079, Loss: 0.2263\n",
      "Epoch: 080, Loss: 0.2315\n",
      "Epoch: 081, Loss: 0.2174\n",
      "Epoch: 082, Loss: 0.2337\n",
      "Epoch: 083, Loss: 0.2105\n",
      "Epoch: 084, Loss: 0.2080\n",
      "Epoch: 085, Loss: 0.2137\n",
      "Epoch: 086, Loss: 0.2022\n",
      "Epoch: 087, Loss: 0.2077\n",
      "Epoch: 088, Loss: 0.2003\n",
      "Epoch: 089, Loss: 0.2026\n",
      "Epoch: 090, Loss: 0.1972\n",
      "Epoch: 091, Loss: 0.1955\n",
      "Epoch: 092, Loss: 0.1961\n",
      "Epoch: 093, Loss: 0.2034\n",
      "Epoch: 094, Loss: 0.1897\n",
      "Epoch: 095, Loss: 0.1900\n",
      "Epoch: 096, Loss: 0.1860\n",
      "Epoch: 097, Loss: 0.1798\n",
      "Epoch: 098, Loss: 0.1799\n",
      "Epoch: 099, Loss: 0.1740\n",
      "Epoch: 100, Loss: 0.1735\n",
      "Epoch: 101, Loss: 0.1716\n",
      "Epoch: 102, Loss: 0.1700\n",
      "Epoch: 103, Loss: 0.1703\n",
      "Epoch: 104, Loss: 0.1682\n",
      "Epoch: 105, Loss: 0.1631\n",
      "Epoch: 106, Loss: 0.1582\n",
      "Epoch: 107, Loss: 0.1600\n",
      "Epoch: 108, Loss: 0.1625\n",
      "Epoch: 109, Loss: 0.1544\n",
      "Epoch: 110, Loss: 0.1522\n",
      "Epoch: 111, Loss: 0.1564\n",
      "Epoch: 112, Loss: 0.1513\n",
      "Epoch: 113, Loss: 0.1524\n",
      "Epoch: 114, Loss: 0.1579\n",
      "Epoch: 115, Loss: 0.1459\n",
      "Epoch: 116, Loss: 0.1442\n",
      "Epoch: 117, Loss: 0.1432\n",
      "Epoch: 118, Loss: 0.1380\n",
      "Epoch: 119, Loss: 0.1461\n",
      "Epoch: 120, Loss: 0.1395\n",
      "Epoch: 121, Loss: 0.1356\n",
      "Epoch: 122, Loss: 0.1373\n",
      "Epoch: 123, Loss: 0.1407\n",
      "Epoch: 124, Loss: 0.1356\n",
      "Epoch: 125, Loss: 0.1359\n",
      "Epoch: 126, Loss: 0.1307\n",
      "Epoch: 127, Loss: 0.1413\n",
      "Epoch: 128, Loss: 0.1332\n",
      "Epoch: 129, Loss: 0.1260\n",
      "Epoch: 130, Loss: 0.1336\n",
      "Epoch: 131, Loss: 0.1253\n",
      "Epoch: 132, Loss: 0.1260\n",
      "Epoch: 133, Loss: 0.1257\n",
      "Epoch: 134, Loss: 0.1257\n",
      "Epoch: 135, Loss: 0.1256\n",
      "Epoch: 136, Loss: 0.1276\n",
      "Epoch: 137, Loss: 0.1231\n",
      "Epoch: 138, Loss: 0.1221\n",
      "Epoch: 139, Loss: 0.1245\n",
      "Epoch: 140, Loss: 0.1159\n",
      "Epoch: 141, Loss: 0.1154\n",
      "Epoch: 142, Loss: 0.1187\n",
      "Epoch: 143, Loss: 0.1210\n",
      "Epoch: 144, Loss: 0.1141\n",
      "Epoch: 145, Loss: 0.1083\n",
      "Epoch: 146, Loss: 0.1172\n",
      "Epoch: 147, Loss: 0.1149\n",
      "Epoch: 148, Loss: 0.1120\n",
      "Epoch: 149, Loss: 0.1074\n",
      "Epoch: 150, Loss: 0.1104\n",
      "Epoch: 151, Loss: 0.1103\n",
      "Epoch: 152, Loss: 0.1062\n",
      "Epoch: 153, Loss: 0.1122\n",
      "Epoch: 154, Loss: 0.1072\n",
      "Epoch: 155, Loss: 0.1037\n",
      "Epoch: 156, Loss: 0.1069\n",
      "Epoch: 157, Loss: 0.1079\n",
      "Epoch: 158, Loss: 0.1039\n",
      "Epoch: 159, Loss: 0.1042\n",
      "Epoch: 160, Loss: 0.1020\n",
      "Epoch: 161, Loss: 0.1018\n",
      "Epoch: 162, Loss: 0.1006\n",
      "Epoch: 163, Loss: 0.1016\n",
      "Epoch: 164, Loss: 0.0996\n",
      "Epoch: 165, Loss: 0.0985\n",
      "Epoch: 166, Loss: 0.1007\n",
      "Epoch: 167, Loss: 0.0991\n",
      "Epoch: 168, Loss: 0.0971\n",
      "Epoch: 169, Loss: 0.1018\n",
      "Epoch: 170, Loss: 0.0934\n",
      "Epoch: 171, Loss: 0.0941\n",
      "Epoch: 172, Loss: 0.0931\n",
      "Epoch: 173, Loss: 0.0891\n",
      "Epoch: 174, Loss: 0.0909\n",
      "Epoch: 175, Loss: 0.0931\n",
      "Epoch: 176, Loss: 0.0900\n",
      "Epoch: 177, Loss: 0.0935\n",
      "Epoch: 178, Loss: 0.0862\n",
      "Epoch: 179, Loss: 0.0907\n",
      "Epoch: 180, Loss: 0.0877\n",
      "Epoch: 181, Loss: 0.0877\n",
      "Epoch: 182, Loss: 0.0900\n",
      "Epoch: 183, Loss: 0.0861\n",
      "Epoch: 184, Loss: 0.0833\n",
      "Epoch: 185, Loss: 0.0864\n",
      "Epoch: 186, Loss: 0.0858\n",
      "Epoch: 187, Loss: 0.0858\n",
      "Epoch: 188, Loss: 0.0825\n",
      "Epoch: 189, Loss: 0.0819\n",
      "Epoch: 190, Loss: 0.0821\n",
      "Epoch: 191, Loss: 0.0832\n",
      "Epoch: 192, Loss: 0.0795\n",
      "Epoch: 193, Loss: 0.0795\n",
      "Epoch: 194, Loss: 0.0768\n",
      "Epoch: 195, Loss: 0.0798\n",
      "Epoch: 196, Loss: 0.0801\n",
      "Epoch: 197, Loss: 0.0777\n",
      "Epoch: 198, Loss: 0.0801\n",
      "Epoch: 199, Loss: 0.0755\n"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.005, weight_decay =5e-4) # Adam optimizer with appropriate learning rate and cross entropy loss\n",
    "criterion = torch.nn.CrossEntropyLoss()  #\n",
    "\n",
    "def train():\n",
    "  # TASK: set model in training mode \n",
    "  model.train()\n",
    "  # TASK:  clear gradients \n",
    "  optimizer.zero_grad()\n",
    "  out = model(data.x, data.edge_index)  #perform a single forward pass \n",
    "  loss = criterion(out[data.train_mask], data.y[data.train_mask]) #compute the loss solely based on the training nodes \n",
    "  #TASK: derive gradients (backward pass)\n",
    "  loss.backward()\n",
    "  #TASK Update parameters based on gradient\n",
    "  optimizer.step()\n",
    "  return loss\n",
    "\n",
    "\n",
    "def test():\n",
    "  model.eval()\n",
    "  out = model(data.x, data.edge_index) \n",
    "  pred = out.argmax(dim=1) # use the class with the highest probability\n",
    "  test_correct = pred[data.test_mask] == data.y[data.test_mask] # check against ground-truth labels\n",
    "  test_acc = int(test_correct.sum()) /int(data.test_mask.sum())  # Derive ratio of correct predictions\n",
    "  return test_acc\n",
    "\n",
    " \n",
    "for epoch in range(1,200):\n",
    "  loss = train()\n",
    "  print(f'Epoch: {epoch:03d}, Loss: {loss:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.7780\n"
     ]
    }
   ],
   "source": [
    "# Observing performance on test set\n",
    "test_acc = test()\n",
    "print(f'Test Accuracy: {test_acc:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More layers seem to lead to overfitting, actually decreasing accuracy with a lower loss than before. Let's try the second approach:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.init import orthogonal_\n",
    "class GAT(torch.nn.Module):\n",
    "    def __init__(self, hidden_channels, heads):\n",
    "        super().__init__()\n",
    "        torch.manual_seed(12345)\n",
    "\n",
    "        self.heads = heads\n",
    "        self.hidden_channels = hidden_channels\n",
    "\n",
    "        self.conv1 = GATConv(dataset.num_features, hidden_channels, heads=heads, concat=True)\n",
    "        self.conv2 = GATConv(hidden_channels * heads, dataset.num_classes, heads=1, concat=False)  # Output layer\n",
    "\n",
    "        # Initialize weights differently for each head\n",
    "        self.initialize_heads()\n",
    "\n",
    "    def initialize_heads(self):\n",
    "        # Apply orthogonal or sparse initialization to the weights of each head\n",
    "        for name, param in self.named_parameters():\n",
    "            if 'lin_l' in name:  # Target the linear transformation weights in GATConv\n",
    "                if 'weight' in name:\n",
    "                    orthogonal_(param.data)\n",
    "                elif 'bias' in name:\n",
    "                    param.data.fill_(0)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        # Apply selective dropout on attention heads' outputs\n",
    "        x = F.dropout(x, p=0.6, training=self.training)\n",
    "        x1 = F.elu(self.conv1(x, edge_index))\n",
    "\n",
    "        # Applying dropout to each head's output before concatenation\n",
    "        split_x1 = x1.split(self.hidden_channels, dim=1)\n",
    "        x1 = torch.cat([F.dropout(head, p=0.6, training=self.training) for head in split_x1], dim=1)\n",
    "\n",
    "        # Again apply dropout selectively for each head\n",
    "        split_x2 = x1.split(self.hidden_channels, dim=1)\n",
    "        x2 = torch.cat([F.dropout(head, p=0.6, training=self.training) for head in split_x2], dim=1)\n",
    "\n",
    "        x3 = self.conv2(x2, edge_index)\n",
    "\n",
    "        return x3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 001, Loss: 1.9468\n",
      "Epoch: 002, Loss: 1.9339\n",
      "Epoch: 003, Loss: 1.9237\n",
      "Epoch: 004, Loss: 1.9112\n",
      "Epoch: 005, Loss: 1.9040\n",
      "Epoch: 006, Loss: 1.8922\n",
      "Epoch: 007, Loss: 1.8785\n",
      "Epoch: 008, Loss: 1.8693\n",
      "Epoch: 009, Loss: 1.8545\n",
      "Epoch: 010, Loss: 1.8407\n",
      "Epoch: 011, Loss: 1.8318\n",
      "Epoch: 012, Loss: 1.8179\n",
      "Epoch: 013, Loss: 1.7894\n",
      "Epoch: 014, Loss: 1.7984\n",
      "Epoch: 015, Loss: 1.7619\n",
      "Epoch: 016, Loss: 1.7594\n",
      "Epoch: 017, Loss: 1.7363\n",
      "Epoch: 018, Loss: 1.7106\n",
      "Epoch: 019, Loss: 1.6883\n",
      "Epoch: 020, Loss: 1.6733\n",
      "Epoch: 021, Loss: 1.6613\n",
      "Epoch: 022, Loss: 1.6365\n",
      "Epoch: 023, Loss: 1.6364\n",
      "Epoch: 024, Loss: 1.5833\n",
      "Epoch: 025, Loss: 1.5651\n",
      "Epoch: 026, Loss: 1.5484\n",
      "Epoch: 027, Loss: 1.5374\n",
      "Epoch: 028, Loss: 1.4951\n",
      "Epoch: 029, Loss: 1.4594\n",
      "Epoch: 030, Loss: 1.4553\n",
      "Epoch: 031, Loss: 1.4357\n",
      "Epoch: 032, Loss: 1.4033\n",
      "Epoch: 033, Loss: 1.4020\n",
      "Epoch: 034, Loss: 1.3598\n",
      "Epoch: 035, Loss: 1.3183\n",
      "Epoch: 036, Loss: 1.3053\n",
      "Epoch: 037, Loss: 1.3080\n",
      "Epoch: 038, Loss: 1.2718\n",
      "Epoch: 039, Loss: 1.2498\n",
      "Epoch: 040, Loss: 1.2080\n",
      "Epoch: 041, Loss: 1.2023\n",
      "Epoch: 042, Loss: 1.1559\n",
      "Epoch: 043, Loss: 1.1177\n",
      "Epoch: 044, Loss: 1.0626\n",
      "Epoch: 045, Loss: 1.0635\n",
      "Epoch: 046, Loss: 1.0544\n",
      "Epoch: 047, Loss: 1.0768\n",
      "Epoch: 048, Loss: 0.9830\n",
      "Epoch: 049, Loss: 0.9969\n",
      "Epoch: 050, Loss: 0.9425\n",
      "Epoch: 051, Loss: 0.9569\n",
      "Epoch: 052, Loss: 0.8880\n",
      "Epoch: 053, Loss: 0.8902\n",
      "Epoch: 054, Loss: 0.8802\n",
      "Epoch: 055, Loss: 0.8522\n",
      "Epoch: 056, Loss: 0.8657\n",
      "Epoch: 057, Loss: 0.7978\n",
      "Epoch: 058, Loss: 0.7725\n",
      "Epoch: 059, Loss: 0.7464\n",
      "Epoch: 060, Loss: 0.8104\n",
      "Epoch: 061, Loss: 0.7412\n",
      "Epoch: 062, Loss: 0.7251\n",
      "Epoch: 063, Loss: 0.7355\n",
      "Epoch: 064, Loss: 0.7319\n",
      "Epoch: 065, Loss: 0.7237\n",
      "Epoch: 066, Loss: 0.6910\n",
      "Epoch: 067, Loss: 0.6172\n",
      "Epoch: 068, Loss: 0.6279\n",
      "Epoch: 069, Loss: 0.6093\n",
      "Epoch: 070, Loss: 0.6127\n",
      "Epoch: 071, Loss: 0.6045\n",
      "Epoch: 072, Loss: 0.5877\n",
      "Epoch: 073, Loss: 0.5807\n",
      "Epoch: 074, Loss: 0.5340\n",
      "Epoch: 075, Loss: 0.5828\n",
      "Epoch: 076, Loss: 0.5109\n",
      "Epoch: 077, Loss: 0.5919\n",
      "Epoch: 078, Loss: 0.5317\n",
      "Epoch: 079, Loss: 0.5273\n",
      "Epoch: 080, Loss: 0.4705\n",
      "Epoch: 081, Loss: 0.5182\n",
      "Epoch: 082, Loss: 0.4803\n",
      "Epoch: 083, Loss: 0.5129\n",
      "Epoch: 084, Loss: 0.5071\n",
      "Epoch: 085, Loss: 0.4794\n",
      "Epoch: 086, Loss: 0.5241\n",
      "Epoch: 087, Loss: 0.4579\n",
      "Epoch: 088, Loss: 0.4811\n",
      "Epoch: 089, Loss: 0.4477\n",
      "Epoch: 090, Loss: 0.4713\n",
      "Epoch: 091, Loss: 0.4826\n",
      "Epoch: 092, Loss: 0.4354\n",
      "Epoch: 093, Loss: 0.4608\n",
      "Epoch: 094, Loss: 0.4610\n",
      "Epoch: 095, Loss: 0.4461\n",
      "Epoch: 096, Loss: 0.4691\n",
      "Epoch: 097, Loss: 0.4895\n",
      "Epoch: 098, Loss: 0.4395\n",
      "Epoch: 099, Loss: 0.4066\n",
      "Epoch: 100, Loss: 0.4287\n",
      "Epoch: 101, Loss: 0.4573\n",
      "Epoch: 102, Loss: 0.3923\n",
      "Epoch: 103, Loss: 0.4190\n",
      "Epoch: 104, Loss: 0.4411\n",
      "Epoch: 105, Loss: 0.4072\n",
      "Epoch: 106, Loss: 0.4042\n",
      "Epoch: 107, Loss: 0.3926\n",
      "Epoch: 108, Loss: 0.3897\n",
      "Epoch: 109, Loss: 0.3516\n",
      "Epoch: 110, Loss: 0.3996\n",
      "Epoch: 111, Loss: 0.4009\n",
      "Epoch: 112, Loss: 0.4041\n",
      "Epoch: 113, Loss: 0.3719\n",
      "Epoch: 114, Loss: 0.3419\n",
      "Epoch: 115, Loss: 0.3550\n",
      "Epoch: 116, Loss: 0.4106\n",
      "Epoch: 117, Loss: 0.3838\n",
      "Epoch: 118, Loss: 0.3722\n",
      "Epoch: 119, Loss: 0.3334\n",
      "Epoch: 120, Loss: 0.3466\n",
      "Epoch: 121, Loss: 0.3458\n",
      "Epoch: 122, Loss: 0.3760\n",
      "Epoch: 123, Loss: 0.3730\n",
      "Epoch: 124, Loss: 0.3566\n",
      "Epoch: 125, Loss: 0.3366\n",
      "Epoch: 126, Loss: 0.3674\n",
      "Epoch: 127, Loss: 0.3471\n",
      "Epoch: 128, Loss: 0.3318\n",
      "Epoch: 129, Loss: 0.3346\n",
      "Epoch: 130, Loss: 0.3518\n",
      "Epoch: 131, Loss: 0.2957\n",
      "Epoch: 132, Loss: 0.3451\n",
      "Epoch: 133, Loss: 0.3736\n",
      "Epoch: 134, Loss: 0.2973\n",
      "Epoch: 135, Loss: 0.3286\n",
      "Epoch: 136, Loss: 0.3464\n",
      "Epoch: 137, Loss: 0.3177\n",
      "Epoch: 138, Loss: 0.3152\n",
      "Epoch: 139, Loss: 0.3532\n",
      "Epoch: 140, Loss: 0.3105\n",
      "Epoch: 141, Loss: 0.3211\n",
      "Epoch: 142, Loss: 0.3012\n",
      "Epoch: 143, Loss: 0.2661\n",
      "Epoch: 144, Loss: 0.3185\n",
      "Epoch: 145, Loss: 0.3460\n",
      "Epoch: 146, Loss: 0.3139\n",
      "Epoch: 147, Loss: 0.3321\n",
      "Epoch: 148, Loss: 0.3241\n",
      "Epoch: 149, Loss: 0.3436\n",
      "Epoch: 150, Loss: 0.3174\n",
      "Epoch: 151, Loss: 0.3157\n",
      "Epoch: 152, Loss: 0.3255\n",
      "Epoch: 153, Loss: 0.3014\n",
      "Epoch: 154, Loss: 0.3096\n",
      "Epoch: 155, Loss: 0.2805\n",
      "Epoch: 156, Loss: 0.3027\n",
      "Epoch: 157, Loss: 0.3030\n",
      "Epoch: 158, Loss: 0.2757\n",
      "Epoch: 159, Loss: 0.3013\n",
      "Epoch: 160, Loss: 0.3059\n",
      "Epoch: 161, Loss: 0.2998\n",
      "Epoch: 162, Loss: 0.2936\n",
      "Epoch: 163, Loss: 0.2677\n",
      "Epoch: 164, Loss: 0.2534\n",
      "Epoch: 165, Loss: 0.2910\n",
      "Epoch: 166, Loss: 0.2547\n",
      "Epoch: 167, Loss: 0.2750\n",
      "Epoch: 168, Loss: 0.2629\n",
      "Epoch: 169, Loss: 0.3111\n",
      "Epoch: 170, Loss: 0.2554\n",
      "Epoch: 171, Loss: 0.2886\n",
      "Epoch: 172, Loss: 0.2812\n",
      "Epoch: 173, Loss: 0.2816\n",
      "Epoch: 174, Loss: 0.2869\n",
      "Epoch: 175, Loss: 0.2868\n",
      "Epoch: 176, Loss: 0.2786\n",
      "Epoch: 177, Loss: 0.2843\n",
      "Epoch: 178, Loss: 0.2830\n",
      "Epoch: 179, Loss: 0.2313\n",
      "Epoch: 180, Loss: 0.3095\n",
      "Epoch: 181, Loss: 0.3008\n",
      "Epoch: 182, Loss: 0.2878\n",
      "Epoch: 183, Loss: 0.3120\n",
      "Epoch: 184, Loss: 0.2576\n",
      "Epoch: 185, Loss: 0.2592\n",
      "Epoch: 186, Loss: 0.2684\n",
      "Epoch: 187, Loss: 0.2764\n",
      "Epoch: 188, Loss: 0.2505\n",
      "Epoch: 189, Loss: 0.3064\n",
      "Epoch: 190, Loss: 0.2663\n",
      "Epoch: 191, Loss: 0.2472\n",
      "Epoch: 192, Loss: 0.2769\n",
      "Epoch: 193, Loss: 0.2746\n",
      "Epoch: 194, Loss: 0.2613\n",
      "Epoch: 195, Loss: 0.2732\n",
      "Epoch: 196, Loss: 0.2742\n",
      "Epoch: 197, Loss: 0.2275\n",
      "Epoch: 198, Loss: 0.2628\n",
      "Epoch: 199, Loss: 0.2529\n"
     ]
    }
   ],
   "source": [
    "model = GAT(hidden_channels  =4, heads=28)  #instantiating model with new parameters\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.005, weight_decay =5e-4) # Adam optimizer with appropriate learning rate and cross entropy loss\n",
    "criterion = torch.nn.CrossEntropyLoss()  #\n",
    "\n",
    "def train():\n",
    "  # TASK: set model in training mode \n",
    "  model.train()\n",
    "  # TASK:  clear gradients \n",
    "  optimizer.zero_grad()\n",
    "  out = model(data.x, data.edge_index)  #perform a single forward pass \n",
    "  loss = criterion(out[data.train_mask], data.y[data.train_mask]) #compute the loss solely based on the training nodes \n",
    "  #TASK: derive gradients (backward pass)\n",
    "  loss.backward()\n",
    "  #TASK Update parameters based on gradient\n",
    "  optimizer.step()\n",
    "  return loss\n",
    "\n",
    "\n",
    "def test():\n",
    "  model.eval()\n",
    "  out = model(data.x, data.edge_index) \n",
    "  pred = out.argmax(dim=1) # use the class with the highest probability\n",
    "  test_correct = pred[data.test_mask] == data.y[data.test_mask] # check against ground-truth labels\n",
    "  test_acc = int(test_correct.sum()) /int(data.test_mask.sum())  # Derive ratio of correct predictions\n",
    "  return test_acc\n",
    "\n",
    " \n",
    "for epoch in range(1,200):\n",
    "  loss = train()\n",
    "  print(f'Epoch: {epoch:03d}, Loss: {loss:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.8230\n"
     ]
    }
   ],
   "source": [
    "# Observing performance on test set\n",
    "test_acc = test()\n",
    "print(f'Test Accuracy: {test_acc:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is more successful, improving the accuracy closer to the 83.0 ± 0.7% achieved in the original GAT paper on the Cora Benchmark."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
