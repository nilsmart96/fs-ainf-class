{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GAN - first tries\n",
    "\n",
    "We are implementing a simple feed-forward GAN architecture on MNIST - just to get the feeling right."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.datasets import mnist\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "(train_data,_),(test_data,_) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The more the merrier :-)\n",
    "# Remember, this is unsupervised learning, so \"holdout\" and such makes less sense\n",
    "data = np.concatenate((train_data,test_data),axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(70000, 28, 28)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAFMAAAGFCAYAAACBos+GAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguMCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy81sbWrAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAPf0lEQVR4nO2df6xX8x/HH6mVfuibm1RYUpJVS4RorRASIV0Tc5Es5kdrhpmWpiUibEnMNKnVRAspu0ujHyzu5NfGTcLGKlMkVymtH98/7HXO+XRvt8/n3ud5f869ez3+6XR+fM77Pvd8n/eP8z6vV5NDhw4dwpFwTLEL0JhwMYW4mEJcTCEuphAXU4iLKcTFFNIs3xObNGmSZjkyTz5jG3emEBdTiIspxMUU4mIKcTGFuJhCXEwhLqYQF1NI3sPJkDRt2jTa/t///nfE8+677z4AWrVqBUDPnj0BuPfee6NznnnmGQBuuukmAPbu3Rsdmz59OgBTpkxRFNudqcTFFBK8mnfp0iXabt68OQADBw4EYNCgQQC0a9cuOqe0tDTv3968eTMAzz//fLTvuuuuA+Dvv/8G4Ouvv46OrVmzppCiHxV3ppAm+S5CqO98Zr9+/QD48MMPo321NS6FcPDgQQDGjh0LwK5du6qd8+uvvwLw559/Rvs2btyY9z18PjMwwZxZUlICQEVFRbSvW7dueV9v1+3cuTPad/HFFwOwb98+QOf0mnBnBsbFFBKsa7Rjxw4AHnrooWjfiBEjAPjyyy+B3C6N8dVXXwFw2WWXAbB79+7oWO/evQGYMGGCvsB1wJ0pJFgDVBNt27YF4g71yy+/DMAdd9wRnVNWVgbA66+/Lr9/IXgDFJiizhpVVVXl/P+vv/6qds64ceMAeOONN4C4g55F3JlCivrMPJzWrVsDsGzZsmjfkCFDABg+fDgA77//furlqAl/ZgbGxRSSqWpudO/ePdr+4osvgHhMvmrVqujY+vXrAZg9ezaQX1WsK17NA5NJZyaxmfK5c+cCcNxxx1U7Z+LEiQDMnz8fiOculbgzA5N5Zxp9+vQB4Lnnnov2DR06NOccG45OmzYt2rdlyxbJ/d2ZgXExhTSYam4kXwNfffXVQNw4WRmTL+1sHrS+eDUPTINzZk38+++/ADRr9t8k2P79+6Njw4YNA2D16tX1uoc7MzCZXAVXE3379gXg+uuvj/add955QOxIo7KyMtpeu3ZtgNL9hztTSCadaessIV6DOWrUKAA6dep0xOsOHDgA5A4nQ87MuzOFuJhCMlHNreraUmmr2gBdu3Y96vU2r2lj8nfffVdcwvxwZwoJ7syOHTtG27169QLghRdeAODMM8886vXJVXQzZswAYOnSpUDxXwO7M4Wk7kxbl2lzjbaCGPJbn7lu3ToAnn32WQBWrFgRHduzZ4+qmBLcmUJcTCHSaj5gwAAgdw3m+eefD8DJJ5981Ov/+ecfIHed5hNPPAHkrsvMKu5MIVJn2mtZ+7cmkjM6y5cvB+L5R2tkkh8BNCTcmUIaxUx7CHymPTAuphAXU4iLKcTFFOJiCsm70+6x8Y+OO1OIiynExRTiYgpxMYW4mEJcTCEuphAXU4iLKcTFFOJiCvFEIXni74AC42IKcTGFuJhCXEwhLqYQF1OIiykkE98BFUL//v2jbfte6NZbbwXi6DGzZs2KzrG4SCFwZwppMEsKa4rzbsFMDycZOrJ9+/aS+/twMjAuppDMN0D2tcaSJUuA3MD2VvUsZrEFv09W7QsuuACIGyI7Jw3cmUIy1QBZxqlzzjkn2rdgwQIATjnllGrlsKKb655++mkAFi1aFJ1j50+aNAmAJ598sk5l8wYoMJl6ZtrHqvYRf76Yk9u0aQPkJk266KKLgDj6TJq4M4W4mEIyUc1tvH3VVVcBNTd2VnWTYcctDeLWrVuBOOFIMhvVJZdccsTfVOPOFFLUrtHh4+2axtrl5eVA3ChZEHyIG5U5c+YAsH379mrXW+Ao+/w6eX0hM0reNQpM8GfmGWecEW3bR/42RPz999+B3PBj8+bNA+KMfe+99150LLl9NFq2bAnAAw88EO27+eabCyr70XBnCgnmzBYtWgBxCwxw5ZVXAvFEhc2YWwQtiB2lIpmRVY07U4iLKSRYNT/77LOBuGonufbaawF9IuLQuDOFBHOmpVFIdv7NiWk68phj/vNLiKB77kwhqTvTsp7a0DE5LAsRNNQcafe1DKtp4M4U4mIKSb2a2wimefPmAGzbti06Zok5Vdgo67HHHqt2zGamHnnkEek9k7gzhQSfNbKkHqDLdWaOtNe5yZCTmzdvBuKgfTb7lAbuTCHBnansDll3y5w4evRoII6ODVBaWiq739FwZwpJ3Zk2fLR/R44cGR2bMGFCwb93//33R9uPPvooEM/UL1y4EIjnRUPjzhTiYgpJvZrbmNj+TebzsajXr776KgB//PEHEK+pBLjlllsAOOuss4B4NRzAL7/8AsSB8F988UX9H1AA7kwhwbtGTZs2jbbvueceIO6+VFVVAdCjR48jXm/pGSBOzz158mR5OeuCO1NI6stj7Bm3ePFiIM7IV9Nv11QUe47aauC6dKcU+PKYwLiYQoKtguvcuTMAd911V7TPZnkOr+YzZ86MznnppZcA+OGHH+p1//ri1Twwmfp0Jcu4MwPjYgpxMYW4mEJcTCEuphAXU4iLKcQThQhxZwpxMYW4mEJcTCEuphAXU4iLKcTFFOJiCnExhbiYQjy3RZ74C7XAuJhCXEwhLqYQF1OIiynExRTiYgpxMYW4mEIyEdk1TYYOHQrE31Um42du3LhRei93phAXU4i0mg8ePBjIzS3x9ttvK29RMPbd0WeffZb6vdyZQqTOtGj9yW8fi+FMi/8GcNpppwFw6qmnAunOy7ozhUidaeEcPvnkE+XPFox9wAUwbtw4IM7e8t1336V2X3emEBdTiLSaJx/8xcQyCiTZtGlT6vfNxl/fSJA403JMdOzYUfFz9SaZtM5YuXJl6vd1ZwqRONPChauj/ReK1QzrqCfZsmVL6vd3ZwqROLNnz545///2228VP1swljcj+ez+/vvvgTh/Rpq4M4W4mEJSeW2R5txhMjXYFVdcAUBZWRkAl19+ebXzp06dCsDOnTtTK5PhzhSSijNLSkryOs8iD9oc46WXXgrkRiK0kOSWois5ZN2zZw8AFRUVQBwculmz+M/6/PPPC/8D6og7U4gkFI/FrbQAUMnnk8W4rAkbhtpv79+/H4jTGgJUVlYCsfuSKcEsW8tvv/0GxGHEjz/++Ogcc3Z98ZXDgXExhUgaIAsq+vPPPwMwcODAvK6zR8A777wDwIYNGwD49NNPC7r/nXfeCUCHDh0A+Omnnwq6XoU7U4i0a/TUU08pfy5vbD2RsWTJkqKUw50ppFGugivWkhx3phAXU4iLKcTFFNKoGiAb4ycT0hc6AKgP7kwhjcqZNrNTrGU67kwhjcqZxoUXXhhtv/baa8Hu684U4mIKaVTVvNhBWdyZQhqFmOXl5ZSXl3Po0KGihvNtFGJmBc+6kif+qjcwLqYQF1OIiynExRTiYgrx3BZC3JlCXEwhLqYQF1OIiynExRTiYgpxMYW4mEJcTCEuphAXU4gnCskTfwcUGBdTiIspxMUU4mIKcTGFuJhCXEwhLqYQF1NIo1qGPWnSJACmTJkS7bNvgiyGvEWcSQN3phAXU0ijqOZjxowB4OGHHwbg4MGD1c4JsbzHnSmkUTjTkoAce+yxRS2HO1NIg3amhYgcP358zv5kMpARI0YAcfC9NHFnCnExhTS4aj5o0KBoe+7cuUD19AszZsyIti3YXwjcmUIanDNvu+22aPukk07KObZ69WoA5s+fH7JIEe5MIQ3mQ9QTTjgByO3i2LDRYhzfcMMNAKxatUp+f1+EEJjMPzO7du0K1B5gdNasWUA6jiwEd6YQF1NI5qu5JQOxAPlJPvjgAwBmzpwZtExHwp0pJJNdo5EjR0bbFn6sdevWAKxbty46Zl2hEDNC3jUKTKaemfl0g5LZAUI4shDcmUJcTCGZqua1vao1pk+fHqo4BePOFJIJZ/br1w+oOTufsXTpUgA2btwYokh1wp0pJBOd9m3btgG5uc8Mi7M+fPhwAHbt2pVaOWrDO+2BycQzs3379kDNrbhlDiyWIwvBnSnExRRS1GpuiwhqS5+QnCXKOu5MIcGdaR10iFexWcOzb98+AGbPnh2dk7WZodpwZwoJ7sx27dpF2506dco5tmXLFgAefPDBkEWS4c4U4mIKcTGFuJhCgjdAycX71iFPrgZuyLgzhWRiPrMh4POZgXExhbiYQlxMIS6mEBdTiCcKEeLOFOJiCnExhbiYQlxMIS6mEBdTiIspxMUU4mIKcTGFeG6LPPHXFoFxMYW4mEJcTCEuphAXU4iLKcTFFOJiCnExhbiYQlxMIS6mEBdTiIspJBMf7x/OgAEDou2ysjIAhgwZAkDv3r2rnW9ftG3duhXI/eBgwYIFAFRUVKRT2ATuTCGZ+kBg9OjRQG48TAvcbPe38OEAHTp0AKBXr145v5Ms6+LFiwG48cYb61U2n2kPjIsppKgNULNm/93+3HPPBeCVV14BoFWrVtE5a9euBWDq1KkAfPzxx9GxFi1aAPDmm28CNUfsWr9+vbrYR8SdKaSozrRuz5w5c3L2r1y5Mtq2Rqmqqqra9XbscEdu3rw52p43b56msHngzhQSvGtkzz6AiRMnAnG3w6JrWWZTqNmRxoYNGwDo0aNHzv7S0tJo26Ib1hfvGgUm2DNz8uTJQOxGiEPvrFixAogju+7Zs6fa9ZYGMfl87NKlCxDXmscffxzQubFQ3JlCXEwhqTdAFsfIwknYWBtg+fLlQG7GgMM5/fTTAVi4cCEA/fv3r3aOxXUfO3YsALt3765TWWvDG6DApO7ME088EYjnGpN069YNgL179wJw++23A3DNNddE5/Tp0weANm3aALkOse1Ro0YBsGzZsjqVMR/cmYEJ9sy0DrbNQSZ/s7YimKPt3M6dO0fHtm/fXm1fWrgzA+NiCkl9BGQ5Ia37Y90hgJKSEgB+/PFHIB65WEIlgB07dgCwaNEiILdK276s4M4UEmxsbq9akw1QPgwePBiIX/UmY7knEy1lAXemkEwuQkjSsmVLIHZksoviz8xGTKYWIdTGgQMHgFxnWstunfc08U57YFxMIZlvgIYNG1bsIuSNO1NI5p1pc54NAXemkMw786OPPgLinEG1JfgsNu5MIS6mkMxX82+++QaATZs2AbkNUvfu3YEwI6B8cGcKaTBj8zFjxgC5aznXrFkDwPjx4wGorKxM7f4+Ng9Mg3Fm27ZtgXj9OsQJ7d566y0gXsTgy2MaAQ3GmYY5FGDatGkA3H333QD07dsXSOfZ6c4MjIsppMFV82Lh1TwwnttCiDtTiIspxMUU4mIKcTGFuJhCXEwhLqYQF1PI/wHTEsxQzzR+HAAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 640x480 with 5 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "example_count = 5\n",
    "\n",
    "for ex in range(example_count):\n",
    "    plt.subplot(5, example_count//5, ex+1)\n",
    "    plt.imshow(data[ex], interpolation=\"nearest\", cmap=\"gray\")\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalization betwenn -1 and 1 !!!!\n",
    "quasi_mean = data.max()/2 # Max is always 255, so this works ok.\n",
    "data = (data.astype(np.float32)-quasi_mean)/quasi_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(70000, 784)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Flattening of the image vectors\n",
    "data = data.reshape(data.shape[0],-1)\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just to get the feeling, how bad this representation is...\n",
    "\n",
    "example_count = 5\n",
    "\n",
    "for ex in range(example_count):\n",
    "    plt.subplot(5, example_count//5, ex+1)\n",
    "    plt.imshow(data[ex].reshape(1,-1), interpolation=\"nearest\", cmap=\"gray\")\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training parameters\n",
    "\n",
    "We will be forced to do manual batching here, so we have to calcculate the number of batches manually, and iterate on a per batch basis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 30\n",
    "BATCH_SIZE = 200\n",
    "HALF_BATCH = BATCH_SIZE // 2\n",
    "BATCH_NUM = (data.shape[0] // BATCH_SIZE)\n",
    "if data.shape[0] % BATCH_SIZE:\n",
    "    BATCH_NUM+=1\n",
    "Z_DIM = 100\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GENERATOR_L1_DIM = 256\n",
    "GENERATOR_L2_DIM = 512\n",
    "GENERATOR_L3_DIM = 1024\n",
    "\n",
    "DISCRIMINATOR_L1_DIM = 512\n",
    "DISCRIMINATOR_L2_DIM = 256\n",
    "\n",
    "LEAKY_ALPHA = 0.2\n",
    "\n",
    "# Add Image Dimension, as recommended below\n",
    "IMAGE_DIMENSIONS = (28, 28, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "# Some empirically set values. \n",
    "# It might well be worth experimenting with newer optimizers / settings\n",
    "\n",
    "optimizer = tf.keras.optimizers.legacy.Adam(learning_rate=1e-4, beta_1=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Input, Dense, LeakyReLU\n",
    "from tensorflow.keras.models import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Define the generator!\n",
    "#########################\n",
    "\n",
    "# We use FUNCTIONAL API!\n",
    "\n",
    "# The generator always gets a noise vector as input\n",
    "noise_input = Input(shape=(Z_DIM,))\n",
    "\n",
    "# Define the first layer of the fully connected network, without activation!\n",
    "# Use the parameters defined with capital letter constants in the cells above for node counts!\n",
    "g_layer_1 = Dense(GENERATOR_L1_DIM)(noise_input)\n",
    "# Define a non-linearioty, namely leaky relu on this layer!\n",
    "# We use LeakyReLU for avoiding sparsity - other options are viable also, just not normal relu\n",
    "# Use the alpha value defined in constants above!\n",
    "g_layer_1_nonlin = LeakyReLU(alpha=LEAKY_ALPHA)(g_layer_1)\n",
    "# You can do this as one layer, but now we do it in two. Does not matter.\n",
    "\n",
    "# Repeat the process two more times!\n",
    "g_layer_2 = Dense(GENERATOR_L2_DIM)(g_layer_1_nonlin)\n",
    "g_layer_2_nonlin = LeakyReLU(alpha=LEAKY_ALPHA)(g_layer_2)\n",
    "\n",
    "g_layer_3 = Dense(GENERATOR_L3_DIM)(g_layer_2_nonlin)\n",
    "g_layer_3_nonlin = LeakyReLU(alpha=LEAKY_ALPHA)(g_layer_3)\n",
    "\n",
    "# The output of the generator is a flattened image\n",
    "# Remeber, we normalized everything between -1 and +1, so what is a nice nonlinearity, bounded between\n",
    "# -1 and 1? \n",
    "# Use that one as an activation for the final fully connected layer!\n",
    "g_output_layer = Dense(np.prod(IMAGE_DIMENSIONS), activation='tanh')(g_layer_3_nonlin)\n",
    "\n",
    "# Please instantiate the model!\n",
    "generator = Model(inputs=noise_input, outputs=g_output_layer)\n",
    "\n",
    "# Please remeber, that the loss for the discriminator will be a binary loss, so this applies here also\n",
    "# Use the appropriate loss measure!\n",
    "generator.compile(loss='binary_crossentropy', optimizer=optimizer)\n",
    "# Think about this carefully, please!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Define the discriminator!\n",
    "#############################\n",
    "\n",
    "# We still use FUNCTIONAL API!\n",
    "\n",
    "# The discriminator always gets (flattened) images as inputs\n",
    "# Where can you find the flattened dimensions? \n",
    "# Please use a variable or property, not a hand \"baked in\" constant.\n",
    "# That will mess up your life if the data changes...\n",
    "image_input = Input(shape=IMAGE_DIMENSIONS)\n",
    "\n",
    "# Following the design pattern from above, please implement two layers with nonlinearities!\n",
    "d_layer_1 = Dense(DISCRIMINATOR_L1_DIM)(image_input)\n",
    "d_layer_1_nonlin = LeakyReLU(alpha=LEAKY_ALPHA)(d_layer_1)\n",
    "\n",
    "d_layer_2 = Dense(DISCRIMINATOR_L2_DIM)(d_layer_1_nonlin)\n",
    "d_layer_2_nonlin = LeakyReLU(alpha=LEAKY_ALPHA)(d_layer_2)\n",
    "\n",
    "# Please implement the output layer!\n",
    "# The output of the discriminator is a single binary decision, \n",
    "# so one use an appropriate activation and dimensionality!\n",
    "d_output_layer = Dense(1, activation='sigmoid')(d_layer_2_nonlin)\n",
    "\n",
    "# Please instantiate the model!\n",
    "discriminator = Model(inputs=image_input, outputs=d_output_layer)\n",
    "\n",
    "# Please remeber, that the loss for the discriminator will be a binary loss, so this applies here also\n",
    "# Use the appropriate loss measure!\n",
    "discriminator.compile(loss='binary_crossentropy', optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Joint model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Define the GAN itself!\n",
    "##########################\n",
    "\n",
    "\n",
    "# STOP!!!!!!\n",
    "# This is a crucial line, since in the joint model, discriminator will be frozen, so no weight update!\n",
    "discriminator.trainable = False\n",
    "\n",
    "# Remove this .... if you understand, why the above line is here!\n",
    "\n",
    "# What is the input for the whole GAN?\n",
    "# Think about the case, when the generator generates, the discriminator only discriminates it's output!\n",
    "gan_input = Input(shape=(Z_DIM,))\n",
    "\n",
    "# Use the generator as a function on the input!\n",
    "generated_image = generator(gan_input)\n",
    "# Use the discriminator as a function on the fake images!\n",
    "gan_output = discriminator(generated_image)\n",
    "\n",
    "# Instantiate the joint model, appropriate input and output!\n",
    "joint_model = Model(inputs=gan_input, outputs=gan_output)\n",
    "\n",
    "# Please think about, why it can be true, that for the joint model a binary decision is still adequate!\n",
    "joint_model.compile(loss='binary_crossentropy', optimizer=optimizer)\n",
    "# Maybe it will get clear below in the training loop..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper functions for visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nothing to see here! :-P\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "\n",
    "\n",
    "def get_example_images(epoch, example_count=25):\n",
    "    input_noise = np.random.normal(0,1, size=(example_count,Z_DIM))\n",
    "    generated_images = generator.predict(input_noise)\n",
    "    generated_images = generated_images.reshape(example_count, 28, 28)\n",
    "    \n",
    "    plt.figure(figsize = (5, example_count // 5))\n",
    "    for ex in range(example_count):\n",
    "        plt.subplot(5, example_count//5, ex+1)\n",
    "        plt.imshow(generated_images[ex], interpolation=\"nearest\", cmap=\"gray\")\n",
    "        plt.axis(\"off\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"example_images_epoch_num_{0}.png\".format(epoch))\n",
    "\n",
    "def show_image_for_epoch(epoch):\n",
    "    imgname = \"example_images_epoch_num_\"+str(epoch)+\".png\"\n",
    "    img = mpimg.imread(imgname)\n",
    "    imgplot = plt.imshow(img)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "Sadly, we can not use simple `fit()`, but have to construct the main training loop ourselves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To see some progress, we use tqdm as a progress bar\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Since we do NOT use fit\n",
    "# sadly, we have to do this ourselves manually\n",
    "history = {\"discriminator\":[],\"generator\":[]}\n",
    "\n",
    "# Main training loop\n",
    "for epoch_num in range(EPOCHS):\n",
    "    epoch_discriminator_loss = 0\n",
    "    epoch_generator_loss = 0\n",
    "    \n",
    "    for batch in tqdm(range(BATCH_NUM)):\n",
    "        # We select randomly a half batch amount of real images from MNIST\n",
    "        # Use Numpy to choose, no replacement!\n",
    "        real_images = train_data[np.random.choice(train_data.shape[0], HALF_BATCH, replace=False)]\n",
    "        \n",
    "        # We generate a half batch amount of fake images\n",
    "        # By first generating a half batch worth of Gaussian noise with zero mean, unit variance\n",
    "        # and appropriate noise dimensions\n",
    "        input_noise = np.random.normal(0, 1, (HALF_BATCH, Z_DIM))\n",
    "        # And then using the fixed generator, to output some images from it\n",
    "        # Using the predict method of the generator!\n",
    "        generated_images = generator.predict(input_noise)\n",
    "        \n",
    "        # STOP, and thik through, WHY predict?!\n",
    "        # Then you can remove the ....\n",
    "        \n",
    "        # We generate our \"labels\"\n",
    "        # Remember one sided label smoothing for the positive class!\n",
    "        # Let's say with 0.9...\n",
    "        # So please, generate a half batch sized, one dimensional matrix with ones, using numpy\n",
    "        # and multiuply it by 0.9\n",
    "        real_y = np.ones((HALF_BATCH, 1)) * 0.9\n",
    "        # And generate a half batch worth of zeroes, again one dimensional matrix\n",
    "        generated_y = np.zeros((HALF_BATCH, 1))\n",
    "\n",
    "        \n",
    "        ### We do the actual training!\n",
    "        \n",
    "        # First for the discriminator on the real data\n",
    "        discriminator_loss_real = discriminator.train_on_batch(real_images, real_y)\n",
    "        \n",
    "        # Then on the fake data\n",
    "        discriminator_loss_generated = discriminator.train_on_batch(generated_images, generated_y)\n",
    "        \n",
    "        # Then average the two losses\n",
    "        discriminator_loss = (discriminator_loss_real + discriminator_loss_generated) / 2\n",
    "        epoch_discriminator_loss += discriminator_loss\n",
    "        \n",
    "        ### We then update the generator\n",
    "        # We use the discriminator that was trained a line above, and is frozen, as defined in the joint model\n",
    "        \n",
    "        # Please generate a new set of input noise, notice, it is a full batch!\n",
    "        # Again, using numpy, normal distribution, zero mean, unit variance\n",
    "        new_input_noise = np.random.normal(0, 1, (BATCH_SIZE, Z_DIM))\n",
    "        \n",
    "        # We try to convince the discriminator, that this is real data - which is not\n",
    "        # So please generate a batch worht of one dimensional matrix filled with ones \n",
    "        convincing_y = np.ones((BATCH_SIZE, 1))\n",
    "        # Notice, no label smoothing!\n",
    "\n",
    "        # Remember, the joint model takes in noise, does the generation, the discrimination, then computes loss\n",
    "        # But the discriminator is frozen, so only the generator will get updated\n",
    "        # It is \"successful\" if the discriminator predicts \"real\" - hence the convincing_y\n",
    "        generator_loss = joint_model.train_on_batch(new_input_noise, convincing_y)\n",
    "        epoch_generator_loss += generator_loss\n",
    "        \n",
    "    # Loss printout in every epoch, averaged over the batches\n",
    "    print(\"Epoch number:\",epoch_num,\"discriminator_loss:\",epoch_discriminator_loss / BATCH_NUM, \"generator_loss:\", epoch_generator_loss / BATCH_NUM)\n",
    "    \n",
    "    # Save it for the future\n",
    "    history[\"discriminator\"].append(epoch_discriminator_loss / BATCH_NUM)\n",
    "    history[\"generator\"].append(epoch_generator_loss / BATCH_NUM)\n",
    "    \n",
    "    #Save model - optional\n",
    "    #generator.save(\"generator.h5\")\n",
    "    \n",
    "    #Save images\n",
    "    get_example_images(epoch_num)\n",
    "    \n",
    "    # Show epoch example\n",
    "    show_image_for_epoch(epoch_num)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization of training progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history[\"discriminator\"], color='blue', linewidth=2, label=\"Discriminator\")\n",
    "plt.plot(history[\"generator\"],  color='red', linewidth=2, label=\"Generator\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend();\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
