{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "abc0b02e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-26T07:09:49.993525Z",
     "start_time": "2021-09-26T07:09:49.957577Z"
    }
   },
   "source": [
    "## Recap: Different architectures, different preassumptions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "867bf878",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-26T07:09:49.993525Z",
     "start_time": "2021-09-26T07:09:49.957577Z"
    }
   },
   "source": [
    "- __Convolutions__ are presupposing, that there is a kind of __local relationship__ between the time points (which can be made more proper with \"causal convolutions\" and more wide with \"dilated convolutions\"), none the less, the basic assumption remains fairly local.\n",
    "\n",
    "<img src=\"http://drive.google.com/uc?export=view&id=1ABYwJ3zUbz4y7yCDxkKEC6YGClIRBS51\" width=45%>\n",
    "\n",
    "\n",
    "Observe, that __convolutions can be executed over a topology__, in the simplest case, topology is a grid. \n",
    "\n",
    "<img src=\"http://drive.google.com/uc?export=view&id=19andQjfpcxEqbiSfqULuDRBRvVsDexnK\" width=45%>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e86b046d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-26T07:09:49.993525Z",
     "start_time": "2021-09-26T07:09:49.957577Z"
    }
   },
   "source": [
    "- __RNN/LSTM models__ though have the potential for long term memory, but have the drawback not just of __sequential computation__ but also tha __challenge of \"dragging the information\" through many steps__, and __relying on a kind of ordering__. There is no easy possibility for an LSTM to pay direct, local attention to a distant points by \"jumping over\" all the steps inbetween.\n",
    "\n",
    "<a href=\"https://static.wixstatic.com/media/3eee0b_969c1d3e8d7943f0bd693d6151199f69~mv2.gif\"><img src=\"https://drive.google.com/uc?export=view&id=1Np87UUp-5YD_34LyG0Z-sO-OyCShaDcv\" width=45%></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b61939dc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-26T07:09:49.993525Z",
     "start_time": "2021-09-26T07:09:49.957577Z"
    }
   },
   "source": [
    "The attention based solutions promise to enable just that: the possibility to attend to any point (inside the processing window, that is), directly and parallelizably.\n",
    "\n",
    "<img src=\"http://drive.google.com/uc?export=view&id=1xM_y7bGSQ7YkT7kFgev26fwuShWb0fVO\" width=65%>\n",
    "\n",
    "[source](https://paperswithcode.com/paper/self-attention-for-raw-optical-satellite-time)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed45500d",
   "metadata": {},
   "source": [
    "# Motivation: A grid is just a rigid graph...\n",
    "\n",
    "<img src=\"http://drive.google.com/uc?export=view&id=1nHd0o1QpDgQfXN7OS3bPHvf1nQjkEeOn\" width=35%>\n",
    "\n",
    "<img src=\"http://drive.google.com/uc?export=view&id=19yG6dChy79ajNw1C47idR3Eh9zkK8w-o\" width=35%>\n",
    "\n",
    "<img src=\"http://drive.google.com/uc?export=view&id=1Brupz-d0ADtNur00WUqlo-5xYIozPFh8\" width=45%>\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
