{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "9ee91621",
      "metadata": {
        "id": "9ee91621"
      },
      "source": [
        "# How are tasks solved in nlp - a short history\n",
        "\n",
        "## Motivation: the many tasks of NLP\n",
        "\n",
        "\n",
        "<img src=\"http://drive.google.com/thumbnail?id=1kZ0f5h26BkTVNZBCcO1WNqE1qi10od_Q&sz=w1000\" width=55%>\n",
        "\n",
        "[source](https://medium.com/nlplanet/two-minutes-nlp-33-important-nlp-tasks-explained-31e2caad2b1b)\n",
        "\n",
        "NLP as a field is pretty wide in a sense, since __multiple tasks__ (suprvised or unsupervised) can be endeavoured to be solved based on the __same text__ and - importantly - it's __representation__ in a way that is conducive of machine learning. We saw ample examples for this so far."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "edb6fb1e",
      "metadata": {
        "id": "edb6fb1e"
      },
      "source": [
        "## Historical context - A general shift in modeling approaches"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5ecd3dff",
      "metadata": {
        "id": "5ecd3dff"
      },
      "source": [
        "## Broader context\n",
        "\n",
        "<img src=\"http://drive.google.com/uc?export=download&id=1TAyGCRvveLE2EvEvm6aRz26VR0W6nxhq\" width=100%>\n",
        "\n",
        "\n",
        "\n",
        "## More narrowly: Deep Learning\n",
        "\n",
        "<img src=\"http://drive.google.com/uc?export=view&id=1uMlevTlXqOHAZfQHPbOwykEJhYmyMW1p\" width=100%>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bc14e708",
      "metadata": {
        "id": "bc14e708"
      },
      "source": [
        "## \"Paradigms\" - Stages of development in NLP\n",
        "\n",
        "If we focus our attention to the development of NLP (and completely disregard the rule based or \"Ontological\" approaches of [\"Good old-fashioned AI\"](https://en.wikipedia.org/wiki/GOFAI), we can distinguish clear paradigms of model training and problem solving in NLP, that followed each-other in rapid (actually accelerating) succession."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b03706cb",
      "metadata": {
        "id": "b03706cb"
      },
      "source": [
        "### Non-learned feature extraction + custom model"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "000754dd",
      "metadata": {
        "id": "000754dd"
      },
      "source": [
        "<img src=\"http://drive.google.com/uc?export=view&id=1NtZy--0eGGg6S2h6D8hFndv3ko8yPiYb\" width=35%>\n",
        "\n",
        "\n",
        "[source](https://www.semanticscholar.org/paper/A-novel-text-mining-approach-based-on-TF-IDF-and-Dadgar-Araghi/ded138171f35309c0af9ecc98eeffb90f0e8f993)\n",
        "\n",
        "(As late as 2016, surprisingly!)\n",
        "\n",
        "In the now \"classical\" paradigm, the features from textual documents were extracted by an elaborate pipeline of hand crafted transformations, that resulted in the representation of texts in terms of word frequency properties. This representation was then used to train a specific classifier for a given task in a supervised manner."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fd9acc20",
      "metadata": {
        "id": "fd9acc20"
      },
      "source": [
        "### Learned features + custom models \"on top\"\n",
        "\n",
        "<img src=\"http://drive.google.com/uc?export=view&id=1V9w8DwNLQ0w_uLvYCnjl7d9TJjNAzsak\" width=20%>\n",
        "\n",
        "[source](https://www.semanticscholar.org/paper/Word2Vec-model-for-sentiment-analysis-of-product-in-Fauzi/ba511aa8390e4a48e4a8273d9e24c35bcfe4cd96)\n",
        "\n",
        "<img src=\"http://drive.google.com/uc?export=view&id=17h6urUe2IySvvQd-DyoValRYjy5dwB_k\" width=55%>\n",
        "\n",
        "<img src=\"http://drive.google.com/uc?export=view&id=1X2HeILaqtd-dAxrhcRYtlTtMrliiPJPx\" width=70%>\n",
        "\n",
        "[Distributed Representations of Sentences and Documents](https://arxiv.org/abs/1405.4053)\n",
        "\n",
        "The big change happened with the introduction of unsupervised learning based representations, namely [word2vec](https://en.wikipedia.org/wiki/Word2vec) in 2013-14. The main paradigm change was, that an unsipervised predictive training task was found to be extremely efficient in coming up with high quality word representations, that could with simple (like averaging) or more complicated (like [SIF](https://openreview.net/pdf?id=SyK00v5xx)) approaches be utilized as representations for texts, and so, custom models could be trained using them as inputs."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5c6553e4",
      "metadata": {
        "id": "5c6553e4"
      },
      "source": [
        "### End-to-end learned models\n",
        "\n",
        "\n",
        "The next level of complexity - and with it performance - arrived by using pre-trained \"embeddings\" (mainly word2vec) as part of and end-to-end pipeline, where every component was neural, mainly in form of LSTMs or their more complex \"seq2seq\" variants.\n",
        "\n",
        "<img src=\"http://drive.google.com/uc?export=view&id=1cftnuhcP9ZhwsGRo75qN5x2kNHsaVvmw\" width=55%>\n",
        "\n",
        "<img src=\"http://drive.google.com/uc?export=view&id=1qXVsSWsny_UbBxbwtvrpv51dnNbwPG6x\" width=55%>\n",
        "\n",
        "[source](https://core.ac.uk/download/pdf/226439962.pdf)\n",
        "\n",
        "Here, the pre-trained layer of word2vec only represented a useful input transformation, the heavy lifting was still done by the task specificly trained architecture on top of them."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "59cc9c7f",
      "metadata": {
        "id": "59cc9c7f"
      },
      "source": [
        "### Pretrain and finetune paradigm\n",
        "\n",
        "<img src=\"http://drive.google.com/uc?export=view&id=1kNI6DrMJNEMV0rZogmClIQ7187OhWX9w\" width=35%>\n",
        "\n",
        "\n",
        "[Source](https://arxiv.org/abs/2109.01652)\n",
        "\n",
        "Though this paradigm reached fame with the advent of [transformer models](https://arxiv.org/abs/1706.03762), especially [BERT](https://arxiv.org/abs/1810.04805), the big leap in performance in the pretrain-finetune paradigm may be attributable more to the paper about [ULMFiT - Universal Language Model Fine-tuning for Text Classification](https://arxiv.org/abs/1801.06146).\n",
        "\n",
        "The paradigm centers ofn __pretrained deep models__, that get applied to task specific settings by __finetuning__.\n",
        "\n",
        "\n",
        "The breakthrough came, as the __\"gradual unfreezing\"__ method prevented the destruction of the knowledge learned by the neural models during the __\"pre-training\"__ process, so a task specific __fine-tuning__ was deemed to be very successful.\n",
        "\n",
        "<a href=\"https://humboldt-wi.github.io/blog/img/seminar/group4_ULMFiT/Figure_21.png\"><img src=\"https://drive.google.com/uc?export=view&id=1KxNr1UqL_1q7FTNjLv8SXivkuFKB343H\" width=65%></a>\n",
        "\n",
        "#### Cut and/or add a layer\n",
        "\n",
        "<img src=\"http://drive.google.com/uc?export=view&id=1SW3xfa1FYQ1TGXaorJHmcx-wAJpJPj4g\" width=55%>\n",
        "\n",
        "In this paradigm the main method for task customization was to either replace the last \"task\" layer of the network, or add a \"task specific\" layer as a new output."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "02bea613",
      "metadata": {
        "id": "02bea613"
      },
      "source": [
        "#### \"Sidenote\": Transformers\n",
        "\n",
        "\n",
        "With the paper [Attention is all you need](https://arxiv.org/abs/1706.03762) a new, extremely important architecture, the \"transformer\" emerged.\n",
        "\n",
        "<img src=\"http://drive.google.com/uc?export=view&id=1SoM5ha5ZikSd3jRxdpZPCNB4xidhh8Tr\" width=65%>\n",
        "\n",
        "The main advantages of this architecture were:\n",
        "- Parallelizability on GPU (circumventing the LSTM's bottlenecks)\n",
        "- Excellent utilization of information in long context windows (LSTM's effective window size was always debated)\n",
        "\n",
        "This allowed for the __rapid scaling up of model depth__, and combined with the pretrain and finetune paradigm established the dominance of deep pretrained models."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b6704d04",
      "metadata": {
        "id": "b6704d04"
      },
      "source": [
        "##### \"More sidenote\": quadratic complexity\n",
        "\n",
        "<img src=\"http://drive.google.com/uc?export=view&id=1Uafn-mvfStrmhcpHuAyCwAFYoLnHKyi-\" width=65%>\n",
        "\n",
        "Though some serious problems exist in transformers, namely [quadratic computation complexity](https://web.stanford.edu/class/archive/cs/cs224n/cs224n.1194/reports/custom/15839671.pdf), there are methods that attempt to scale transformers to extremely long context sizes, like [Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context](https://arxiv.org/abs/1901.02860), [Big Bird: Transformers for Longer Sequences](https://arxiv.org/pdf/2007.14062.pdf) or more recently [Scaling Transformer to 1M tokens and beyond with RMT](https://arxiv.org/abs/2304.11062).\n",
        "\n",
        "Suffice it to say, that a kind of \"cottage industry\" sprang up with respect to transformer verisons.\n",
        "\n",
        "For a nice genealogy, see:\n",
        "\n",
        "<img src=\"http://drive.google.com/uc?export=view&id=1TnQFZVz5zdOiPF6wN1hzyDYenjc-nHbQ\" width=65%>\n",
        "\n",
        "Source: [The Practical Guides for Large Language Models ](https://github.com/Mooler0410/LLMsPracticalGuide)\n",
        "\n",
        "##### \"Mixture of experts\" - \"Hacks\" to make large transformers practical\n",
        "\n",
        "One of the observations that can help reduce the computational burden of large trasformer networks is, that beyond the attention layer, it is often tha case that the largest number of parameters - thus memory requirement - is represented by the fully connected layers in a transformer block (see above), that operate on an individual token basis. Remember: attention is \"contextual\", that is it works on token combinations, but the fully connected layers are \"per token\".\n",
        "\n",
        "Thus one ide is to replace the \"expert\" fully connected layer with a \"mixture of experts\", that is, smaller fully connected layers, that are selectively activated, and their outputs are weighted together to priduce the final output.\n",
        "\n",
        "For this to be feasible, a simple \"gating\" or \"routing\" newtork element is introduced that generates a softmax over the different \"experts\" (small feed forward NN-s \"addresses\"), then the \"TopK\" of this softmax is taken (and considered as weighting). Based on this, the top K \"experts\" are called upon, so forward pass is done, then their outputs are weighted summed together.\n",
        "\n",
        "\n",
        "<img src=\"http://drive.google.com/uc?export=view&id=12ZxlP6dwJ7Rty17Or4jGWrUfxi6nZD1I\" width=55%>\n",
        "\n",
        "\n",
        "This way, one can show, that this architecture can express a function similar in complexity to the original fully connected layers, but at once, only a certain fraction of it's parameters (depending on K) are active, hence are requiring memory resources.\n",
        "\n",
        "For the history of the MoE idea (which has a lot to do with ensemble methods, by the way), see the [wikipedia entry](https://en.wikipedia.org/wiki/Mixture_of_experts), for it's practical deep learning use: [Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer](https://arxiv.org/abs/1701.06538)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "611282c8",
      "metadata": {
        "id": "611282c8"
      },
      "source": [
        "#### \"Adapter\" finetuning\n",
        "\n",
        "As model sizes became prohibitively large to train on consumer grade hardware, the - in a sense a kind of \"reaching back to old style\" - idea came to __\"freeze\" the pretrained model__, and only take a shallow network to finetune.\n",
        "\n",
        "This shallow network was in the beginning a new, full \"task specific\" output layer, but then more efficient methods like \"PEFT\" or [Parameter-Efficient Transfer Learning for NLP](https://arxiv.org/pdf/1902.00751.pdf) came up with __adatpter layers__, that are built in, task specificly \"after the fact\" finetuneable layers, we can inject to the model \"cheaply\"."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fcb5ed8c",
      "metadata": {
        "id": "fcb5ed8c"
      },
      "source": [
        "<img src=\"http://drive.google.com/uc?export=view&id=1W1XqUMl9Zud7pRldI6qPKkI2cfKvQ_kG\" width=65%>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ef105b5e",
      "metadata": {
        "id": "ef105b5e"
      },
      "source": [
        "\n",
        "<img src=\"http://drive.google.com/uc?export=view&id=1qWN9DVYkhyQ1c3lrbiXqyizaw3kMeg4_\" width=35%>\n",
        "\n",
        "A more modern approach to this is [LoRA: Low-Rank Adaptation of Large Language Models](https://arxiv.org/abs/2106.09685)\n",
        "\n",
        "More detailed explanation can be found [here](https://xiaosean5408.medium.com/fine-tuning-llms-made-easy-with-lora-and-generative-ai-stable-diffusion-lora-39ff27480fda), or an exen more excellent intro [here](https://lightning.ai/pages/community/tutorial/lora-llm/).\n",
        "\n",
        "With the later dominance of really large language models, even the adapter finetuning proved to be prohibitive, since the models often don't fit into GPU memory in their original full floating point representation, so in later practice, \"Quantized\" LoRA, [QLoRA](https://arxiv.org/pdf/2305.14314.pdf) has been introduced, and gained considerable traction.\n",
        "\n",
        "<img src=\"http://drive.google.com/uc?export=view&id=1FTN9KmkfaR597Rvx22MkkwebL3g8-T71\" width=75%>\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "da909dc2",
      "metadata": {
        "id": "da909dc2"
      },
      "source": [
        "### Zero shot learning, \"prompting\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5636c0f3",
      "metadata": {
        "id": "5636c0f3"
      },
      "source": [
        "<img src=\"http://drive.google.com/uc?export=view&id=1YoWhTN3bW9mrpjTH7NHm0S_c69M6__Da\" width=35%>\n",
        "\n",
        "\n",
        "[Source](https://arxiv.org/abs/2109.01652)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "03ab559f",
      "metadata": {
        "id": "03ab559f"
      },
      "source": [
        "Quite quickly after the emerence of large pre-trained models, it was realized, that the models were capable of __non-trivial performance on unseen tasks without any training__. Subsequently, many papers (like [this](https://arxiv.org/pdf/1909.00161.pdf)) started to investigate the phenomena, which revived the fields of [zero shot learning](https://en.wikipedia.org/wiki/Zero-shot_learning) and [few-shot learning](https://en.wikipedia.org/wiki/Few-shot_learning) that were kind of nieche disciplines up till that point."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6022a518",
      "metadata": {
        "id": "6022a518"
      },
      "source": [
        "In contrast with the previous approaches, where some examples were used to modify weights, that is to train the networks, __in the \"zero shot\" learning paradigm, weights don't change__.\n",
        "\n",
        "In this paradigm the models already sotred enough __general knowledge__ in their weights (which are completely frozen), so that they can __output texts that can be taken as answers__.\n",
        "\n",
        "<img src=\"http://drive.google.com/uc?export=view&id=1KZtdSCh4uZ-45ygY4S7ctSHOMwcaar6y\" width=65%>\n",
        "\n",
        "\n",
        "In their paper [Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer](https://arxiv.org/abs/1910.10683) the authors establish the idea, that __the main form of interaction with a model is via it's textual inputs and outputs__, that can represent a wide variety of tasks.\n",
        "\n",
        "Since the performance of certain large models in zero shot context beat the finetuned state-of-the-art, the paragidm of __in context learning__, a form of zero shot learning became dominant.\n",
        "\n",
        "<img src=\"http://drive.google.com/uc?export=view&id=1huLGkkIx4wwzaqGP7IOPh6wW_OwkGOrN\" width=55%>\n",
        "\n",
        "\n",
        "Source: \"GPT3\" - [Language Models are Few-Shot Learners](https://arxiv.org/abs/2005.14165)\n",
        "\n",
        "The main observation later on will be that __the instructions in the context__, that is __the \"prompts\"__ will have a very strong influence on the task specific performance!\n",
        "\n",
        "It is also important to note, that __prompt based \"learning\" has to be strongly distinguished from finetuning__!!! In \"in context learning\" the model weights are completely frozen, so it is kind of a misnomer (albeit extremely popular) to call this paradigm learning at all.\n",
        "\n",
        "Prompting is a way to get a model to execute a task. Learning in the gradient sense is not involed.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "542cd833",
      "metadata": {
        "id": "542cd833"
      },
      "source": [
        "#### Instruction finetuning\n",
        "\n",
        "Further observation was, that if large [\"foundationa models\"](https://fsi.stanford.edu/publication/opportunities-and-risks-foundation-models) models were explicitly finetuned after their initial unsupervised pre-training in a next phase __to follow instructions__ (like in the line of [\"InstructGPT\"](https://arxiv.org/abs/2203.02155)), their performance in solving zero shot tasks in the \"in context learning\" sense (so by responding to prompts) becomes even more impressive.\n",
        "\n",
        "<img src=\"http://drive.google.com/uc?export=view&id=1HCOV0jyp265sSOFsiKmQjypMWtPU-w5U\" width=35%>\n",
        "\n",
        "Source: [Finetuned Language Models Are Zero-Shot Learners](https://arxiv.org/abs/2109.01652)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c174c8a3",
      "metadata": {
        "id": "c174c8a3"
      },
      "source": [
        "<img src=\"http://drive.google.com/uc?export=view&id=16ApApmggJQZ9xzq4DdcCx1ht8o67Gc25\" width=65%>\n",
        "\n",
        "\n",
        "Source: [Finetuned Language Models Are Zero-Shot Learners](https://arxiv.org/abs/2109.01652)\n",
        "\n",
        "Thus, the era of \"instruction finetuned pre-trained Large Language Models\" was born."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5vaOPrFb9k-_",
      "metadata": {
        "id": "5vaOPrFb9k-_"
      },
      "source": [
        "#### Reinforcement learning from human feedback (RLHF)\n",
        "\n",
        "An additional method, building on the results of instruction finetuning is the incorporation of (sparse) human feedback and preferences via reinforcement learning.\n",
        "\n",
        "\n",
        "\n",
        "[Learning to summarize from human feedback\n",
        "](https://arxiv.org/abs/2009.01325)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "LY4SCOIxgE3Y",
      "metadata": {
        "id": "LY4SCOIxgE3Y"
      },
      "source": [
        "##### Motivation\n",
        "- Large language models (LLMs) excellent in producing coherent and consistent text\n",
        "- However, text may often not be exactly what human is expecting e.g. from a question\n",
        "- Fine tune model for specific tasks such as text summarization\n",
        "- Thus this technique requires a pre-trained large language model!\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "qKiRnMfLgokh",
      "metadata": {
        "id": "qKiRnMfLgokh"
      },
      "source": [
        "##### Particular (original) setting - **text summarization**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "acCi0Jlyg825",
      "metadata": {
        "id": "acCi0Jlyg825"
      },
      "source": [
        "<img src=\"http://drive.google.com/uc?export=view&id=1bJEFGUSo5duA341srpxbuOmdpC9Cm7Sd\" width=55%>\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b5iNHpKbhUGO",
      "metadata": {
        "id": "b5iNHpKbhUGO"
      },
      "source": [
        "##### Approach\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bM-VPJSMhlGl",
      "metadata": {
        "id": "bM-VPJSMhlGl"
      },
      "source": [
        "<img src=\"http://drive.google.com/uc?export=view&id=1QGN2SyvnGZPz3F3WPvkd3IwsklvDjDJZ\" width=70%>\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "UHD_-C5Uil9T",
      "metadata": {
        "id": "UHD_-C5Uil9T"
      },
      "source": [
        "**RL part in more detail**\n",
        "- [PPO algorithm](https://openai.com/research/openai-baselines-ppo)\n",
        "- Specific version of actor critique method\n",
        "- Each time step BPE (Byte Pair Encoding)\n",
        "\n",
        "- Full reward $R$ can be written as:\n",
        "$$\n",
        "R(x, y)=r_\\theta(x, y)-\\beta \\log \\left[\\pi_\\phi^{\\mathrm{RL}}(y \\mid x) / \\pi^{\\mathrm{SFT}}(y \\mid x)\\right]\n",
        "$$\n",
        "\n",
        "- Second term penalizes the KL divergence between the learned RL policy $\\pi_\\phi^{\\mathrm{RL}}$ with parameters $\\phi$ and this original supervised model $\\pi^{\\mathrm{SFT}}$\n",
        "- Additional penalty super important as RL model might otherwise find nonsensical results that get high reward from the classifier (in a sense adversarial examples)\n",
        "- Outputs not too different from what the model produces\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "loH_YonolFK8",
      "metadata": {
        "id": "loH_YonolFK8"
      },
      "source": [
        "##### Transfer Learning Task\n",
        "- Same pre-trained algorithm applied to summarization of news articles\n",
        "- Significantly outperforms models trained on supervised basline"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "wuHmyfZymSs-",
      "metadata": {
        "id": "wuHmyfZymSs-"
      },
      "source": [
        "##### Performance comparison\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "tTg_iACymWpe",
      "metadata": {
        "id": "tTg_iACymWpe"
      },
      "source": [
        "<img src=\"http://drive.google.com/uc?export=view&id=13ZkkZOYAE5zy0PzJr6HlLVU7cvC4xCaa\" width=60%>\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b_bBGfbzpT7E",
      "metadata": {
        "id": "b_bBGfbzpT7E"
      },
      "source": [
        "**Note that the paper does not tell us a lot of important details**\n",
        "- What parts of the model are actually retrained\n",
        "- How long the retraining is done for\n",
        "- How exactly the original language model is used (most likely it provides an input for the RL algorithm)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "58420b43",
      "metadata": {
        "id": "58420b43"
      },
      "source": [
        "#### The effect of instruction finetuning\n",
        "\n",
        "<img src=\"http://drive.google.com/uc?export=view&id=1uTcDqPfioaOOgAjb_PA4SNE5ONHLlK_t\" width=75%>\n",
        "\n",
        "(Comparison of Davini 003 and gpt3.5 of OpenAI)\n",
        "\n",
        "Since the human preference for a language system is to have an interactive conversation, not just a leading a monologue with itself, the main effect of RLHF and other instruction finetuning is **the increased adherence of the models to roles and \"roleplay\"**.\n",
        "\n",
        "This not just makes them practically useful as \"chat\", aka. dialogue systems, but will later on have serious consequences for autonomuos behavior."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8824c036",
      "metadata": {
        "id": "8824c036"
      },
      "source": [
        "#### Sidenote: on the openness of AI\n",
        "\n",
        "As the instruction finetuning results show, good quality (instruction) data is still important, publication of finetune procedures and datasets would be key, and in this regard, the leading research group at OpenAI is not exactly transparent, and protects it's business interests.\n",
        "\n",
        "Thus, such initiatives as [\"The Pile\"](https://pile.eleuther.ai/) as a pretraing dataset, [OpenAssistant's dataset](https://github.com/LAION-AI/Open-Assistant/blob/main/docs/docs/data/datasets.md) for instruction finetuning, open for research models like [LLaMA]() or fully open source models like [Pythia](https://github.com/EleutherAI/pythia) are of vital importance."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1bb4686e",
      "metadata": {
        "id": "1bb4686e"
      },
      "source": [
        "# Works, but why?\n",
        "\n",
        "SInce the \"zero shot\" paradigm represents quite a strong change from what used to be the standard for machine learning, some more investigation of the mechanisms behind it can be useful."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bee7ace8",
      "metadata": {
        "id": "bee7ace8"
      },
      "source": [
        "## What's in the box? - Investigation of structural properties of transformers\n",
        "\n",
        "\n",
        "### What behavior do the (pre- and instruction) trainings  emphasize?\n",
        "\n",
        "It is important to understand, that already the pre-training of language models - since it is done on an extremely wide variety of documents - emphasizes contextuality, that is, a model can not hope to solve a task sufficietly well, if it does not influence it's generation to adhere to local contexts.\n",
        "\n",
        "The explicit instriction finetuning aprpoaches, (such as [this](https://arxiv.org/abs/2203.02155)) give even more emphasis to context, since in case of dialogues for example, people explicitly prefer in context consistent behavior (like \"remembering\" past conversation steps) for the model, thus give __extremely strong signal to enhance in context generation__.\n",
        "\n",
        "We can see this as a strong push towards enhancing the abilities of models in __in context learning__, that is, their __few and zero shot performance__.\n",
        "\n",
        "### In context learning and \"induction heads\"\n",
        "\n",
        "Recently, some in-depth analysis was carried out that identified (amongst others) one important mechanism that modern Large Language Models based on the transformer architecture learn to solve their tasks. The researchers gave the name __\"induction heads\"__ to the attention patterns that were most frequent.\n",
        "\n",
        "<img src=\"http://drive.google.com/uc?export=view&id=1InObwz_wR9ry32lDLPHeS72eySeoJYhj\" width=65%>\n",
        "\n",
        "The basic induction head is learning a bigram like model of \"if this then that\" probability association between a word and the one that most frequently follows it. This enhances the ability of the model to utilize bigram re-occurence __in context__ to give nice baseline predictions.\n",
        "\n",
        "Over and beyond these simple heads more sophisticated complex \"induction\" mechaisms are learned by the model (given it has at least 2 hidden attention layers - in the broad spirit of the [Cybenko theorem](https://en.wikipedia.org/wiki/Universal_approximation_theorem)).\n",
        "\n",
        "<img src=\"http://drive.google.com/uc?export=view&id=1CJu1izJWYjKArukjYHzw1Hw9KGYhRKlk\" width=65%>\n",
        "\n",
        "It is to be emphasized, that __induction heads are geared towards in context learning__, so the strong instruction following performance of modern (especially instruction finetuned) LLMs is vindicated. In a sense, one can argue, that this is a case of adding __human bias__ to the models - albeit in a __positive sense__!\n",
        "\n",
        "For a more in-depth introduction see:\n",
        "\n",
        "[In-context Learning and Induction Heads](https://transformer-circuits.pub/2022/in-context-learning-and-induction-heads/index.html)\n",
        "\n",
        "and"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c9b32448",
      "metadata": {
        "id": "c9b32448",
        "outputId": "79bcd06c-1db0-4248-87bf-60302013c8ca"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/pC4zRb_5noQ?si=0rkaDODBQPAwucmd\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen></iframe>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "%%HTML\n",
        "<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/pC4zRb_5noQ?si=0rkaDODBQPAwucmd\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen></iframe>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "56e89142",
      "metadata": {
        "id": "56e89142"
      },
      "source": [
        "\n",
        "(For other curios phenomena, like the \"SolidGoldMagikarp\" phenomena of __\"glitch tokens\"__, and for a better in-depth understandign of the trained vectorspace see [here](https://www.lesswrong.com/posts/aPeJE8bSo6rAFoLqg/solidgoldmagikarp-plus-prompt-generation#A_possible__partial_explanation).)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7935634d",
      "metadata": {
        "id": "7935634d"
      },
      "source": [
        "## Too big to fail? - Model sizes and scaling laws\n",
        "\n",
        "With the advent of transformers, \"scaling\" in therms of parameters became possible. With the GPT line of work, and the arising of the \"zero shot paradigm\" it became a widespread experience, that larger models offer better performance.\n",
        "\n",
        "Thus, __scaling to larger and larger sizes became the imperative__, and a kind of \"size race\" ensued.\n",
        "\n",
        "<img src=\"http://drive.google.com/uc?export=view&id=1yB8MgigkQgyGY-BdwooGp84auRwRW9ID\" width=65%>\n",
        "\n",
        "[source](https://developer.nvidia.com/blog/using-deepspeed-and-megatron-to-train-megatron-turing-nlg-530b-the-worlds-largest-and-most-powerful-generative-language-model/)\n",
        "\n",
        "<img src=\"http://drive.google.com/uc?export=view&id=14nvvS_xdSpzFTFQKJq4hbwZYxwqS1r0b\" width=65%>\n",
        "\n",
        "[source](https://lifearchitect.ai/models/)\n",
        "\n",
        "The assumption, that the size of the models is crucial for their performance was first formalized in the paper [Scaling Laws for Neural Language Models](https://arxiv.org/abs/2001.08361)\n",
        "\n",
        "<img src=\"http://drive.google.com/uc?export=view&id=1K4cRzFEy5F4vjzp1RSo3iAdVn7L1cIf8\" width=75%>\n",
        "\n",
        "The paper - and the substantial amount of followup work - established a notion, that training larger and larger models on increasingly huge datasets predictably increases perfromance.\n",
        "\n",
        "Later on, though, in th work [Training Compute-Optimal Large Language Models](https://arxiv.org/pdf/2203.15556.pdf) it was found, that maybe training dataset size has stronger influence on the final result than the sheer model size. ([Later work](https://github.com/EleutherAI/pythia) also seems to corroborate this finding.)\n",
        "\n",
        "<img src=\"http://drive.google.com/uc?export=view&id=1fzH_tmjFdGGYIOZJkdZ4Y5vdDWYEgh0W\" width=45%>\n",
        "\n",
        "Source: [Chinchilla's wild implications](https://www.alignmentforum.org/posts/6Fpvch8RR29qLEWNH/chinchilla-s-wild-implications)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "254f21ec",
      "metadata": {
        "id": "254f21ec"
      },
      "source": [
        "### Do we REALLY need that big of a scale?\n",
        "\n",
        "In spite of the popularity of the notion, that we need huge scales in number of parameters for complex behavior to \"emerge\" in Large Language Models, some evidence is being collected to the contraty:\n",
        "\n",
        "The research presented recently in the paper [Are Emergent Abilities of Large Language Models a Mirage?](https://arxiv.org/abs/2304.15004) points towards a different explanation, namely that there is no \"sudden appearance\" of more sophisticated abilities, just as we scale models, they get gradually better and better (we just measure them with performance metrics that skew the result, eg. there is non-zero performance of zero shot learning in small models also, just it is not expressible in terms of accuracy).\n",
        "\n",
        "<img src=\"http://drive.google.com/uc?export=view&id=1AkwWXPzyInjV4InRh92spJHx0BKepZs4\" width=65%>\n",
        "\n",
        "If this turns out to be true, we might be tempted to conlude, that size is not the only factor that gives good performance for LLMs, but especialy the instruction finetuning seems to be a reasonable bias towards what we consider useful. This would mean, that we could potentially get away with way smaller model sizes than currently employed, which is a benefit, since the sheer scale of state-of-the-art models is in itself a barrier, thus hampering open innovation."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ef03cae9",
      "metadata": {
        "id": "ef03cae9"
      },
      "source": [
        "# How to use LLMs in practice?"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "53734b60",
      "metadata": {
        "id": "53734b60"
      },
      "source": [
        "## Prompts"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "904da828",
      "metadata": {
        "id": "904da828"
      },
      "source": [
        "### How to write good prompts?\n",
        "\n",
        "Motto:\n",
        "\n",
        "__Context is everything, keep the context \"in mind\"!__\n",
        "\n",
        "#### Anatomy of a prompt\n",
        "\n",
        "<img src=\"http://drive.google.com/uc?export=view&id=15NXmYlHop5H47nsTFE1woojWJOJmaMM_\" width=90%>\n",
        "\n",
        "#### \"Formalization\"\n",
        "\n",
        "Also, \"formalization\" helps. The usage of markings and structuring aids in:\n",
        "- clarifies roles\n",
        "- creates \"slots\" (which will be of crucial importance!\n",
        "\n",
        "<img src=\"http://drive.google.com/uc?export=view&id=1x7U3ylG9vrjNSELraoKhIV7NPnjB0IJw\" width=90%>\n",
        "\n",
        "#### Example\n",
        "\n",
        "<img src=\"http://drive.google.com/uc?export=view&id=1uMt0r5RRrX5d82qHkbN1Sng7XvDxD29X\" width=90%>\n",
        "\n",
        "__More about promting [here](https://www.youtube.com/watch?v=xnXDsYquT5A&list=PLey6rpDz-3B9fymKhp_TPOomx-1_iJxCY).__"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a690975a",
      "metadata": {
        "id": "a690975a"
      },
      "source": [
        "### Simple strategies for building prompts\n",
        "\n",
        "As a background, it worth noting, that the instruction finetuning datasets themselves are giving a certain structural bias to the models, so some approaches to prompting might be more efficient than others. If any open information would be available fro eg. ChatGPT, it would simplify the manual buildup of prompts.\n",
        "\n",
        "For some models, like the Open Source [Alpaca](https://github.com/tatsu-lab/stanford_alpaca), we do have information about the distribution / structure of the instruction finetuning datasets. This can be of interest for efficient prompting:\n",
        "\n",
        "<img src=\"http://drive.google.com/uc?export=view&id=1K6gxgSWosqwSs930mjAAVzUiExWJdMlV\" width=80%>\n",
        "\n",
        "Beyond just \"common sense\", for the manual construction of prompts some simple strategies can help a lot:\n",
        "\n",
        "- __Trial and error__: iterate, refine continuoously\n",
        "- __Refinement__: Start simple, gradually add complexity\n",
        "- __Decomposition__: Try to decompose tasks to smaller elements\n",
        "- __\"Nesting\"__: From smaller elements, compose more and more high level solutions\n",
        "\n",
        "We will see some more elaborate \"reasoning\" blow, why these \"decomposition\" and \"step-by-step\" approaches can work well."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "30d1f976",
      "metadata": {
        "id": "30d1f976"
      },
      "source": [
        "#### \"Reverse prompting\"\n",
        "\n",
        "Since prompting is basically goal oriented creative work in the domain of words, practitioners soon realized, that one can use an LLM (given some solved examples) to generate some prompts for LLMs (the same or different) to solve the given task. This approach can be called \"reverse prompt engineering\".\n",
        "\n",
        "<img src=\"http://drive.google.com/uc?export=view&id=1Nv9HaWQWRwk4JDBn1QQ1extuL4UiMMhf\" width=45%>\n",
        "\n",
        "[source](https://the-prompt-engineer.beehiiv.com/p/8-reverse-prompt-engineering)\n",
        "\n",
        "(It is important to note, that a \"jailbreaking\" technique exists with the same name, derived from \"prompt inhection\". For more details on that, see [here](https://www.latent.space/p/reverse-prompt-eng).)\n",
        "\n",
        "This approach already points towards the fact, that prompting can be nderstood as a \"search\" problem, hence the apparatus of computer science (namely search and optimization algorithms) can be unleashed upon it."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6f97a100",
      "metadata": {
        "id": "6f97a100"
      },
      "source": [
        "### The automated search for prompts"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "-nZJy8NOG3uF",
      "metadata": {
        "id": "-nZJy8NOG3uF"
      },
      "source": [
        "#### Motivation and context: Controlling Neural Text Generation"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9tw5WBBd31MP",
      "metadata": {
        "id": "9tw5WBBd31MP"
      },
      "source": [
        "**Example**: the Frankfurt School wants to write an email to potential MBA students convincing them to join the MBA programme\n",
        "\n",
        "There are three requirments\n",
        "\n",
        "1.   The language model has to use content that is true about the Frankfurt School\n",
        "2.   The text should be as effective as possible in reaching its goal - in our case applications to the School's MBA programme\n",
        "3.   In many cases there are only few examples of texts and target values (click through rates) available\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "BPz-JOTXG6uz",
      "metadata": {
        "id": "BPz-JOTXG6uz"
      },
      "source": [
        "**The setting**\n",
        "- If LLMs are creating consistent answers of high quality a key question is how to get the answer required for the particular business or research context (closely related to grounding)\n",
        "- Task may include getting text that meets particular (1) content, (2) style and (3) quantitative metrics, such as optimizing for a click through rate\n",
        "- Sometimes it may be necessary to build a model that completes this task: generating a prompt that fits with our objective\n",
        "- The less labled examples we need to achieve the target the more valuable is the business application\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "pk78OaRB3RN5",
      "metadata": {
        "id": "pk78OaRB3RN5"
      },
      "source": [
        "**Typical process for controlling neural text generation**\n",
        "\n",
        "<img src=\"http://drive.google.com/uc?export=view&id=1253xgMcQ8R8dtZRWX-vD3LT-9sOjUmm4\" width=800 heigth=800>\n",
        "\n",
        "\n",
        "[source](https://arxiv.org/pdf/2201.05337.pdf)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bN4eyl5H4sQo",
      "metadata": {
        "id": "bN4eyl5H4sQo"
      },
      "source": [
        "**Taxonomy of control conditions**\n",
        "\n",
        "<img src=\"http://drive.google.com/uc?export=view&id=1Qst_n8NZu42YYr_ab9OyHTKiLhaefdII\" width=600 heigth=600>\n",
        "\n",
        "\n",
        "[source](https://arxiv.org/pdf/2201.05337.pdf)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9Cj5K5YwDjhy",
      "metadata": {
        "id": "9Cj5K5YwDjhy"
      },
      "source": [
        "#### SOFT methods\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bCZTh29gcSFp",
      "metadata": {
        "id": "bCZTh29gcSFp"
      },
      "source": [
        "Soft prompts with machine learning/optmization\n",
        "\n",
        "- Soft as they are not discrete (numbers in the vectors can be changed gradually)\n",
        "\n",
        "see for example the following paper on [prefix tuning](https://aclanthology.org/2021.acl-long.353/)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "oJeqeH0FDqRe",
      "metadata": {
        "id": "oJeqeH0FDqRe"
      },
      "source": [
        "##### Permutation based - auto- prompt\n",
        "\n",
        "[**AUTOPROMPT: Eliciting Knowledge from Language Models\n",
        "with Automatically Generated Prompts**](https://arxiv.org/pdf/2010.15980.pdf)\n",
        "\n",
        "- Add to the input sequence a sequence of additional tokens (called trigger tokens) that can be learned is added to the prompt\n",
        "\n",
        "\n",
        "\n",
        "<img src=\"http://drive.google.com/uc?export=view&id=1oyLil3lppN84reSi1paX7NgpgszinSzu\" width=700 heigth=700>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8-ZL1DPEF116",
      "metadata": {
        "id": "8-ZL1DPEF116"
      },
      "source": [
        "**How to optimize**\n",
        "\n",
        "- Requires access to vector outputs of model at each level (embeddings)\n",
        "\n",
        "- Feed prompt into the language model produces - measure probability distribution $p\\left([\\mathrm{MASK}] \\mid \\boldsymbol{x}_{\\text {prompt }}\\right)$ over mask tokens (in this case negative vs positive sentiment)\n",
        "\n",
        "- At each step compute a first-order approximation of the change in the log-likelihood that would be produced by swapping the $j$ th trigger token $x_{\\text {trig }}^{(j)}$ with another token $w \\in \\mathcal{V}$\n",
        "\n",
        "-  Identify a candidate set $\\mathcal{V}_{\\text {cand }}$ of the top- $k$ tokens estimated to cause the greatest increase:\n",
        "$$\n",
        "\\mathcal{V}_{\\text {cand }}=\\text { top } k\\left[\\boldsymbol{w}_{\\text {in }}^T \\nabla \\log p\\left(y \\mid \\boldsymbol{x}_{\\text {prompt }}\\right)\\right]\n",
        "$$\n",
        "\n",
        "- Chose best candiates.."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Pdxro6T2a-bD",
      "metadata": {
        "id": "Pdxro6T2a-bD"
      },
      "source": [
        "Further development using similar techniques:\n",
        "[Making Pre-trained Language Models Better Few-shot Learners\n",
        "](https://arxiv.org/pdf/2012.15723.pdf\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "wKiPczENalOj",
      "metadata": {
        "id": "wKiPczENalOj"
      },
      "source": [
        "##### Reinforcement learning based methods\n",
        "\n",
        "1. How the space of possible prompts is searched/ pre-slected : **RL**\n",
        "2. Evaluating performance of proposals: **Train separate classifier or Use Existing language model/ fine tune for classification**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "L1eEWht_bPIT",
      "metadata": {
        "id": "L1eEWht_bPIT"
      },
      "source": [
        "[**Efficient (Soft) Q-Learning\n",
        "for Text Generation with Limited Good Data**](https://arxiv.org/pdf/2106.07704.pdf)\n",
        "\n",
        "- Example of this approach\n",
        "- Uses RL for generating suitable prompts\n",
        "- Trains classifier to evaluate the suitabliy of changed prompt"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "rgBiCeudoEIj",
      "metadata": {
        "id": "rgBiCeudoEIj"
      },
      "source": [
        "**Motivation of the paper**\n",
        "\n",
        "- *MLE (Maximum likelihood estimates)* need large amounts of supervised data -> problem when literally no supervised data is available\n",
        "- *RL learning* advantage: learning reward function, for discrete non differentiable events\n",
        "- *RL Problems:* unstable/ notoriously difficult to train: (1) sparse reward: only once text is finished; (2) large action space - vocab of millions of words  "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "nx4y0xw-73p3",
      "metadata": {
        "id": "nx4y0xw-73p3"
      },
      "source": [
        "<img src=\"http://drive.google.com/uc?export=view&id=1PVuwAoFTXN1peGTGrGUmmrcjWczYwXQS\" width=600 heigth=600>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "vuOPIqbWCA6P",
      "metadata": {
        "id": "vuOPIqbWCA6P"
      },
      "source": [
        "**Propose *soft* Q-Learning**\n",
        "\n",
        "-  Maximum-entropy (MaxEnt) extension to the standard (hard) Q-learning\n",
        "- Agent is encouraged to optimize the reward while staying as stochastic as possibleÂ´\n",
        "- Objective $J_{\\operatorname{MaxEnt}}(\\pi)=$ $\\mathbb{E}_{\\tau \\sim \\pi}\\left[\\sum_{t=0}^T \\gamma^t r_t+\\alpha \\mathcal{H}\\left(\\pi\\left(\\cdot \\mid \\boldsymbol{s}_t\\right)\\right)\\right]$\n",
        "- Augments the vanilla $J(\\pi)$ with the additional Shannon entropy term $\\mathcal{H}$ with coefficient $\\alpha $\n",
        "- Connects $Q$-values to the familiar output logits of a text generation model, which enables straightforward implementation of the SQL formulation"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "clZ-Q3xGxnK_",
      "metadata": {
        "id": "clZ-Q3xGxnK_"
      },
      "source": [
        "**Soft q-learning replaces the argmax operator with a Softmax**\n",
        "- Connection of the $Q$-values with the logits, i.e., outputs right before the softmax layer.\n",
        "- Following relationship between optimal policy $\\pi^*$ and action-value $Q^*$ holds (Haarnoja et al., 2017; Schulman et al., 2017):\n",
        "$$\n",
        "\\pi^*(a \\mid s)=\\frac{\\exp Q^*(\\boldsymbol{s}, a)}{\\sum_{a^{\\prime}} \\exp Q^*\\left(\\boldsymbol{s}, a^{\\prime}\\right)} .\n",
        "$$\n",
        "This form is highly reminiscent of the softmax layer of the generation model\n",
        "\n",
        "- **Key point:** the softmax leads to exploration, but not in an epsylon greedy style where each non argmax action is equally likely -> no arbitrary sampling- this is language after all!\n",
        "- The temperature of the softmax can be decreased over time to to make the model converge\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "IAiEdmQwyU8E",
      "metadata": {
        "id": "IAiEdmQwyU8E"
      },
      "source": [
        "**Connection to softmax 2**\n",
        "\n",
        "In other words, the model output $f_\\theta(a \\mid s)$, originally interpreted as the \"logit\" of token a given the preceding tokens $s$, is now re-interpreted as the $Q$-value of action $a$ in state $s$. When achieving optimality, $f_{\\theta^*}(a \\mid s)$, namely $Q^*(s, a)$, represents the best possible future reward achievable by generating token $a$ in state $s$. Similarly, the full generation $\\operatorname{model} p_\\theta(a \\mid \\boldsymbol{s})$ in Eq. (1) that applies softmax to $f_\\theta$ now precisely corresponds to the policy $\\pi_\\theta$ induced from $Q_\\theta(s, a)$. That is,\n",
        "$$\n",
        "\\begin{aligned}\n",
        "\\pi_\\theta(a \\mid \\boldsymbol{s}) & =\\frac{\\exp Q_\\theta(\\boldsymbol{s}, a)}{\\sum_{a^{\\prime}} \\exp Q_\\theta\\left(\\boldsymbol{s}, a^{\\prime}\\right)} \\\\\n",
        "& \\equiv \\frac{\\exp f_\\theta(a \\mid \\boldsymbol{s})}{\\sum_{a^{\\prime}} \\exp f_\\theta\\left(a^{\\prime} \\mid \\boldsymbol{s}\\right)}=p_\\theta(a \\mid \\boldsymbol{s}) .\n",
        "\\end{aligned}\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "piRKk-11zcuD",
      "metadata": {
        "id": "piRKk-11zcuD"
      },
      "source": [
        "**Additionally effective training with path consistency**\n",
        "\n",
        "- Adapt unified path consistency [learning $(P C L)$](https://dl.acm.org/doi/pdf/10.5555/3294996.3295037) (excelled in game control)\n",
        "- Good for directly learning RL algorithm on past data\n",
        "- Simple value vased reinfocement learning - does not require actor critique\n",
        "- PCL-based training updates $Q$-values of all tokens at once through a connection between the value function and the induced policy\n",
        "-  Optimal policy $\\pi^*$ and the optimal state value function $V^*$  in SQL must satisfy the following consistency property for all states and actions:\n",
        "$$\n",
        "V^*\\left(\\boldsymbol{s}_t\\right)-\\gamma V^*\\left(\\boldsymbol{s}_{t+1}\\right)=r_t-\\log \\pi^*\\left(a_t \\mid \\boldsymbol{s}_t\\right), \\forall \\boldsymbol{s}_t, a_t\n",
        "$$\n",
        "\n",
        "- Accordingly, the PCL-based training attempts to encourage the satisfaction of the consistency with the following regression objective $\\mathcal{L}_{\\mathrm{SQL}, \\mathrm{PCL}}(\\boldsymbol{\\theta})$ :\n",
        "$$\n",
        "\\mathbb{E}_{\\pi^{\\prime}}\\left[\\frac{1}{2}\\left(-V_{\\bar{\\theta}}\\left(\\boldsymbol{s}_t\\right)+\\gamma V_{\\bar{\\theta}}\\left(\\boldsymbol{s}_{t+1}\\right)+r_t-\\log \\pi_\\theta\\left(a_t \\mid \\boldsymbol{s}_t\\right)\\right)^2\\right],\n",
        "$$\n",
        "where $\\pi_\\theta$ is the induced policy; $V_{\\bar{\\theta}}$  depends on the target $Q_{\\bar{\\theta}}$ network (i.e., a slow copy of the $Q_\\theta$ to be learned), and recall that $\\pi^{\\prime}$ is an arbitrary behavior policy (e.g., data distribution). Please see"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "aaqh5lFI7-jQ",
      "metadata": {
        "id": "aaqh5lFI7-jQ"
      },
      "source": [
        "\n",
        "<img src=\"http://drive.google.com/uc?export=view&id=115CR08rGbOpmKxu1YIg-b9ugvT8b-RU5\" width=600 heigth=600>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "FBH3yxX41SZp",
      "metadata": {
        "id": "FBH3yxX41SZp"
      },
      "source": [
        "**Multi-step PCL for Sparse Reward**\n",
        "$$\n",
        "V^*\\left(\\boldsymbol{s}_t\\right)-\\gamma^{T-t} V^*\\left(s_{T+1}\\right)=\\sum_{l=t}^T \\gamma^{l-t}\\left(r_l-\\log \\pi^*\\left(a_l \\mid \\boldsymbol{s}_l\\right)\\right),\n",
        "$$\n",
        "where the value of past-terminal state is zero, $V^*\\left(s_{T+1}\\right)=0$; and the rewards are only available at the end, $\\sum_{l=t}^T \\gamma^{l-t} r_l=\\gamma^{T-t} r_T$. We can then come to the following multi-step objective function $\\mathcal{L}_{\\mathrm{SQL}}$ PCL-ms $(\\boldsymbol{\\theta})$,\n",
        "$\\mathbb{E}_{\\pi^{\\prime}}\\left[\\frac{1}{2}\\left(-V_{\\bar{\\theta}}\\left(\\boldsymbol{s}_t\\right)+\\gamma^{T-t} r_T-\\sum_{l=t}^T \\gamma^{l-t} \\log \\pi_\\theta\\left(a_l \\mid \\boldsymbol{s}_l\\right)\\right)^2\\right]$.\n",
        "We can see the objective side-steps the need to bootstrap intermediate value functions $V_{\\bar{\\theta}}\\left(s_{t^{\\prime}}\\right)$ for $t^{\\prime}>t$. Instead, it directly uses the non-zero end reward $r_T$ to derive the update for $\\boldsymbol{\\theta}$. Please see Figure 2 (right) for an illustration. In practice, we combine the single- and multi-step objectives (Eqs. 7 and 9) together for training."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "mWNuxDzN5FIf",
      "metadata": {
        "id": "mWNuxDzN5FIf"
      },
      "source": [
        "**Reward**\n",
        "we use a distilled GPT-2 model\n",
        "as the pretrained LM to be controlled.\n",
        "For rewards, we use the topic accuracy of the con\u0002tinuation sentences measured by a zero-shot clas\u0002sifier, plus the the log-likelihood of continuation\n",
        "sentences as the language quality reward measured\n",
        "by a distilled GPT-2"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "jw2Tykxw6Vb-",
      "metadata": {
        "id": "jw2Tykxw6Vb-"
      },
      "source": [
        "\n",
        "<img src=\"http://drive.google.com/uc?export=view&id=1ezZ4PCL2LGJl75JG_TAI_aQqmG9Tuv0e\" width=600 heigth=600>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "sazE2UPXz7qT",
      "metadata": {
        "id": "sazE2UPXz7qT"
      },
      "source": [
        "Other RL based approaches:\n",
        "[RLPrompt: Optimizing Discrete Text Prompts with Reinforcement Learning\n",
        "](https://arxiv.org/abs/2205.12548)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "213d1091",
      "metadata": {
        "id": "213d1091"
      },
      "source": [
        "#### HARD methods\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "37c6b733",
      "metadata": {
        "id": "37c6b733"
      },
      "source": [
        "##### Evolutionary prompt serch\n",
        "\n",
        "What if we don't want to deal with gradients? - Evolution to the rescue!\n",
        "\n",
        "**[GPS: Genetic Prompt Search for Efficient Few-shot Learning](https://arxiv.org/pdf/2210.17041.pdf)**\n",
        "\n",
        "The discrete nature of prompts - since they are ideally based on discrete tokens - makes the application of gradient based procedures on them rather cumbersome. Luckily optimization literature has a rich set of non-gradient based optimizers that can come in handy for this task.\n",
        "\n",
        "One family of such optimizers is \"evolutionary computation\", that multiple papers propose to use in automated prompt tuning.\n",
        "\n",
        "The basic idea of an evolutionary algorithm is to use a __population of solutions, that undergo evolutionary operators, that is:\n",
        "- __Selection__\n",
        "- __Crossover__\n",
        "- __Mutation__\n",
        "\n",
        "to produce an ever increasing quality.\n",
        "\n",
        "(see details in the addon material about optimization)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "jG4qz1T3gZgI",
      "metadata": {
        "id": "jG4qz1T3gZgI"
      },
      "source": [
        "**Basic idea**\n",
        "- Use some heuristics to generate new prompts from an inital one\n",
        "- Chose top prompts through a fitness function (genetic algorithm inspired)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "LNCds4RtgZcb",
      "metadata": {
        "id": "LNCds4RtgZcb"
      },
      "source": [
        "\n",
        "<img src=\"http://drive.google.com/uc?export=view&id=18pR6S9WG-4ZnNJUogsu-n0ZJ3x-BTetH\" width=700 heigth=700>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "CKmEyZTagZYM",
      "metadata": {
        "id": "CKmEyZTagZYM"
      },
      "source": [
        "<img src=\"http://drive.google.com/uc?export=view&id=1KHOYEzRP8Cxcuj3AwIxXBl0joUtLc3q1\" width=350 heigth=350>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "DlDUYEd_gZAv",
      "metadata": {
        "id": "DlDUYEd_gZAv"
      },
      "source": [
        "**How are prompts generated?**\n",
        "- Back Translation\n",
        "- Cloze\n",
        "- Sentence continuation"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "VZFGGCVrkn8p",
      "metadata": {
        "id": "VZFGGCVrkn8p"
      },
      "source": [
        "**Evaluation**:\n",
        "Score on accuracy for the required task for the train dataset (no surrogate model)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "871886b6",
      "metadata": {
        "id": "871886b6"
      },
      "source": [
        "##### Newer version\n",
        "\n",
        "The work [Connecting Large Language Models with Evolutionary Algorithms Yields Powerful Prompt Optimizers](https://arxiv.org/abs/2309.08532) is another attempt to apply genetic algorithms to prompting.\n",
        "\n",
        "The general framework is applied to prompts as follows:\n",
        "\n",
        "<img src=\"http://drive.google.com/uc?export=view&id=1ubC0uDwbB7RsjgXzrq2IKU8SFoam1rr6\" width=55%>\n",
        "\n",
        "__Some notable modifications of the general algorithm:__\n",
        "\n",
        "- \"__Initial population__: Based on our notation that most existing prompt-based methods neglect human knowledge providing efficient priori initialization, we apply several manual prompts as the initial population to leverage the wisdom of humans as prior knowledge. Besides, EAs typically start from randomly generated solutions, resulting in a diverse population and avoiding being trapped in a local optimum. Accordingly, we also introduce some prompts generated by LLMsinto the initial population.\"\n",
        "- __Selection__ is done in a standard \"roulette wheel\" fashion\n",
        "\n",
        "\n",
        "Maybe the moast important contribution of the paper is to elaborate two approaches for the \"crossover\" and \"mutaiton\" operators, that are heavily utilizing LLMs themselves for an intelligent, guided approach for searching in the neighborhood of good solutions.\n",
        "\n",
        "One is based on \"classical\" evolution:\n",
        "\n",
        "<img src=\"http://drive.google.com/uc?export=view&id=1xPag5h3pdO0EdIXHpjLLYFpAVSBP7gYR\" width=75%>\n",
        "\n",
        "The other is based on \"differential evolution\":\n",
        "\n",
        "<img src=\"http://drive.google.com/uc?export=view&id=1xFvAnaXUE0Cg505KPrtm0TrRcE0kBApO\" width=70%>\n",
        "\n",
        "Both approaches seem to offer superior performance to other prompt searching methods, and most importantly to human manual prompting baselines."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "105ff6ed",
      "metadata": {
        "id": "105ff6ed"
      },
      "source": [
        "### Still fails sometimes!\n",
        "\n",
        "#### Hallucination\n",
        "\n",
        "A major form of bad performance - and a frivolously often cited shortcoming - of earlier LLMs was hallucination, especially in numeric and reasoning type tasks.\n",
        "\n",
        "Some vocal critics of the LLM paradigm went so far as to call the whole paradigm \"flawed\" and \"useless\" based on the poor performance of (mainly non instruction finetuned) LLMs on reasoning tasks.\n",
        "\n",
        "This resulted in considerable focus from the side of the scientific community.\n",
        "\n",
        "<img src=\"http://drive.google.com/uc?export=view&id=1dkUAL_WV1TtZJCnZhfMfGCqtAr9vl44Z\" width=65%>\n",
        "\n",
        "In the work [Chain-of-Thought Prompting Elicits Reasoning in Large Language Models](https://arxiv.org/abs/2201.11903) the authors realized, that instead of the final task, the __LLM is made to produce a step-by-step reasoning__ for the task solution, it's __final task performance becomes significantly better__!\n",
        "\n",
        "This lead to \"chain-of-thought prompting\" to become the de-facto standard solution strategy.\n",
        "\n",
        "##### Chain-of-thought: \"Let's think step by step!\"\n",
        "\n",
        "In the work [Large Language Models are Zero-Shot Reasoners](https://arxiv.org/pdf/2205.11916.pdf) it is shown, that this kind of strategy, namely only hinting towards chain of thought in the prompt for an LLM in a task can ellicit strong reasoning performance.\n",
        "\n",
        "<img src=\"http://drive.google.com/uc?export=view&id=1uyJR4LNzRVsh1dYpAevHyI3YP1t8VgvG\" width=75%>\n",
        "\n",
        "<img src=\"http://drive.google.com/uc?export=view&id=1GQJDP4cGJpV8JCMh5Ggk1TeYxNS8TnvG\" width=65%>\n",
        "\n",
        "\"Let's think step-by-step!\" became a standard part in nearly every well \"engineered\" prompt.\n",
        "\n",
        "**But why does this work?**\n",
        "\n",
        "In his [excellent course](https://www.deeplearning.ai/short-courses/chatgpt-prompt-engineering-for-developers/) Andrew Ng uses the metaphor **\"giving models time to think\"**.\n",
        "\n",
        "<img src=\"http://drive.google.com/uc?export=view&id=1v6gD_EBTxbgueJWlincLdj859RYCqhYJ\" width=65%>\n",
        "\n",
        "\n",
        "This can be understood as **the LLM laying down a reasoning path by decomposing the problem to smaller steps as it moves forward in the generation process, thus effectively utilizing the output window as a \"mental scratchpad\"**.\n",
        "\n",
        "This idea will be really important later on."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1f7fdfcb",
      "metadata": {
        "id": "1f7fdfcb"
      },
      "source": [
        "##### \"Advanced\" versions of thinking step by step\n",
        "\n",
        "Based on the observed phenomena of \"giving time to think\", a whole area of experimentation started to try out newer and newer strategies for enhancing reasoning.\n",
        "\n",
        "Some version of the same \"thought\" (in approximate original publication time order):\n",
        "\n",
        "1. [\"Memetic proxy\"](https://arxiv.org/abs/2102.07350)\n",
        "\n",
        "- __Direct Task Specification__: Define the task clearly and concisely, ensuring the model understands the exact requirements; for example, instructing the model to __\"Summarize the article in three sentences focusing on the main argument and conclusions.\"__\n",
        "- __Task Specification by Demonstration or Proxy__: Provide examples or analogies that closely mirror the desired outcome, guiding the model through similarity; for instance, showing a dataset and asking the model to __\"Classify these images into categories, similar to the provided example classifications.\"__\n",
        "- __Use of Narratives and Cultural References__: Embed the task within a story or cultural context to guide the model's approach; for example, presenting a dilemma similar to a scenario in __\"The Tortoise and the Hare\"__ and asking the model to __\"Analyze the characters' strategies and predict the outcome, considering the moral of the story and the characters' traits.\"__\n",
        "- __Metaprompts__: Craft prompts that encourage the model to generate its own task-specific prompts, essentially guiding the model to be self-reflective; for example, __\"Create a list of questions someone might ask about this topic,\"__ guiding the model to frame its own inquiries.\n",
        "\n",
        "2. [Chain-of-Thought](https://arxiv.org/abs/2201.11903)\n",
        "\n",
        "Incorporate step-by-step thought process exemplars into the input, the reasoning examples provide a scaffold, guiding the model to generate more structured and logical answers.\n",
        "\n",
        "3. [Chain-of-Thought-Self-Consistency](https://arxiv.org/abs/2203.11171)\n",
        "\n",
        "- __Generate Reasoning Paths__: Instead of using a single, greedy reasoning path, sample multiple diverse reasoning paths from the language model.\n",
        "- __Answer Aggregation__: Aggregate these multiple reasoning paths to identify the most consistent answer. This can be done by selecting the answer that appears most frequently among the different reasoning paths.\n",
        "- __Analysis of Results__: Evaluate the final, most consistent answer for its accuracy and relevance to the given task.\n",
        "\n",
        "4. [Least-to-most](https://arxiv.org/abs/2205.10625)\n",
        "\n",
        "<img src=\"http://drive.google.com/uc?export=view&id=1smPZnQA3ddfsLNXwTQxxMazO4rSTSW1-\" width=75%>\n",
        "\n",
        "Still, a manual decomposition, but works a bit better than simple CoT.\n",
        "\n",
        "5. [Program-of-Thoughts](https://arxiv.org/abs/2211.12588)\n",
        "\n",
        "- Use a language model capable of understanding and generating code (e.g., Codex).\n",
        "- __Formulate Prompt__: Create a prompt that clearly defines the problem and asks the model to generate a solution in the form of a program. This can be a natural language description of the problem.\n",
        "- __Execute PoT Prompting__: Input the prompt to the language model. The model generates a program that represents the step-by-step reasoning process to solve the problem.\n",
        "- __Run Generated Program__: Execute the generated program in a suitable environment (e.g., a Python interpreter) to perform the actual computation and obtain the solution.\n",
        "- __Evaluate Output__: Assess the correctness of the output produced by the executed program.\n",
        "\n",
        "6. [Plan-and-Solve Prompting](https://arxiv.org/abs/2305.04091)\n",
        "\n",
        "<img src=\"http://drive.google.com/uc?export=view&id=1EBFN0-BFkrXp2ScjVHsq255VzDOpQZt6\" width=85%>\n",
        "\n",
        "\n",
        "- __Planning Phase__: In this phase, the model is prompted to first understand the problem and devise a plan for solving it. This step involves breaking down the task into smaller, more manageable subtasks.\n",
        "\n",
        "- __Execution Phase__: Here, the model carries out the plan step by step, solving each subtask. The model focuses on careful calculation and attention to detail in this phase to ensure accurate and logical reasoning.\n",
        "\n",
        "7. [Tree-of-Thoughts](https://arxiv.org/abs/2305.10601)\n",
        "\n",
        "<img src=\"http://drive.google.com/uc?export=view&id=1OGJVkMCaIna2yCjWPkd2CB66mD7llVQv\" width=75%>\n",
        "\n",
        "<img src=\"http://drive.google.com/uc?export=view&id=1TKzBSTk6Z5c4oXLKZcjhDUXNzotH1tHh\" width=75%>\n",
        "\n",
        "\n",
        "- __Thought Decomposition__: Break down the problem-solving process into intermediate thought steps. This allows for the generation of diverse and promising thought samples, each contributing meaningfully towards solving the problem.\n",
        "\n",
        "- __Thought Generation__: Use a thought generator function to create multiple candidate thoughts for each step of the problem-solving process. There are two strategies for this: sampling independent, diverse thoughts or sequentially proposing thoughts in the same context.\n",
        "\n",
        "- __State Evaluation__: Employ a state evaluator to assess the progress of different thought paths towards solving the problem. This can be done by valuing each state independently or by voting across states to choose the most promising one.\n",
        "\n",
        "- __Search Algorithm__: Implement a search algorithm (like breadth-first or depth-first search) to systematically explore the 'tree of thoughts.' The algorithm decides which paths to continue exploring based on the evaluations of the state evaluator.\n",
        "\n",
        "<img src=\"http://drive.google.com/uc?export=view&id=1sXCjg35tVDSZO6kbmWfHWcPeEOaltu6D\" width=65%>\n",
        "\n",
        "8. [Logic-LM](https://arxiv.org/abs/2305.12295)\n",
        "\n",
        "<img src=\"http://drive.google.com/uc?export=view&id=1wQJ4vRk7OOAfqiMT4rFn_98e7PTxsXJB\" width=55%>\n",
        "\n",
        "Take a natural language problem, \"transcribe\" it to __logical formalism__, send it to a __reasoning engine__, get and refine results...\n",
        "\n",
        "<img src=\"http://drive.google.com/uc?export=view&id=1vvf5rzkqiDOtW81bcQrLPj2unAuuk-eP\" width=75%>\n",
        "\n",
        "\n",
        "9. [Emotion prompts](https://arxiv.org/abs/2307.11760)\n",
        "\n",
        "\"...experiments show that LLMs have a grasp of emotional intelligence, and their performance can be improved with emotional prompts (which we call \"EmotionPrompt\" that combines the original prompt with emotional stimuli), e.g., 8.00% relative performance improvement in Instruction Induction and 115% in BIG-Bench.\"\n",
        "\n",
        "<img src=\"http://drive.google.com/uc?export=view&id=1QT7mlVKCduRIUv9JfDvlepoxrnsWCpJm\" width=75%>\n",
        "\n",
        "\n",
        "10. [Skeleton-of-Thought](https://arxiv.org/abs/2307.15337)\n",
        "\n",
        "- __Skeleton Stage__:\n",
        "\n",
        "Input: Question\n",
        "Process: Generate a concise skeleton of the answer (a list of points or key ideas).\n",
        "Output: Skeleton (set of points)\n",
        "\n",
        "- __Point-Expanding Stage__:\n",
        "    For each point in the skeleton:\n",
        "    - Input: Point from the skeleton\n",
        "    - Process: Expand the point with more detailed information.\n",
        "    - Output: Expanded content for each point\n",
        "\n",
        "\n",
        "- __Combining Expanded Points__:\n",
        "\n",
        "Combine the expanded contents of each point to form a comprehensive answer.\n",
        "\n",
        "- __Output Final Answer__:\n",
        "\n",
        "The final, detailed answer is presented, synthesized from the expanded points.\n",
        "\n",
        "11. [Graph-of-Thoughts](https://arxiv.org/abs/2308.09687)\n",
        "\n",
        "<img src=\"http://drive.google.com/uc?export=view&id=1EeZ72lRAVT3gO9xcBUl9u4NsnuHZQM-C\" width=85%>\n",
        "\n",
        "\n",
        "- __Modeling Thoughts as a Graph__: Represent thoughts from a language model as vertices in a graph, with edges indicating dependencies between thoughts. This approach allows for more complex thought patterns than linear or tree-based models.\n",
        "\n",
        "- __Thought Transformations__: Implement various thought transformations, such as aggregating multiple thoughts into a new one, refining a current thought, or generating new thoughts based on existing ones. These transformations are modeled as operations on the graph.\n",
        "\n",
        "- __Scoring and Ranking Thoughts__: Introduce mechanisms to score and rank thoughts based on their relevance or accuracy, facilitating the selection of the most promising thoughts for further exploration or as part of the final output.\n",
        "\n",
        "- __System Architecture and Extensibility__: Develop a modular architecture consisting of components like a prompter, parser, scoring module, and controller to manage the thought generation and transformation process. The system is designed to be extensible for different use cases and prompting strategies.\n",
        "\n",
        "- __Implementation for Specific Tasks__: Apply the GoT framework to specific tasks, decomposing them into smaller subtasks that can be individually addressed and then merged for a comprehensive solution.\n",
        "\n",
        "<img src=\"http://drive.google.com/uc?export=view&id=1sQ-8Sr-V8uft-KBkttMZvpHTcbp-qojm\" width=95%>\n",
        "\n",
        "<img src=\"http://drive.google.com/uc?export=view&id=12XONwk6v14MMLeiXfzl-x4iRWawYz9Yf\" width=95%>\n",
        "\n",
        "\n",
        "\n",
        "12. [Algorithm-of-Thoughts](https://arxiv.org/abs/2308.10379)\n",
        "\n",
        "- __Defining the Problem Scope__: Clearly articulate the problem.\n",
        "\n",
        "- __Identifying Subproblems__: Break the main problem into smaller segments.\n",
        "\n",
        "- __Algorithmic Reasoning Pathways__:\n",
        "    - __Initialization__: Set relevant variables and initial conditions for each subproblem.\n",
        "    - __Iteration__: Use loops to process and refine solutions, adjusting variables based on intermediate outcomes.\n",
        "    - __Conditional Processing__: Implement IF-THEN-ELSE logic for decision-making and varied execution paths.\n",
        "    - __Recursive Thinking__: Employ recursive functions for self-similar subproblems until a base condition is achieved.\n",
        "    - __Backtracking__: Apply backtracking for dead-end scenarios, reversing some decisions to explore alternatives.\n",
        "- __Synthesizing Results__: Combine solutions of each segment.\n",
        "- __Optimization possibilities__:\n",
        "    - __Algorithm Efficiency__: Implement a binary search instead of a linear search when dealing with sorted data, significantly reducing the time complexity from O(n) to O(log n).\n",
        "    -__Memory Optimization__: Use lazy loading for data-intensive processes, where data is only loaded when necessary, rather than all at once, thereby reducing memory usage.\n",
        "    -__Parallel Processing__: Employ multi-threading to handle different subproblems of a complex task simultaneously, such as processing different sections of a large dataset in parallel.\n",
        "    -__Heuristic Methods__: Apply a genetic algorithm for optimization problems, where solutions evolve over generations to approximate the best solution, useful in scenarios where finding an exact solution is computationally impractical.\n",
        "    -__Feedback Loops__: Integrate real-time user feedback to adjust the language model's responses, dynamically refining the algorithm based on user satisfaction or specific input.\n",
        "    -__Benchmarking Against Known Solutions__: Regularly test the model's outputs against a set of pre-solved cases or industry-standard benchmarks to evaluate performance and identify areas for improvement.\n",
        "    \n",
        "13. [Step-Back Prompting](https://arxiv.org/abs/2310.06117)\n",
        "\n",
        "- __Abstract Question Generation__:\n",
        "    - __Input__: Original question.\n",
        "    - __Process__: Generate a higher-level, abstract question related to the original query.\n",
        "    - __Output__: Abstract question.\n",
        "- __Retrieval (Optional)__:\n",
        "    - __Process__: Retrieve relevant external information.\n",
        "    - __Output__: Retrieved data.\n",
        "- __Answering Abstract Question__:\n",
        "    - __Input__: Abstract question (and retrieved data if any).\n",
        "    - __Process__: Answer the abstract question using available information.\n",
        "    - __Output__: Abstract answer.\n",
        "- __Applying Abstract Answer__:\n",
        "    - __Input__: Abstract answer and original question.\n",
        "    - __Process__: Apply the abstract answer to the specific context of the original question.\n",
        "    - __Output__: Final answer to the original question.\n",
        "\n",
        "14. [Chain Of Natural Language Inference (CoNLI)](https://arxiv.org/abs/2310.03951)\n",
        "\n",
        "The core idea is to use a two-stage process involving detection and mitigation agents to identify and correct ungrounded hallucinations in the output of LLMs.\n",
        "\n",
        "- __Input__: Receive source text (X) and raw response (Yraw) from the LLM.\n",
        "- __Hypothesis Selection__: Segment Yraw into individual hypotheses.\n",
        "- __Sentence-Level Detection__: For each hypothesis, evaluate against the source text to categorize it as entailment, contradiction, or neutral. Both contradiction and neutral categories are treated as hallucinations.\n",
        "- __Entity-Level Detection__: For non-hallucinated hypotheses, perform a more detailed entity-level inspection to identify specific hallucinated parts.\n",
        "- __Combining Results__: Merge sentence-level and entity-level detection results to finalize the list of hallucinations.\n",
        "- __Mitigation__: Use the detection results as instructions to refine Yraw. This involves editing or removing parts of the text that were identified as hallucinated.\n",
        "- __Output__: Provide the refined response (Yrefined), which has reduced hallucinations while retaining the essence of Yraw.\n",
        "\n",
        "15. [Language Agent Tree Search](https://arxiv.org/abs/2310.04406)\n",
        "\n",
        "The paper is considered agent-based because it focuses on utilizing independent agents, each with specific roles and functionalities, within a computational model to interact with and process the data, thus simulating a multi-agent system environment.\n",
        "\n",
        "- __Initialize Environment__: Set up the environment that the language model will interact with. This could include tasks, data, or interactive elements.\n",
        "\n",
        "- __Define Inputs__: Identify and prepare the input data or situations for the language model.\n",
        "\n",
        "- __Prompt Generation__: Develop prompts that will guide the language model in generating responses. This might involve creating specific questions, scenarios, or tasks.\n",
        "\n",
        "- __LM Response Generation__: Use the language model to generate responses based on the prompts. This involves processing the prompts and outputting the model's responses.\n",
        "\n",
        "- __Evaluation__: Assess the quality of the responses. This could involve checking for accuracy, relevance, or other quality metrics.\n",
        "\n",
        "- __Iteration and Refinement__: Based on the evaluation, refine the prompts or model parameters, and repeat the process to improve the outcomes.\n",
        "\n",
        "- __Output Final Response__: Once a satisfactory response is generated, output the final response for the given task or query.\n",
        "\n",
        "This process resembles a tree search because each step, especially in prompt generation and refinement, branches out to explore multiple possibilities and paths, much like how nodes expand in a tree structure in search algorithms.\n",
        "\n",
        "16. [Rephrase and Respond](https://arxiv.org/abs/2311.04205)\n",
        "\n",
        "- __One-Step RaR__:\n",
        "    - __Prompt__: \"{question}\".\n",
        "    - __Instruction__: \"Rephrase and expand the question and respond.\"\n",
        "    - The model rephrases the original question for clarity and then provides an answer.\n",
        "\n",
        "- __Two-Step RaR__:\n",
        "    - __Step 1__:\n",
        "        - __Prompt__: \"{question}\".\n",
        "        - __Instruction__: \"Given the above question, rephrase and expand it to help you do better answering. Maintain all information in the original question.\"\n",
        "        - The model generates a rephrased question.\n",
        "    - __Step 2__:\n",
        "        - __Prompt__: Combine the original and rephrased questions.\n",
        "        - __Instruction__: \"Use your answer for the rephrased question to answer the original question.\"\n",
        "        - The model uses the rephrased question to provide a response.\n",
        "\n",
        "Both methods aim to enhance the clarity and accuracy of language model responses by addressing ambiguities in the original questions.\n",
        "\n",
        "17. [Contrastive chain-of-thought](https://arxiv.org/abs/2311.09277)\n",
        "\n",
        "- __Prepare Demonstrations__: Collect a set of demonstrations, each consisting of a question, a correct answer with reasoning steps (valid reasoning), and an incorrect answer with reasoning steps (invalid reasoning).\n",
        "\n",
        "- __Formulate the Prompt__: Present the model with a question and follow it with demonstrations that include both valid and invalid reasoning steps.\n",
        "\n",
        "- __Generate Model Output__: The model uses these demonstrations to generate a reasoning chain that leads to the correct answer.\n",
        "\n",
        "This approach teaches the model to differentiate between valid and invalid reasoning, potentially leading to more accurate problem-solving and reasoning capabilities.\n",
        "\n",
        "18. [Probabilistic Tree-of-thoughts](https://arxiv.org/abs/2311.13982)\n",
        "\n",
        "- __Understanding Phase__: Transform the input question into a query tree using language understanding capabilities. This tree has the original question as the root and sub-questions as non-root nodes.\n",
        "\n",
        "- __Reasoning Phase__: Conduct reasoning from the leaf nodes to the root. Use a combination of Closed-book QA (using internal model knowledge), Open-book QA (using external knowledge), and Child-aggregating QA (considering answers of child nodes).\n",
        "\n",
        "- __Decision Making__: For each node in the tree, choose the most confident answer from the three QA modules based on a calculated confidence score.\n",
        "\n",
        "This approach aims to integrate external and parametric knowledge in a unified framework, enabling the model to better handle complex, multi-part questions.\n",
        "\n",
        "19. [Chain of code](https://arxiv.org/abs/2312.04474)\n",
        "\n",
        "The core idea is to use a language model to generate code or pseudocode which is then executed by either a traditional code interpreter or a language model emulator, depending on the nature of the code. This approach allows the language model to leverage both its natural language understanding and code execution capabilities, improving its performance on a wide range of tasks.\n",
        "\n",
        "- __Generate Code__: The language model generates reasoning steps in the form of code, pseudocode, or natural language.\n",
        "- __Execute Code__: Attempt to execute the generated code with a traditional code interpreter. If the code is not executable or throws an exception, use a language model emulator to simulate the execution and update the program state accordingly.\n",
        "\n",
        "This method enables the language model to handle tasks that require a mix of semantic and numerical reasoning, expanding its problem-solving capabilities.\n",
        "\n",
        "20. [Forest of thoughts](https://www.youtube.com/watch?v=y6SVA3aAfco)\n",
        "\n",
        "Basically multiple Tree of thought processes running in parallel, slightly different prompts or even different LLMs, then the result is synthetized.\n",
        "\n",
        "***\n",
        "\n",
        "A very good summary article:\n",
        "[Something-of-Thoughts in LLM Prompting: An Overview of Structured LLM Reasoning](https://towardsdatascience.com/something-of-thought-in-llm-prompting-an-overview-of-structured-llm-reasoning-70302752b390)\n",
        "\n",
        "\n",
        "Some of these approaches utilize more heavy \"external\" algorithmic constructs to move around the \"chain\" of generation, so basically one can regard them as a kind of fusion attempt between classical AI / problem solving algorithms, like graph traversal, and the new LLM paradigm.\n",
        "\n",
        "It is so far unclear, if there is a single most efficient strategy for every problem, or how these algorithms can be elegantly integrated with LLM-s. In this regard, the versions where more of the process is implemented on the LLM side, just by repeatedly calling self-modifying prompts are the most promising.\n",
        "\n",
        "From this it seems obvious, that the **most fundamental topic, having AGI level breakthrough potential is the question of reasoning ability and true generalization for LLMs**.\n",
        "\n",
        "\n",
        "###### Another view on the same topic\n",
        "\n",
        "Also a nice overview - with a different emphasis - is can be found here in [A Systematic Survey of Prompt Engineering in Large Language Models: Techniques and Applications](https://arxiv.org/abs/2402.07927).\n",
        "\n",
        "<img src=\"http://drive.google.com/uc?export=view&id=15UKX4CJV83ieEVyIK-K8VIqR_cCCslJj\" width=80%>\n",
        "\n",
        "\n",
        "The interesting point is - that beyond the heavy \"reasoning\" focus of the list we presented above, other problems, like __hallucination__ can be tackled with some techniques. A little bit of an overkill to include RAG in here though...\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "__Conclusion__:\n",
        "\n",
        "- Two main effects are visible accross all these techniques:\n",
        "    - Usage of generation (or another representation) as __working memory__\n",
        "    - __Integration with rule based / algorithmic systems__ like reasoning engines, code execution environments, ...\n",
        "- All of this naturally flows into the direction of __multi agent systems__"
      ],
      "metadata": {
        "id": "zVfV2irQQo5C"
      },
      "id": "zVfV2irQQo5C"
    },
    {
      "cell_type": "markdown",
      "id": "f1983cf3",
      "metadata": {
        "id": "f1983cf3"
      },
      "source": [
        "#### Detour(?): Working memory\n",
        "\n",
        "As the methods above indicate, the \"context window\" of the LLMs, that hold prompts, as well as the input data to reliably steer the generation process can be considered as a kind of **working memory** of the model. The research on \"induction heads\" quoted above underpins this phenomena: The model tends to pay attention to specific parts of the context in next word prediciton, thus, **one of the ways to ensure good performance, is to add data to the inputs**.\n",
        "\n",
        "For this, the question of **longer context windows** needs to be solved."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a2d5d320",
      "metadata": {
        "id": "a2d5d320"
      },
      "source": [
        "### Problems of context length\n",
        "\n",
        "The primary limitation of the context widnow is the **resource constraint**, that is caused by the **quadratic computation requirements of the attention mechanism**.\n",
        "\n",
        "There is a long sequence of innovations in [LogSparse attention](https://arxiv.org/abs/1907.00235),  [Flash Attention](https://arxiv.org/abs/2205.14135), [FlashAttention-2](https://hazyresearch.stanford.edu/blog/2023-07-17-flash2), [Faster ring attention](https://arxiv.org/pdf/2311.09431.pdf), ... ...\n",
        "\n",
        "<img src=\"http://drive.google.com/uc?export=view&id=1eIxZ49q7My0fL-4elIDdVFjUj5o3m3IU\" width=75%>\n",
        "\n",
        "**The list is exremely rapidly evolving.**\n",
        "\n",
        "Also, there is a separate strand of research, that aims to **replace transformers and attention altogether** with some other, potentially RNN inspired structure, like [Raven](https://wiki.rwkv.com/) and it's many relatives, up till eg. [Mamba](https://arxiv.org/abs/2312.00752).\n",
        "\n",
        "(A nice intro can be found [here](https://youtu.be/ouF-H35atOY?si=kdj7bS3AxQPntReK).)\n",
        "\n",
        "\n",
        "The verdict is still out, which of these approaches will lead to breakthrough.\n",
        "\n",
        "The effect of this search in practice is the **rapid rise of larger context windows**.\n",
        "\n",
        "<img src=\"http://drive.google.com/uc?export=view&id=1BcIN6Eb9iRS-pokHthJb0-vhOvRXBckm\" width=90%>\n",
        "\n",
        "([source](https://cobusgreyling.medium.com/how-does-large-language-models-use-long-contexts-da9472bbb278))\n",
        "\n",
        "The question is, if, by the introduction of these nifty mechanisms, we **loose some data in the long context**?\n",
        "\n",
        "This seems to be the case:\n",
        "\n",
        "<img src=\"http://drive.google.com/uc?export=view&id=1di-dLj81lWRZEd62QgQv-p_pSIGcB1JD\" width=85%>\n",
        "\n",
        "[source](https://twitter.com/GregKamradt/status/1727018183608193393)\n",
        "\n",
        "Though some clever prompting tricks like [these](https://www.anthropic.com/index/claude-2-1-prompting) seems to at least mitigate this effect.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "44330c74",
      "metadata": {
        "id": "44330c74"
      },
      "source": [
        "### Back to facts!\n",
        "\n",
        "#### \"Mind in a box\"\n",
        "\n",
        "<img src=\"http://drive.google.com/uc?export=view&id=1GYEV_6oWcoXolIe0qDVTUhePwn_2icmc\" width=45%>\n",
        "\n",
        "As illustrated by the image above, LLMs by default, when the pretraining and finetuning is finished are \"frozen\" in time (since continuous training _as of now_ is computaitonally infeasible), so the act like __\"minds in a box\"__ that have no access to the outside world.\n",
        "\n",
        "In many (actually dominantly many) real life tasks the models ideally should have up to date information, and access to it's sources. The paradigm of [REALM: Retrieval-Augmented Language Model Pre-Training](https://arxiv.org/abs/2002.08909) - and the many followup works based upon it - __give access to an external knowledge base of facts - or later the whole internet - via a search engine__ that helps provide rich context for the LLMs to solve the tasks.\n",
        "\n",
        "<img src=\"http://drive.google.com/uc?export=view&id=1p7Ajkz_QnzhhI6zzUAgdtKKP04kPyVln\" width=45%>\n",
        "\n",
        "The whole paradigm of information retrieval - mainly via the help of high quality, contextual, neural (dense) and non-neural sparse) embeddings, hybrid methods, re-rankers,.. - comes to aid the LLM performance, and defines the new state-of-the-art.\n",
        "\n",
        "The typical schematic system architecture for this applications is as follows:\n",
        "\n",
        "<img src=\"http://drive.google.com/uc?export=view&id=1FRaJClzfkfTxszUOxVeNXM0SA2AFq_3Y\" width=80%>\n",
        "\n",
        "\n",
        "On more recent advancements in this area see the great survey: [Augmented Language Models: a Survey](https://arxiv.org/abs/2302.07842)\n",
        "\n",
        "The kind of \"procedural\" reasoning ability, combined with the inspired research towards the external resources lead to a new paradigm."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "181fa2ab",
      "metadata": {
        "id": "181fa2ab"
      },
      "source": [
        "### Detour: What strategy to use to reach a goal?\n",
        "\n",
        "Based on the discussions above, it is important to point out, that multiple strategies exist that one can choose for solving business problems, like \"prompting\", \"Retrieval augmentation\" and \"finetuning\". Though one can easily confuse their effects, typically in their most ideal setup, they have different goals and capabilities.\n",
        "\n",
        "|Method |Desired effect|Limitation |\n",
        "|-------|---------------|-----------|\n",
        "|Prompting|Temporarily modifies knowledge and behavior of the model| Knowledge is limited to context window, behavior is limited to pre-trained constraints|\n",
        "|Retrieval augmentation|Adds constantly updateable knowledge to the model from an auditable source |Does not modify behavior or capabilities |\n",
        "|Finetuning |Persistently modify behavioral traits (like style), or capabilities (like function calling) of a model|Can work, but is not ideal for adding new factual knowledge|\n",
        "\n",
        "In practice, dependeing on the business goal, potentially all of these soutions have to be combined together.\n",
        "\n",
        "It is worth noting though, that since prompting is by far the leat expensive solution, it is always worth starting with it, and only gradually add complexity, if the use case requires.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3d8033eb",
      "metadata": {
        "id": "3d8033eb"
      },
      "source": [
        "## \"Agents\" and \"Tools\" - access the \"outside world\"\n",
        "\n",
        "\n",
        "### Basic idea: ReACT\n",
        "\n",
        "Based on the emergent reasoning abilities and the introduction of external \"sources\", the authors of the paper [ReAct: Synergizing Reasoning and Acting in Language Models](https://arxiv.org/abs/2210.03629) went a step further, and proposed a solution in which the LLMs themselves are presented with a set of \"tools\", and asked to do a \"step-by-step palnning\" of how would they use the tools. The addon here is: __the planned steps of tool usage gets executed by a framework__, so if the LLM decides that it should search for a thing on the internet via a \"search engine tool\", the environment executes this \"query\" and gives back the result (in textual, \"serialized\" form) to the LLM.\n",
        "\n",
        "<img src=\"http://drive.google.com/uc?export=view&id=10HLujjZ-lZsyrlbhjNp_HofY_lAm1iNw\" width=85%>\n",
        "\n",
        "This way, the __LLM acts as an autonomuos agent, formulating \"plans\" and using \"tools\"__.\n",
        "\n",
        "\n",
        "### Result: Tool ecosystems\n",
        "\n",
        "As a result pf this paradigm, a __huge ecosystem__ of tools emerged, that aim to connect external services to LLMs, thus add extraordinary capabilities, and open up the floodgate for practical applications.\n",
        "\n",
        "The two dominant - in a sense mutually reinforcing - ecosystems are:\n",
        "\n",
        "<img src=\"http://drive.google.com/uc?export=view&id=1uRcgPG1k_z00rXkWV4TG_pm_HkGbuzqc\" width=80%>\n",
        "\n",
        "Read more about:\n",
        "- [ChatGPT plugins](https://openai.com/blog/chatgpt-plugins)\n",
        "- [Langchain](https://langchain.readthedocs.io/)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b2c3ef2e",
      "metadata": {
        "id": "b2c3ef2e",
        "outputId": "1c80aa78-5478-4bac-f3b3-4983836c63bc"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/nE2skSRWTTs?si=wsyQZmGeXky6BRmC\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen></iframe>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "%%HTML\n",
        "<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/nE2skSRWTTs?si=wsyQZmGeXky6BRmC\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen></iframe>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "76e030ca",
      "metadata": {
        "id": "76e030ca"
      },
      "source": [
        "# Agents - the new paradigm\n",
        "\n",
        "In a \"logical\" continuation of this paradigm, people started to experiment with models, that were capable of __learning the usage of new tools__ like in [Toolformer: Language Models Can Teach Themselves to Use Tools](https://arxiv.org/abs/2302.04761). The gust here is, that an appropriately trained, but also just an __appropriately prompted__ model can dynamically adapt (eg. via API description texts) to a new tool environment.\n",
        "\n",
        "## Single agent system\n",
        "\n",
        "This lead to experiments in **enhanced autonomy for single agent systems**, like:\n",
        "- [BabyAGI](https://www.youtube.com/watch?v=QBcDLSE2ERA)\n",
        "- [AutoGPT](https://www.youtube.com/watch?v=LqjVMy2qhRY) and\n",
        "- [HuggingGPT](https://www.youtube.com/watch?v=PfY9lVtM_H0)\n",
        "\n",
        "\n",
        "<img src=\"http://drive.google.com/uc?export=view&id=1oQklbjEhJHHI3I-5BBUV1Bls_S0JTo4F\" width=100%>\n",
        "\n",
        "[The Anatomy of Autonomy: Why Agents are the next AI Killer App after ChatGPT](https://www.latent.space/p/agents)\n",
        "\n",
        "[LLM Powered Autonomous Agents](https://lilianweng.github.io/posts/2023-06-23-agent/)\n",
        "\n",
        "<img src=\"http://drive.google.com/uc?export=view&id=18auUqCbpHnKu80dxv5dnqvqoWAgfSN0y\" width=100%>\n",
        "\n",
        "[MemGPT: Towards LLMs as Operating Systems](https://arxiv.org/abs/2310.08560)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0443535a",
      "metadata": {
        "id": "0443535a",
        "outputId": "bb6c103a-0517-4d48-e3c2-5edb260e5cf9"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<video src=\"http://drive.google.com/uc?export=view&id=1x4deJqkkJcLw_BBdp1vJIm3T_cBzsypq\" width=500 controls/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from IPython.display import HTML\n",
        "HTML(f\"\"\"<video src=\"http://drive.google.com/uc?export=view&id=1x4deJqkkJcLw_BBdp1vJIm3T_cBzsypq\" width=500 controls/>\"\"\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f1195baa",
      "metadata": {
        "id": "f1195baa"
      },
      "source": [
        "## Multi agent systems\n",
        "\n",
        "Beyond the level of a single autonomous agent, the collaborative coperation of agents holds the highest promise for truly complex problem solving.\n",
        "\n",
        "Some examples of frameworks for buiilding such **multi agent systems**:\n",
        "- [ChatDev](http://www.youtube.com/watch?v=Zlgkzjndpak)\n",
        "- [Autgen](https://www.microsoft.com/en-us/research/project/autogen/)\n",
        "- [TaksWeaver](https://arxiv.org/abs/2311.17541)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d81dee29",
      "metadata": {
        "id": "d81dee29",
        "outputId": "264ff841-0a64-49a5-bb99-90b3760291ac"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/Zlgkzjndpak?si=eac18mrf1CwligqN\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen></iframe>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "%%HTML\n",
        "<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/Zlgkzjndpak?si=eac18mrf1CwligqN\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen></iframe>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2a49e658",
      "metadata": {
        "id": "2a49e658"
      },
      "source": [
        "In case of the AutoGen framework, the single agents are esily customizable, and their connection / communication patterns definable.\n",
        "\n",
        "<img src=\"http://drive.google.com/uc?export=view&id=18alrj5xmaccPD_s423xsd3k0-DiABR68\" width=55%>\n",
        "\n",
        "Some secenarios that are \"trivial\":\n",
        "\n",
        "<img src=\"http://drive.google.com/uc?export=view&id=1jyWLN9xjSldu49bFYgM9vjRFR2LrN95s\" width=75%>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5b8e4d7c",
      "metadata": {
        "id": "5b8e4d7c"
      },
      "source": [
        "__These materials are definitely worth a look__, since they represents the absolute state-of-the-art.\n",
        "\n",
        "<font color='red'> They would merit a more thorough elaboration... </font>\n",
        "\n",
        "See [here](https://docs.google.com/presentation/d/1zl2Jg3ZH_aY6iD2-bi6Q5RQEU9f65Yzf1BSVnKHe4kg/edit#slide=id.p45)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4cad2805",
      "metadata": {
        "id": "4cad2805"
      },
      "source": [
        "Some notable __multi agent frameworks__:\n",
        "\n",
        "| Repository | Stars | Link | Note |\n",
        "|------------|-------|------| ---- |\n",
        "| microsoft/autogen | 21.2k | [AutoGen](https://github.com/microsoft/autogen) | Microsoft's most pushed has nice new UI |\n",
        "| OpenBMB/ChatDev | 19.6k | [ChatDev](https://github.com/OpenBMB/ChatDev) | Early, open, frequently used|\n",
        "| geekan/MetaGPT | 10k | [MetaGPT](https://github.com/geekan/MetaGPT) | |\n",
        "| joaomdmoura/crewAI | 6.3k | [crewAI](https://github.com/joaomdmoura/crewAI) | |\n",
        "| camel-ai/camel | 4.1k | [camel](https://github.com/camel-ai/camel) | |\n",
        "| microsoft/TaskWeaver | 3.9k | [TaskWeaver](https://github.com/microsoft/TaskWeaver) | Microsoft's other. WHY? |\n",
        "| OpenBMB/AgentVerse | 3.3k | [AgentVerse](https://github.com/OpenBMB/AgentVerse) | |\n",
        "| 101dotxyz/GPTeam |1.5k | [GPTeam](https://github.com/101dotxyz/GPTeam) | |\n",
        "| langroid/langroid | 1k | [langroid](https://github.com/langroid/langroid) | |\n",
        "| langchain-ai/LangGraph |~900| [LangGraph](https://github.com/langchain-ai/langgraph)| |\n",
        "| yeagerai/genworlds | 255 | [genworlds](https://github.com/yeagerai/genworlds) | |\n",
        "\n",
        "(...as of February 2024...)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1ce96d72",
      "metadata": {
        "id": "1ce96d72"
      },
      "source": [
        "### Additonal notes\n",
        "\n",
        "This vision - and in fact reality - of trained neural agents working in cooperation to achieve a commong complex goal very much resembles [Minsky's society of mind](https://en.wikipedia.org/wiki/Society_of_Mind) hypothesis, and gives some strong, empiric evidence in favour of it. It is basically the current incarnation of the dream of \"atonomuos software agents\" from decades ago.\n",
        "\n",
        "Also, it is important to stress, that with the advent of the T5 style text in - text out paradigm, and the prevalence of the zero shot approach, (nearly) __all interactions with the models are via text__.\n",
        "\n",
        "This __\"via language\" paradigm__ proved to be so general, that some researchers (eg. in [TabLLM: Few-shot Classification of Tabular Data with Large Language Models](https://arxiv.org/abs/2210.10723)) endeavored to cast \"traditional\", non-nlp problems into language in an attempt to solve them with LLMs.\n",
        "\n",
        "And since __software code is wirtten in human parseable languages___ (so as to enable us writing them), __the whole area of software development and software services became accessible for LLMs__. Natural language became the \"glue\", the \"communicaiton medium\" between humans, software, and in a sense amongst software. And as the famous saying goes [\"Software eats the world\"](https://a16z.com/2011/08/20/why-software-is-eating-the-world/)."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9530765e",
      "metadata": {
        "id": "9530765e"
      },
      "source": [
        "# Outlook - what to look for?\n",
        "\n",
        "- Scaling wars might stop (even OpenAI's [Sam Altman hints](https://www.wired.com/story/openai-ceo-sam-altman-the-age-of-giant-ai-models-is-already-over/) to this)\n",
        "- Full OpenSource models will become (are) available (eg. [LAION-AI/Open-Assistant](https://github.com/LAION-AI/Open-Assistant))\n",
        "- Multi-modality gets dominant (see for example [here](https://github.com/X-PLUG/mPLUG-Owl))\n",
        "- Auto-coding gets prevalent (see for example [this](https://levelup.gitconnected.com/the-end-of-coding-experts-predict-a-future-of-automated-development-c0aae9c458a2) summary)\n",
        "- __Business and personal integration will be the main challenge__\n",
        "- Reinforcment Learning might be relevant to [ground models](https://arxiv.org/abs/2302.02662)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}