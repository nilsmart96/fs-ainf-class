{"cells":[{"cell_type":"markdown","metadata":{"id":"K5i0bXyQto9W"},"source":["# Bellman-equation"]},{"cell_type":"markdown","metadata":{"id":"XHz7SKxsto9Y"},"source":["## Motivation\n","\n","we have learned how reinforcement learning is about leaning a good (or optimal) path of actions in situations of sequential decision making\n","\n","The \"simplest\" way of solving this problem is sampling all actions\n","\n","As there are more actions complexity explodes\n","\n","Remainder of the class is about how to deal with such complexity\n","- Efficiently sampling (learning from previous data) (e.g. off-policy learning/ causality)\n","- Efficiently finding \"optimal\" policies (e.g. Bellman Equations)\n","- Efficiently summarizing what we have \"learned\" (e.g. deep reinforcement learning) and generalizing"]},{"cell_type":"markdown","metadata":{"id":"M-hwMe3Nto9Z"},"source":["### Efficiency of the Bellman Equation - dynamic programming\n","\n","Lets denote the number of states with $n_s$ and the number of actions with $n_a$.\n","The dynamic programming method is guaranteed to find the optimal policy in **polynomial time** (of $n_s$ and $n_a$).\n","The total number of possible policies are $n_a^{n_s}$. Therefore the dynamic programming approach is **exponentially faster than any direct search e.g. Monte Carlo Tree search** in policy space.\n","\n","\n","In practice, the dynamic programming approach can solve MDP problems (to be defined in what follows) with **millions of states**. This is important because if you manage to formalize  a problem in a tabular setting than it can be solved by dynamic programming and you can take advantage of its good convergence properties."]},{"cell_type":"markdown","metadata":{"id":"rINjMqJmto9Z"},"source":["## State-value, action-value\n","\n","**V - state-value function**\n","\n","**Q - action-value function**\n"]},{"cell_type":"markdown","metadata":{"id":"aCf2N4_Qto9Z"},"source":["### State-value function (shortly: value-function)\n","\n","The $\\pi$ policy generates trajectories ($\\tau$) until the end of the episode, starting from $s_0$.\n","$\\tau = [s_0, a_0, r_0, s_1, a_1, r_1, ..., s_i, a_i, r_i, ..., s_T, a_T, r_T]$\n","\n","$$V^\\pi(s) = E_\\tau \\left[ G(\\tau) | s_0 = s, \\pi \\right]$$\n","\n","Where $G$ is the return. If the discounted return is used:\n","\n","$$V^\\pi(s) = E_\\tau \\left[ \\sum_i{\\gamma^i r_i} | s_0 = s, \\pi, r_i \\in \\tau \\right].$$"]},{"cell_type":"markdown","metadata":{"id":"GkQQ49Afto9Z"},"source":["### Action-value function\n","\n","The $\\pi$ policy generates trajectories ($\\tau$) until the end of the episode, starting from $s_0$.\n","$\\tau = [s_0, a_0, r_0, s_1, a_1, r_1, ..., s_i, a_i, r_i, ..., s_T, a_T, r_T]$\n","\n","$$Q^\\pi(s, a) = E_\\tau \\left[ G(\\tau) | s_0 = s, a_0 = a, \\pi \\right]$$\n","\n","Where $G$ is the return. If the discounted return is used ($\\gamma < 1$):\n","\n","$$Q^\\pi(s, a) = E_\\tau \\left[ \\sum_i{\\gamma^i r_i} | s_0 = s, a_0 = a, \\pi, r_i \\in \\tau \\right].$$"]},{"cell_type":"markdown","metadata":{"id":"exRXnnk2to9a"},"source":["## Monte Carlo solution"]},{"cell_type":"markdown","metadata":{"id":"16BnXTTzto9a"},"source":["The most naiv approach would be sampling trajectories and calculate the returns for each of them. Then it is possible to average them out and find the value function for each state:\n","\n","$$V(s) = \\frac{\\sum_i^n{G(\\tau_i)}}{n}$$\n","\n","However, we can use a trajectory for sampling return for all the states encountered during the trajectory.\n","There are two strategies how to do that: first visit and every visit.\n","\n","First visit means, if a state encountered several times during a trajectory, the return is calculated for only the first visit.\n","\n","Every visit means, if a state encountered several times, the returns are calculated for each of them."]},{"cell_type":"markdown","metadata":{"id":"1UbjbdCXto9a"},"source":["\n","<img src=\"http://drive.google.com/uc?export=view&id=1uboWLi-NoQ1GMUZtrF1DBsc4Rxqx1BTU\" width=65%>"]},{"cell_type":"markdown","metadata":{"id":"BDr-qHeSto9a"},"source":["The $s_i$ and $s_j$ is the same state but encountered after $i$ and $j$ steps.\n","The return gathered after state $s_i$:\n","\n","$$G_i = \\sum_{k=i}^T{ r(s_k) }$$\n","\n","$$G_j = \\sum_{k=j}^T{ r(s_k) }$$\n","\n","For the first-visit MC, only $G_i$ is considered while for every-visit MC both $G_i$ and $G_j$ is considered."]},{"cell_type":"markdown","metadata":{"id":"xDhVlfYyto9a"},"source":["## Dynamic programming solution"]},{"cell_type":"markdown","metadata":{"id":"gKJn7jQ-to9b"},"source":["The Bellman-equation was named after Richard E. Bellman who applied this approach for engineering control problems.\n","\n","*Almost any problem that can be solved using optimal control theory can also be solved by analyzing the appropriate Bellman equation. However, the term 'Bellman equation' usually refers to the dynamic programming equation associated with discrete-time optimization problems. In continuous-time optimization problems, the analogous equation is a partial differential equation that is usually called the Hamilton–Jacobi–Bellman equation.* (See [Wikipedia](https://en.wikipedia.org/wiki/Bellman_equation))"]},{"cell_type":"markdown","metadata":{"id":"iJhpqf2Xto9b"},"source":["### Bellman-equation"]},{"cell_type":"markdown","metadata":{"id":"N4hEZiK2to9b"},"source":["Derivation of the Bellman-equation (mathematical version):\n","\n","$$V^\\pi(s) = E_\\tau \\left[ G(\\tau) | s_0 = s, \\pi \\right]$$\n","$$V^\\pi(s) = E_\\tau \\left[ r_1 + \\gamma r_2 + \\gamma^2 r_3 + ... | s_0 = s, \\pi \\right]$$\n","$$V^\\pi(s) = E_\\tau \\left[ r_1 + \\gamma (r_2 + \\gamma r_3 + ...) | s_0 = s, \\pi \\right]$$\n","$$V^\\pi(s) = E_\\tau \\left[ r_1 + \\gamma G(\\tau / s_0) | s_0 = s, \\pi \\right]$$\n","The expectation is an affine operation:\n","$$V^\\pi(s) = E_\\tau \\left[ r_1 \\right] + \\gamma E_\\tau \\left[ G(\\tau / s_0) | s_0 = s, \\pi \\right]$$\n","The expected value of a scalar is the scalar itself:\n","$$V^\\pi(s) = E_\\tau \\left[ r_1 + \\gamma E_\\tau \\left[ E_\\tau G(\\tau / s_0) | s_0 = s, \\pi \\right] \\right]$$\n","$$V^\\pi(s) = E_\\tau \\left[ r_1 + \\gamma V^\\pi(s_1) | s_0 = s, \\pi \\right]$$"]},{"cell_type":"markdown","metadata":{"id":"-bVjez8Eto9b"},"source":["[link text](https://)Understanding the Bellman-equation:\n","\n","<img src=\"http://drive.google.com/uc?export=view&id=1J-jj-LQLnKIMFy9KV6cdKONtXQlMHhWI\" width=70%>"]},{"cell_type":"markdown","metadata":{"id":"EGdJbYxUto9b"},"source":["Understanding the Bellman-equation:\n","\n","The Bellman-equation is the result of the dynamic programming description of the trajectory. In a nutshell, we can think of dynamic programming as recursion: in order to get the current value, we can rely on the previous results. If we have a well defined initial value, base, then we can calculate the actual value. E.g.:\n","\n","$H(k+1) = f(H(k))$\n","\n","and $H(0)$ is known or trivial. $f$ is a function which is the connection between the current and the previous result. $H$ is the function of interest we would like to calculate.\n","\n","Of course, this is not a general description of dynamic programming, just a short example."]},{"cell_type":"markdown","metadata":{"id":"fPPRaVG6to9b"},"source":["How does this look like for the RL setting?\n","\n","Relying on the previous example, we can write:\n","\n","$$V^\\pi(s) = f(V^\\pi(N(s)))$$\n","\n","where $N(s)$ is the set of states which is the neighborhood of the state $s$. Neighborhood means, the states available directly (with higher probability than 0) from state $s$."]},{"cell_type":"markdown","metadata":{"id":"gsuxQVrLto9b"},"source":["What is the $f$ function?\n","\n","For the trajectory of an agent, the Markov-property is true as we discussed earlier. Therefore it is not required to know the whole sequence to calculate the current state's value.\n","\n","It is enough to know the current state and the transition probabilities to make one step forward.\n","\n","Instead of using the recursion (backward direction), we can use a forward view. We can ask, if I know the value of all the neighboring states (possible next states), then what is the value of the current state."]},{"cell_type":"markdown","metadata":{"id":"aKHjz2xVto9b"},"source":["<img src=\"http://drive.google.com/uc?export=view&id=16RAhhRENpOgFv_F6MgVdECrP63Bw0HRx\" width=70%>"]},{"cell_type":"markdown","metadata":{"id":"MOFtAigcto9b"},"source":["Formalizing the Bellman-equation (intuitive version):\n","\n","* Current state: $s$\n","* A next state: $s' \\in N(s)$\n","* Next action: $a$\n","* Immediate reward when moving from s with a: $r(s, a)$\n","\n","For a single transition, $s \\rightarrow a \\rightarrow r \\rightarrow s'$:\n","\n","$$v(s) = r(s, a) + \\gamma V(s')$$\n","\n","$v(s)$ is for showing this is for only one path. Then we need the expected value of the $v(s)$ values.\n","Therefore we want to know the probability of the transition, shown above:\n","\n","$$p(s, a, s') = \\pi(s, a) \\cdot T(s, a, s')$$"]},{"cell_type":"markdown","metadata":{"id":"c7l9Lxxdto9b"},"source":["Formalizing the Bellman-equation (intuitive version):\n","\n","Then the expected value:\n","\n","$$V^\\pi(s) = \\sum_{a \\in A}{\\sum_{s'\\in N(s)}{ p(s, a, s') \\cdot v(s)}}$$\n","\n","After putting them together:\n","$$V^\\pi(s) = \\sum_{a \\in A}{\\sum_{s'\\in N(s)}{ \\pi(s, a) T(s, a, s') \\cdot \\left[ r(s, a) + \\gamma V^\\pi(s') \\right]}}$$"]},{"cell_type":"markdown","metadata":{"id":"oipf37s9to9c"},"source":["The previous formulation is true for:\n","* state-value function\n","* stochastic policy (known, or fixed)\n","\n","As a consequence, there are several forms of the Bellman-equation. Let's go through them!"]},{"cell_type":"markdown","metadata":{"id":"syNQtcWvto9c"},"source":["**What is the Bellman-equation for the state-value function with a deterministic policy?**"]},{"cell_type":"markdown","metadata":{"id":"UegprSlQto9c"},"source":["State-value, deterministic, fixed policy:\n","\n","$$V^\\pi(s) = \\sum_{s'\\in N(s)}{T(s, \\pi(s), s') \\cdot \\left[ r(s, \\pi(s)) + \\gamma V^\\pi(s') \\right]}$$"]},{"cell_type":"markdown","metadata":{"id":"Kg4DgCdkto9c"},"source":["**What is the Bellman-equation for the action-value function with a stochastic policy?**"]},{"cell_type":"markdown","metadata":{"id":"ZxDb-Htwto9c"},"source":["$$Q^\\pi(s, a) = \\sum_{s'}{T(s, a, s') \\cdot \\left[ r(s, a) + \\gamma \\cdot V^\\pi(s') \\right]}$$\n"]},{"cell_type":"markdown","metadata":{"id":"LzAQObt7to9c"},"source":["### Examples\n","\n","Calculate the $V$ and $Q$ values for states, marked with question mark. The rewards are recieved when the agent enters to the state. There are two actions, according to which state the agent wants to go. The transition probabilities are the shown beside the arrows and it is the probability of the transition if the action triggers move to that state. The policy is the random policy. $\\gamma = 0.8$."]},{"cell_type":"markdown","metadata":{"id":"u39Pl5-kto9c"},"source":["**Example 1**\n","\n","<img src=\"http://drive.google.com/uc?export=view&id=1kyp0TwU3OsCoRWI7gnd56AlFhkDFqkM9\" width=65%>"]},{"cell_type":"markdown","metadata":{"id":"EMuQXn5qto9c"},"source":["**Example 2**\n","\n","<img src=\"http://drive.google.com/uc?export=view&id=1hRJHlGQgCZy42170feEWALHyWBcJuC0G\" width=65%>"]},{"cell_type":"markdown","metadata":{"id":"zzczQshkto9c"},"source":["**Example 3**\n","\n","<img src=\"http://drive.google.com/uc?export=view&id=1sMXIbM7969JquYXrIB7kmMrX0U6RhqUI\" width=65%>"]},{"cell_type":"markdown","metadata":{"id":"Ppsrf5Rjto9c"},"source":["**What is the connection between $Q$ and $V$?**\n","\n","Try to answer this!"]},{"cell_type":"markdown","metadata":{"id":"aw5OeAegto9c"},"source":["**What is the connection between $Q$ and $V$?**\n","\n","$$V^\\pi(s) = \\sum_a{\\pi(s, a) \\cdot Q^\\pi(s, a)}$$\n","\n","and\n","\n","$$Q^\\pi(s, a) = \\sum_{s'}{ T(s, a, s') \\cdot \\left[ r(s, a) + \\gamma V^\\pi(s') \\right] }$$"]},{"cell_type":"markdown","metadata":{"id":"eJ1BYFJAto9c"},"source":["**Calculating the policy**\n","\n","If the state-value function is known (**requires the transition matrix**):\n","\n","$$\\pi'(s) = \\arg\\max_a\\left( \\sum_{s'}T(s, a, s') \\cdot \\left[ r(s, a) + \\gamma V^\\pi(s') \\right]\\right)$$\n","\n","If the action-value function is known:\n","\n","$$\\pi'(s) = \\arg\\max_a\\left( Q^\\pi(s, a) \\right)$$\n","\n","As you can see, only deterministic policies can be calculated from the value-functions. $\\pi' = \\pi$ if the Q-function (and V) is the optimal one."]},{"cell_type":"markdown","metadata":{"id":"BI1GJWZHto9c"},"source":["### Solution analysis\n","\n","So far we have discussed a dynamic programming relation among states for the state-value and action-value functions.\n","The next three question we would like to address is, how to find:\n","\n","* the solution (V or Q) for a given policy\n","* how to improve the policy\n","* how to find the optimal policy\n","\n","We will answer this questions later on. Regarding the first point, the natural question is: Is the solution unique?"]},{"cell_type":"markdown","metadata":{"id":"uIab71Nhto9c"},"source":["Banach's fix-point theorem (brief):\n","\n","<img src=\"http://drive.google.com/uc?export=view&id=1drPT32Y6HKG1r8k11LFMOYUbX0AWWimw\" width=65%>"]},{"cell_type":"markdown","metadata":{"id":"Xo26zoODto9c"},"source":["Assume we have a vector space ($Z$) and we have a defined distance metric $d$ (e.g. euclidian-distance). The operation $F$ is a contraction if the following holds:\n","\n","$$d(F(s), F(t)) < d(s, t)\\ \\forall s, t \\in Z$$\n","\n","For such an $F$ contraction operator, there exists **exactly one point** (fix point):\n","\n","$$x = F(x)$$"]},{"cell_type":"markdown","metadata":{"id":"wfJjGypnto9d"},"source":["**Proof by contradiction**\n","\n","If we would have two fix points:\n","\n","$$x_1 = F(x_1)$$\n","and\n","$$x_2 = F(x_2)$$\n","\n","Then:\n","\n","$$d(F(x_1), F(x_2)) < d(x_1, x_2)$$\n","\n","Because the inequality is strict, this is a contradiction, $x_1$ and $x_2$ can not be different."]},{"cell_type":"markdown","metadata":{"id":"J1P-PDdMto9d"},"source":["How does this connect to the Bellman-equation?\n","\n","Take the V(s) function as a vector (discrete states and finite). Then one can reformalize the Bellman-equation:\n","\n","$$\\overline{V} = \\overline{R} + \\gamma P \\overline{V} = F^\\pi_\\gamma\\left( \\overline{V} \\right)$$\n","\n","It can be proven, that $F^\\pi_\\gamma$, $\\gamma < 1$ is a contraction (L2-norm). Therefore there is only one, unique $V$ for solution. The same is true for $Q$. $P$ contains the $\\pi$ and $T$.\n","\n"]},{"cell_type":"markdown","metadata":{"id":"4HGXI6Bvto9d"},"source":["Which is the Bellman equation written in a vectorized form\n","\n","$$\n","\\left[\\begin{array}{c}\n","v_\\pi(1) \\\\\n","\\vdots \\\\\n","v_\\pi(n)\n","\\end{array}\\right]=\\left[\\begin{array}{c}\n","\\mathcal{R}_1^\\pi \\\\\n","\\vdots \\\\\n","\\mathcal{R}_n^\\pi\n","\\end{array}\\right]+\\gamma\\left[\\begin{array}{ccc}\n","\\mathcal{P}_{11}^\\pi & \\ldots & \\mathcal{P}_{1 n}^\\pi \\\\\n","\\vdots & \\ddots & \\vdots \\\\\n","\\mathcal{P}_{n 1}^\\pi & \\ldots & \\mathcal{P}_{n n}^\\pi\n","\\end{array}\\right]\\left[\\begin{array}{c}\n","v_\\pi(1) \\\\\n","\\vdots \\\\\n","v_\\pi(n)\n","\\end{array}\\right]\n","$$\n","\n","Based on the below form of the value function\n","\n","$$\n","v_\\pi(s)=\\sum_{a \\in \\mathcal{A}} \\pi(a \\mid s)\\left(\\mathcal{R}_s^a+\\gamma \\sum_{s^{\\prime} \\in \\mathcal{S}} \\mathcal{P}_{s s^{\\prime}}^a v_\\pi\\left(s^{\\prime}\\right)\\right)\n","$$\n","\n","where\n","\n","$$\n","\\mathcal{R}_s^\\pi=\\sum_{a \\in \\mathcal{A}} \\pi(a \\mid s) \\mathcal{R}_s^a\n","$$\n","\n","and\n","\n","$$\n","\\mathcal{P}_{s s^{\\prime}}^\\pi=\\sum_{a \\in \\mathcal{A}} \\pi(a \\mid s) \\mathcal{P}_{s s^{\\prime}}^a\n","$$\n","\n"]},{"cell_type":"markdown","metadata":{"id":"EAXsnfbZto9g"},"source":["### Matrix solution"]},{"cell_type":"markdown","metadata":{"id":"hbBXpnMnto9g"},"source":["The Bellman-equation in a matrix form:\n","\n","$$\\overline{V} = \\overline{R} + \\gamma P \\overline{V}$$\n","\n","Then, we can solve this directly:\n","\n","$$\\overline{V} = \\left( I - \\gamma P \\right)^{-1}\\overline{R}$$"]},{"cell_type":"markdown","metadata":{"id":"mp-kO5v3to9g"},"source":["This is a really appealing solution. Seems easy. But it has $O(n^3)$ computational complexity if the state space contains $n$ states. Therefore it is only useful, if:\n","\n","* the state space is small enough\n","* the reward function is known\n","* the transition probability (the dynamics of the environemnt) is known.\n","\n","Furthermore this method, still not answeres how to find the optimal policy. This is just the calculation of the V (or Q) functions."]},{"cell_type":"markdown","metadata":{"id":"qOpEhrtrto9h"},"source":["### Iterative solution"]},{"cell_type":"markdown","metadata":{"id":"Pji_uwOXto9h"},"source":["The solution has the following phases:\n","1. Initialize the policy $\\pi$ and value table $V$\n","2. Calculate $V^\\pi$ for the current $\\pi$\n","3. Improve the policy according to $V^\\pi$\n","4. Repeat until convergence\n","\n","This can be used to find the solution. Below we will focus on step 2 and 3. For step 2, the matrix solution can be used as well. This method is similar for $Q$."]},{"cell_type":"markdown","metadata":{"id":"BGOane41to9h"},"source":["**Policy evaluation**\n","\n","Due to the fact that the Bellman-equation can be considered as a contraction operation, the repeated use of the operator will get the value of $V$ closer to the solution (the fix point), step-by-step.\n","\n","Therefore, the following equation should be evaluated several times, until the $V$ does not change significantly:\n","\n","$$V^\\pi_{t+1}(s) = \\sum_{s'\\in N(s)}{ T(s, \\pi(s), s') \\cdot \\left[ r(s, \\pi(s)) + \\gamma V^\\pi_t(s') \\right]}$$"]},{"cell_type":"markdown","metadata":{"id":"Txmo_wyyto9h"},"source":["**Policy improvement (policy improvement theorem)**\n","\n","After knowing the $V$ or $Q$, calculate the best policies suggested by $V$:\n","\n","$$\\pi'(s) = \\arg\\max_a\\left( \\sum_{s'\\in N(s)}{ T(s, a, s') \\cdot \\left[ r(s, a) + \\gamma V^\\pi(s') \\right]}\\right)$$\n","\n","Then:\n","\n","$$\\pi \\leftarrow \\pi'$$"]},{"cell_type":"markdown","metadata":{"id":"7CCw8QeUto9h"},"source":["**Policy improvement**\n","\n","The key question is whether the policy $\\pi'$ is better than $\\pi$. The **policy improvement theorem** answers this:\n","\n","*Let $\\pi$ and $\\pi'$ be any pair of deterministic policies such that, for all $s \\in S$*,\n","\n","$$Q^\\pi(s, \\pi'(s)) \\geq V^\\pi(s)$$\n","\n","*Then the policy $\\pi'$ must be as good as, or better than, $\\pi$:*\n","\n","$$V^{\\pi'}(s) \\geq V^\\pi(s)$$"]},{"cell_type":"markdown","metadata":{"id":"JjOO3hF8to9h"},"source":["**Proof of policy improvement**\n","\n","Keep expanding the $Q^\\pi(s, \\pi'(s))$:\n","\n","$$V^\\pi(s) \\leq Q^\\pi(s, \\pi'(s))$$\n","$$V^\\pi(s) \\leq E_\\pi\\left[ r(s, a) + \\gamma V^\\pi(s_1) | s = s_0, a = \\pi'(s) \\right]$$\n","$$V^\\pi(s) \\leq E_{\\pi'}\\left[ r(s, a) + \\gamma V^\\pi(s_1) | s = s_0 \\right]$$\n","$$V^\\pi(s) \\leq E_{\\pi'}\\left[ r(s, a) + \\gamma Q^\\pi(s_1, \\pi'(s_1)) | s = s_0 \\right]$$\n","$$V^\\pi(s) \\leq E_{\\pi'}\\left[ r(s, a) + \\gamma E_{\\pi'}\\left[ r(s_1, a_1) + \\gamma V^\\pi(s_2) \\right] | s = s_0 \\right]$$\n","$$V^\\pi(s) \\leq E_{\\pi'}\\left[ r(s, a) + \\gamma r(s_1, a_1) + \\gamma^2 V^\\pi(s_2) | s = s_0 \\right]$$\n","$$V^\\pi(s) \\leq E_{\\pi'}\\left[ r(s, a) + \\gamma r(s_1, a_1) + \\gamma^2 r(s_2, a_2)  + \\gamma^3 V^\\pi(s_2) | s = s_0 \\right]$$\n","$$V^\\pi(s) \\leq E_{\\pi'}\\left[ r(s, a) + \\gamma r(s_1, a_1) + \\gamma^2 r(s_2, a_2)  + \\gamma^3 r(s_3, a_3) + ... | s = s_0 \\right]$$\n","$$V^\\pi(s) \\leq V^{\\pi'}(s)$$"]},{"cell_type":"markdown","metadata":{"id":"QJucDBYtto9h"},"source":["**Policy iteration**\n","\n","<img src=\"http://drive.google.com/uc?export=view&id=1CupNVaOMh5H3ikPIV9rllXkKdoasBzPb\" width=75%>"]},{"cell_type":"markdown","metadata":{"id":"nIAwwpb_to9h"},"source":["**Value iteration**\n","\n","It is possible to fuse the policy evaluation and policy improvement and apply only one operation at a time:\n","\n","$$V^*_{t+1}(s) = \\max_a{\\sum_{s'\\in N(s)}{T(s, a, s') \\cdot \\left[ r(s, a) + \\gamma V^*_t(s') \\right]}}$$"]},{"cell_type":"markdown","metadata":{"id":"Vur3JSwNto9h"},"source":["**Value iteration**\n","\n","<img src=\"http://drive.google.com/uc?export=view&id=1NdOv59iJULMNDv8yt7itaNLSV8OIVWfG\" width=75%>"]},{"cell_type":"markdown","metadata":{"id":"RhBCU0Knto9i"},"source":["### Complexity of dynamic programming (revisited)\n","\n","- Recall that the Bellman equation is a particular type of dynamic programming\n","\n","**Time Complexity**\n","In Dynamic programming problems, Time Complexity is the number of unique states/subproblems * time taken per state.\n","\n","In this problem, for a given n, there are n unique states/subproblems. For convenience, each state is said to be solved in a constant time. Hence the time complexity is O(n * 1).\n","\n","**Space Complexity**\n","We use one array called cache to store the results of n states. Hence the size of the array is n. Therefore the space complexity is O(n).\n","\n","\n","Note that there are trade-offs between complexity in space and time\n","\n","\n","For large problems (many states and many actions) the Bellman formulation is still not efficient.\n","However (as shown above), it is more efficient than the Monte Carlo method.\n","\n","\n","[source](https://www.freecodecamp.org/news/demystifying-dynamic-programming-24fbdb831d3a/#:~:text=In%20Dynamic%20programming%20problems%2C%20Time,O(n%20*%201))"]},{"cell_type":"markdown","metadata":{"id":"p3KiZZ5wto9i"},"source":["## Bellman-equation summary/recap"]},{"cell_type":"markdown","metadata":{"id":"qmK-ijbfto9i"},"source":["<img src=\"http://drive.google.com/uc?export=view&id=1J-jj-LQLnKIMFy9KV6cdKONtXQlMHhWI\" width=75%>"]},{"cell_type":"markdown","metadata":{"id":"fdkMrgYIto9i"},"source":["Value-function, fixed stochastic policy\n","$$V^\\pi(s) = \\sum_{s', a}{\\pi(s, a) \\cdot T(s, a, s') \\cdot \\left[ r(s, a) + \\gamma \\cdot V^\\pi(s') \\right]}$$\n","\n","Value-function, fixed deterministic policy\n","$$V^\\pi(s) = \\sum_{s'}{T(s, \\pi(s), s') \\cdot \\left[ r(s, \\pi(s)) + \\gamma \\cdot V^\\pi(s') \\right]}$$"]},{"cell_type":"markdown","metadata":{"id":"vFOb1285to9i"},"source":["Action value-function, fixed stochastic policy\n","$$Q^\\pi(s, a) = \\sum_{s'}{T(s, a, s') \\cdot \\left[ r(s, a) + \\gamma \\cdot V^\\pi(s') \\right]}$$\n"]},{"cell_type":"markdown","metadata":{"id":"EJXm_dMQto9i"},"source":["Optimal value-function\n","$$\\tilde{V}(s) = \\max_a \\sum_{s'}{T(s, a, s') \\cdot \\left[ r(s, a, s') + \\gamma \\cdot \\tilde{V}(s') \\right]}$$\n","\n","Optimal action value-function\n","$$\\tilde{Q}(s, a) = \\sum_{s'}{T(s, a, s') \\cdot \\left[ r(s, a, s') + \\gamma \\cdot \\max_{a'} \\tilde{Q}(s', a') \\right]}$$"]}],"metadata":{"celltoolbar":"Slideshow","kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.7"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":0}